{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3td-FsdxahjW"
      },
      "source": [
        "# Basic Functions used"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eQxvhNQpalfm"
      },
      "source": [
        "## Pool BN\n",
        "\n",
        "A pooling followed by a batch normalization, for both maxpooling and average pooling methods."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rnBq-TxtGWB7"
      },
      "outputs": [],
      "source": [
        "import tensorflow.keras as K\n",
        "\n",
        "class PoolBN(K.layers.Layer):\n",
        "  \"\"\"\n",
        "  Description\n",
        "  ---------------\n",
        "  This class is used to handle both the max pooling layer and the average pooling layer.\n",
        "  In both cases, the pooling is followed by a layer of batch normalization.\n",
        "  \"\"\"\n",
        "  def __init__(self, pool_type, C, kernel_size, stride, padding):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    Creates an instance of the class PoolBN\n",
        "\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    pool_type: string (either \"max\" or \"avg\")\n",
        "    C: int, axis on which to perform batch normalization\n",
        "    kernel_size: int or enumerable of ints\n",
        "    stride: int or enumerable of ints\n",
        "    padding: int\n",
        "\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.pool_type = pool_type\n",
        "    self.channels = C\n",
        "    self.kernel_size = kernel_size\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "    # self.architecture = K.models.Sequential()\n",
        "    if pool_type == \"max\":\n",
        "      # self.architecture.add(K.layers.MaxPooling2D(kernel_size, stride, padding))\n",
        "      self.pool = K.layers.MaxPooling2D(kernel_size, stride, padding)\n",
        "    elif pool_type == \"avg\":\n",
        "      # self.architecture.add(K.layers.AveragePooling2D(kernel_size, stride, padding))\n",
        "      self.pool = K.layers.AveragePooling2D(kernel_size, stride, padding)\n",
        "    else:\n",
        "      raise ValueError()\n",
        "    self.BN = K.layers.BatchNormalization()\n",
        "  \n",
        "  def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"pool_type\": self.pool_type,\n",
        "            \"C\": self.channels,\n",
        "            \"kernel_size\": self.kernel_size,\n",
        "            \"stride\": self.stride,\n",
        "            \"padding\": self.padding,\n",
        "        })\n",
        "        return config\n",
        "  \n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    ---------------\n",
        "    Similar to the forward function in tensorflow, this function is used to train the AI.\n",
        "\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x : tf.Tensor\n",
        "\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    # print('pooling',x)\n",
        "    out = self.pool(x)\n",
        "    out = self.BN(out)\n",
        "    return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hdzecPjMautn"
      },
      "source": [
        "Test of the pool BN method"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Kj9xp-4kZOUO",
        "outputId": "7c998a45-9cc4-470c-d131-1a1cefe95ba8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 2s 2s/step - loss: 1.0000\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " pool_bn (PoolBN)            (None, 3, 3, 3)           12        \n",
            "                                                                 \n",
            " pool_bn_1 (PoolBN)          (None, 3, 3, 3)           12        \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24\n",
            "Trainable params: 12\n",
            "Non-trainable params: 12\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "test_tensors = tf.ones((3,3,3,3))\n",
        "test_tensors_2 = tf.ones((3,3,3,3))\n",
        "test_model = K.models.Sequential()\n",
        "# test_pool = PoolBN(\"max\", 3, 3, 1, \"same\")\n",
        "test_model.add(PoolBN(\"max\", 3, 3, 1, \"same\"))\n",
        "test_model.add(PoolBN(\"avg\", 3, 3, 1, \"same\"))\n",
        "# test_model.add(SepConv(3,3,3,1,'valid'))\n",
        "\n",
        "# test_model.add(Layer()) # this is apparently a identity layer\n",
        "test_model.compile(loss = \"mse\")\n",
        "test_model.fit(test_tensors, tf.ones(3,3))\n",
        "test_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qKZsbt9MbwDu"
      },
      "source": [
        "## Drop path\n",
        "\n",
        "Changes to probability of keeping the path in later iterations (not used here, except for experimentations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bEQHiBM8_eWH"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "import tensorflow_probability as tfp\n",
        "\n",
        "class drop_path(K.layers.Layer):\n",
        "  def __init__(self, p=0):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    Creates an instance of the class drop_path.\n",
        "\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    p: float (0 if you don't want, else, between 0 and 1)\n",
        "\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.p=p\n",
        "  \n",
        "  def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"p\": self.p,\n",
        "        })\n",
        "        return config\n",
        "  \n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    ---------------\n",
        "    Similar to the forward function in tensorflow, this function is used to train the AI.\n",
        "\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x : tf.Tensor\n",
        "\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    if self._trainable and self.p>0:\n",
        "      keep_prob = 1. - self.p\n",
        "      shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
        "      mask = K.backend.random_bernoulli(shape, keep_prob, dtype = tf.float32)\n",
        "      return x/keep_prob*mask\n",
        "    return x\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uy5ufYToA9HS",
        "outputId": "9cf5c3ac-e2b3-4d00-dbbb-8944ffcab245"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 613ms/step - loss: 1.0000\n",
            "Model: \"sequential_1\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " pool_bn_2 (PoolBN)          (None, 3, 3, 3)           12        \n",
            "                                                                 \n",
            " pool_bn_3 (PoolBN)          (None, 2, 2, 3)           12        \n",
            "                                                                 \n",
            " drop_path (drop_path)       (None, 2, 2, 3)           0         \n",
            "                                                                 \n",
            " layer (Layer)               (None, 2, 2, 3)           0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 24\n",
            "Trainable params: 12\n",
            "Non-trainable params: 12\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "nb_input = 10\n",
        "test_tensors = tf.ones((nb_input,3,3,3))\n",
        "test_model = K.models.Sequential()\n",
        "test_model.add(PoolBN(\"max\", 3, 3, 1, \"same\"))\n",
        "test_model.add(PoolBN(\"max\", 3, 3, 2, \"same\"))\n",
        "test_model.add(drop_path(0.3))\n",
        "\n",
        "test_model.add(tf.keras.layers.Layer()) # this is an identity layer\n",
        "test_model.compile(loss = \"mse\")\n",
        "test_model.fit(test_tensors, tf.ones((nb_input,2,2,3)))\n",
        "test_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RKJU1xXydH4G"
      },
      "source": [
        "## Factorized reduce\n",
        "\n",
        "Reduce the size of the model using factorized pointwise convolution (replaces identity in reduction cells)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_qSebkgKFFiv"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "class FactorizedReduce(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Reduce feature map size by factorized pointwise (stride=2).\n",
        "  \"\"\"\n",
        "  def __init__(self, C_in, C_out, affine=True):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function initializes an instance of the FactorizedReduce class.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    C_in: int\n",
        "    C_out: int\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    self.c_in = C_in\n",
        "    self.c_out = C_out\n",
        "    self.affine = affine\n",
        "    super().__init__()\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "    self.conv2d_1 = tf.keras.layers.Conv2D(filters=C_out // 2 + C_out % 2,\n",
        "                                      strides=2,\n",
        "                                      padding='same' ,\n",
        "                                      data_format=\"channels_last\",\n",
        "                                      kernel_size=3,\n",
        "                                      use_bias=False)\n",
        "    self.conv2d_2 = tf.keras.layers.Conv2D(filters=C_out // 2,\n",
        "                                      strides=2,\n",
        "                                      padding='same' ,\n",
        "                                      data_format=\"channels_last\",\n",
        "                                      kernel_size=3,\n",
        "                                      use_bias=False)\n",
        "    self.concatenation = tf.keras.layers.Concatenate()\n",
        "    self.batch_normalisation = tf.keras.layers.BatchNormalization()\n",
        "  \n",
        "  def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"C_in\": self.c_in,\n",
        "            \"C_out\": self.c_out,\n",
        "            \"affine\": self.affine,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function initializes an instance of the FactorizedReduce class.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    C_in: int\n",
        "    C_out: int\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    x2 = tf.keras.layers.ReLU()(x)\n",
        "    # print(\"factorized reduce x2\", x2)\n",
        "    # print(\"factorized reduce x2.shape\", x2.shape)\n",
        "    out_1 = self.conv2d_1(x2)\n",
        "    out_2 = self.conv2d_2(x2[:,1:,1:,:])\n",
        "    # print(\"out_1\", out_1)\n",
        "    # print(\"out_2\", out_2)\n",
        "    out = self.concatenation([out_1,out_2])\n",
        "    out = self.batch_normalisation(out)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iIFuMQtdZpdF",
        "outputId": "fac8de20-6bcb-4b71-9c44-10aff6ecc197"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_channels : 80 \n",
            "channels : 20\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0000\n",
            "Model: \"sequential_13\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " pool_bn_15 (PoolBN)         (None, 100, 100, 80)      320       \n",
            "                                                                 \n",
            " pool_bn_16 (PoolBN)         (None, 50, 50, 80)        320       \n",
            "                                                                 \n",
            " factorized_reduce_8 (Factor  (None, 25, 25, 20)       14480     \n",
            " izedReduce)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 15,120\n",
            "Trainable params: 14,760\n",
            "Non-trainable params: 360\n",
            "_________________________________________________________________\n",
            "input_channels : 40 \n",
            "channels : 40\n",
            "1/1 [==============================] - 1s 994ms/step - loss: 1.0000\n",
            "Model: \"sequential_14\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " pool_bn_17 (PoolBN)         (None, 100, 100, 40)      160       \n",
            "                                                                 \n",
            " pool_bn_18 (PoolBN)         (None, 50, 50, 40)        160       \n",
            "                                                                 \n",
            " factorized_reduce_9 (Factor  (None, 25, 25, 40)       14560     \n",
            " izedReduce)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 14,880\n",
            "Trainable params: 14,640\n",
            "Non-trainable params: 240\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "nb_input = 10\n",
        "shape_1 = 100\n",
        "shape_2 = 100\n",
        "## channels_inside = 10, channels input = 40\n",
        "channels = 20\n",
        "input_channels = 80\n",
        "print(\"input_channels :\", input_channels, \"\\nchannels :\", channels)\n",
        "test_tensors = tf.ones((nb_input,shape_1,shape_2,input_channels))\n",
        "test_model = K.models.Sequential()\n",
        "# test_pool = PoolBN(\"max\", 3, 3, 1, \"same\")\n",
        "test_model.add(PoolBN(\"max\", input_channels, channels, 1, \"same\"))\n",
        "test_model.add(PoolBN(\"max\", channels, channels, 2, \"same\"))\n",
        "test_model.add(FactorizedReduce(channels,channels))\n",
        "# test_model.add(drop_path(0.3))\n",
        "# test_model.add(SepConv(3,3,3,1,'valid'))\n",
        "\n",
        "# test_model.add(Layer()) # this is apparently a identity layer\n",
        "test_model.compile(loss = \"mse\")\n",
        "test_model.fit(test_tensors, tf.ones((nb_input,shape_1//4,shape_2//4,channels)))\n",
        "test_model.summary()\n",
        "\n",
        "## channels_inside = 10, channels input = 40\n",
        "channels = 40\n",
        "input_channels = 40\n",
        "print(\"input_channels :\", input_channels, \"\\nchannels :\", channels)\n",
        "test_tensors = tf.ones((nb_input,shape_1,shape_2,input_channels))\n",
        "test_model = K.models.Sequential()\n",
        "# test_pool = PoolBN(\"max\", 3, 3, 1, \"same\")\n",
        "test_model.add(PoolBN(\"max\", input_channels, channels, 1, \"same\"))\n",
        "test_model.add(PoolBN(\"max\", channels, channels, 2, \"same\"))\n",
        "test_model.add(FactorizedReduce(channels,channels))\n",
        "# test_model.add(drop_path(0.3))\n",
        "# test_model.add(SepConv(3,3,3,1,'valid'))\n",
        "\n",
        "# test_model.add(Layer()) # this is apparently a identity layer\n",
        "test_model.compile(loss = \"mse\")\n",
        "test_model.fit(test_tensors, tf.ones((nb_input,shape_1//4,shape_2//4,channels)))\n",
        "test_model.summary()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "nb_input = 10\n",
        "shape_1 = 100\n",
        "shape_2 = 100\n",
        "## channels_inside = 10, channels input = 40\n",
        "channels = 20\n",
        "input_channels = 80\n",
        "print(\"input_channels :\", input_channels, \"\\nchannels :\", channels)\n",
        "test_tensors = tf.ones((nb_input,shape_1,shape_2,input_channels))\n",
        "test_model = K.models.Sequential()\n",
        "# test_pool = PoolBN(\"max\", 3, 3, 1, \"same\")\n",
        "test_model.add(DilConv(channels, channels, 1, 1, \"same\",1))\n",
        "test_model.add(PoolBN(\"max\", channels, channels, 2, \"same\"))\n",
        "test_model.add(FactorizedReduce(channels,channels))\n",
        "# test_model.add(drop_path(0.3))\n",
        "# test_model.add(SepConv(3,3,3,1,'valid'))\n",
        "\n",
        "# test_model.add(Layer()) # this is apparently a identity layer\n",
        "test_model.compile(loss = \"mse\")\n",
        "test_model.fit(test_tensors, tf.ones((nb_input,shape_1//4,shape_2//4,channels)))\n",
        "test_model.summary()\n",
        "\n",
        "## channels_inside = 10, channels input = 40\n",
        "channels = 40\n",
        "input_channels = 40\n",
        "print(\"input_channels :\", input_channels, \"\\nchannels :\", channels)\n",
        "test_tensors = tf.ones((nb_input,shape_1,shape_2,input_channels))\n",
        "test_model = K.models.Sequential()\n",
        "# test_pool = PoolBN(\"max\", 3, 3, 1, \"same\")\n",
        "test_model.add(DilConv(channels, channels, 1, 1, \"same\",1))\n",
        "test_model.add(PoolBN(\"max\", channels, channels, 2, \"same\"))\n",
        "test_model.add(FactorizedReduce(channels,channels))\n",
        "# test_model.add(drop_path(0.3))\n",
        "# test_model.add(SepConv(3,3,3,1,'valid'))\n",
        "\n",
        "# test_model.add(Layer()) # this is apparently a identity layer\n",
        "test_model.compile(loss = \"mse\")\n",
        "test_model.fit(test_tensors, tf.ones((nb_input,shape_1//4,shape_2//4,channels)))\n",
        "test_model.summary()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3_cb3IBzQ5P2",
        "outputId": "eb008cff-3ff7-457a-b874-b8b55d5b1bfc"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "input_channels : 80 \n",
            "channels : 20\n",
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x7fdf1a52c8c0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
            "1/1 [==============================] - 2s 2s/step - loss: 1.0000\n",
            "Model: \"sequential_9\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dil_conv_1 (DilConv)        (None, 100, 100, 20)      2120      \n",
            "                                                                 \n",
            " pool_bn_13 (PoolBN)         (None, 50, 50, 20)        80        \n",
            "                                                                 \n",
            " factorized_reduce_6 (Factor  (None, 25, 25, 20)       3590      \n",
            " izedReduce)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 5,790\n",
            "Trainable params: 5,670\n",
            "Non-trainable params: 120\n",
            "_________________________________________________________________\n",
            "input_channels : 40 \n",
            "channels : 40\n",
            "1/1 [==============================] - 1s 1s/step - loss: 1.0000\n",
            "Model: \"sequential_11\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " dil_conv_2 (DilConv)        (None, 100, 100, 40)      3440      \n",
            "                                                                 \n",
            " pool_bn_14 (PoolBN)         (None, 50, 50, 40)        160       \n",
            "                                                                 \n",
            " factorized_reduce_7 (Factor  (None, 25, 25, 40)       14380     \n",
            " izedReduce)                                                     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 17,980\n",
            "Trainable params: 17,740\n",
            "Non-trainable params: 240\n",
            "_________________________________________________________________\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-obFg-uQdkQO"
      },
      "source": [
        "## Convolutions\n",
        "\n",
        "Alternatives to convolutions to use in the model (based on dilation)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Dilated convolution"
      ],
      "metadata": {
        "id": "e8LzAZg9L10b"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nrhhp55PA4G0"
      },
      "outputs": [],
      "source": [
        "class DilConv(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  This class is the dilated depthwise separable convolution class.\n",
        "  \"\"\"\n",
        "  def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function initializes an instance of the DilConv class.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    C_in: int\n",
        "    C_out: int\n",
        "    kernel_size: int\n",
        "    stride: int\n",
        "    padding: int\n",
        "    dilation: int\n",
        "    affine: boolean\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.c_in = C_in\n",
        "    self.c_out = C_out\n",
        "    self.k_size = kernel_size\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "    self.dilation = dilation\n",
        "    self.affine = affine\n",
        "\n",
        "    self.net = tf.keras.Sequential()\n",
        "    self.net.add(tf.keras.layers.ReLU())\n",
        "    self.net.add(tf.keras.layers.Conv2D(C_in, kernel_size, strides=stride, dilation_rate=dilation, padding=\"same\",  data_format=\"channels_last\" ) )\n",
        "    self.net.add(tf.keras.layers.Conv2D(C_out, kernel_size, strides=(1, 1), padding=padding, data_format=\"channels_last\"))\n",
        "    self.net.add(tf.keras.layers.BatchNormalization() )\n",
        "  \n",
        "  def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"C_in\": self.c_in,\n",
        "            \"C_out\": self.c_out,\n",
        "            \"kernel_size\": self.k_size,\n",
        "            \"stride\": self.stride,\n",
        "            \"padding\": self.padding,\n",
        "            \"dilation\": self.dilation,\n",
        "            \"affine\": self.affine,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function applies the instance of the DilConv class to the data.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x: tf.Tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    # print(\"Dilconv\",x)\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Separable convolution"
      ],
      "metadata": {
        "id": "XeN3py_mLyjy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "999KkfKMA5Cz"
      },
      "outputs": [],
      "source": [
        "class SepConv(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Depthwise separable conv.\n",
        "  DilConv(dilation=1) * 2.\n",
        "  \"\"\"\n",
        "  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function initializes an instance of the SepConv class.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    C_in: int\n",
        "    C_out: int\n",
        "    kernel_size: int\n",
        "    stride: int\n",
        "    padding: int\n",
        "    affine: boolean\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    \n",
        "    self.c_in = C_in\n",
        "    self.c_out = C_out\n",
        "    self.k_size = kernel_size\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "    self.affine = affine\n",
        "\n",
        "    self.net = tf.keras.Sequential()\n",
        "    self.net.add(DilConv(C_in, C_in, kernel_size, stride, padding, dilation=1,affine=affine))\n",
        "    self.net.add(DilConv(C_in, C_out, kernel_size, 1, padding, dilation=1, affine=affine))\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"C_in\": self.c_in,\n",
        "            \"C_out\": self.c_out,\n",
        "            \"kernel_size\": self.k_size,\n",
        "            \"stride\": self.stride,\n",
        "            \"padding\": self.padding,\n",
        "            \"affine\": self.affine,\n",
        "        })\n",
        "        return config\n",
        "  \n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function applies the instance of the SepConv class to the data.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x: tf.Tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    # print(\"sepconv\",x)\n",
        "    return self.net(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Standard convolution"
      ],
      "metadata": {
        "id": "1MJQzQa8LvMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class StdConv(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Description\n",
        "  ---------------\n",
        "  This class is used to do an average convolution.\n",
        "  It is composed of a relu followed by the convolution.\n",
        "  Then a batchnormalisation is applied.\n",
        "  The model used is a sequential keras model.\n",
        "  \"\"\"\n",
        "  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    Creates an instance of the class StdConv\n",
        "\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    C_in: int\n",
        "    C_out: int\n",
        "    kernel_size: int\n",
        "    stride: int\n",
        "    padding: string (\"same\" ou \"valid\")\n",
        "    affine: bool\n",
        "\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    # print(\"-----------entrée dans la fonction __init__ de la classe StdConv\")\n",
        "    super().__init__()\n",
        "    \n",
        "    self.c_in = C_in\n",
        "    self.k_size = kernel_size\n",
        "    self.stride = stride\n",
        "    self.padding = padding\n",
        "    self.affine = affine\n",
        "\n",
        "    self.C_out = C_out\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "    self.conv2d = tf.keras.layers.Conv2D(filters=C_out,\n",
        "                                        kernel_size=kernel_size,\n",
        "                                        strides=stride,\n",
        "                                        padding=padding, \n",
        "                                        use_bias=False )\n",
        "    self.batch_normalisation = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"C_in\": self.c_in,\n",
        "            \"C_out\": self.C_out,\n",
        "            \"kernel_size\": self.k_size,\n",
        "            \"stride\": self.stride,\n",
        "            \"padding\": self.padding,\n",
        "            \"affine\": self.affine,\n",
        "        })\n",
        "        return config\n",
        "    \n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    ---------------\n",
        "    Similar to the forward function in tensorflow, this function is used to train the AI.\n",
        "\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x : tf.Tensor\n",
        "\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    # print(\"-----------------entrée dans la fonction call de la classe StdConv --------------\")\n",
        "    # print(\"x = {}\".format(x))\n",
        "    # x is a Tensor\n",
        "    # print(\"x.get_shape() = {}\".format(x.get_shape()))\n",
        "\n",
        "    # print(\"x shape std conv\", x.shape)\n",
        "    out_relu = self.relu(x)\n",
        "    # print(\"C_out\", self.C_out)\n",
        "    # print(\"out_relu shape std conv\", out_relu.shape)\n",
        "    # print(\"out_relu = {}\".format(out_relu))\n",
        "    # print(\"out_relu.get_shape()\".format(out_relu.get_shape()))\n",
        "    out_conv2d = self.conv2d(out_relu)\n",
        "    # print(\"out_conv2d = {}\".format(out_conv2d))\n",
        "    # print(\"out_conv2d.get_shape() = {}\".format(out_conv2d.get_shape()))\n",
        "    out_batch_normalisation = self.batch_normalisation(out_conv2d)\n",
        "    # print(\"out_batch_normalisation = {}\".format(out_batch_normalisation))\n",
        "    # print(\"out_batch_normalisation.get_shape() = {}\".format(out_batch_normalisation.get_shape()))\n",
        "    out = out_batch_normalisation\n",
        "    \n",
        "    # print(\"stdconv = {}\".format(out))\n",
        "    return out\n"
      ],
      "metadata": {
        "id": "TypzlnRb6keV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OtmEyLMlaLJh",
        "outputId": "44dcac15-286b-4c68-d93c-e48912d17e02"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 6s 6s/step - loss: 1.6111\n",
            "Model: \"sequential_3\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " sep_conv (SepConv)          (None, 120, 120, 30)      27090     \n",
            "                                                                 \n",
            " dil_conv_2 (DilConv)        (None, 120, 120, 20)      25120     \n",
            "                                                                 \n",
            " std_conv (StdConv)          (None, 120, 120, 10)      5040      \n",
            "                                                                 \n",
            " std_conv_1 (StdConv)        (None, 120, 120, 20)      5080      \n",
            "                                                                 \n",
            " pool_bn_6 (PoolBN)          (None, 120, 120, 20)      80        \n",
            "                                                                 \n",
            " pool_bn_7 (PoolBN)          (None, 60, 60, 20)        80        \n",
            "                                                                 \n",
            " factorized_reduce_1 (Factor  (None, 30, 30, 3)        4791      \n",
            " izedReduce)                                                     \n",
            "                                                                 \n",
            " drop_path_2 (drop_path)     (None, 30, 30, 3)         0         \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 67,281\n",
            "Trainable params: 66,975\n",
            "Non-trainable params: 306\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "nb_input = 10\n",
        "test_tensors = tf.ones((nb_input,120,120,9))\n",
        "test_model = K.models.Sequential()\n",
        "# test_pool = PoolBN(\"max\", 3, 3, 1, \"same\")\n",
        "test_model.add(SepConv(30,30,3,1,\"same\"))\n",
        "test_model.add(DilConv(20,20,5,1,\"same\",1))\n",
        "test_model.add(StdConv(10,10,5,1,\"same\"))\n",
        "test_model.add(StdConv(20,20,5,1,\"same\"))\n",
        "test_model.add(PoolBN(\"max\", 3, 3, 1, \"same\"))\n",
        "test_model.add(PoolBN(\"avg\", 3, 3, 2, \"same\"))\n",
        "test_model.add(FactorizedReduce(9,3))\n",
        "test_model.add(drop_path(0.3))\n",
        "# test_model.add(SepConv(3,3,3,1,'valid'))\n",
        "\n",
        "# test_model.add(Layer()) # this is apparently a identity layer\n",
        "test_model.compile(loss = \"mse\")\n",
        "test_model.fit(test_tensors, tf.ones((nb_input,30,30,3)))\n",
        "test_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mEmNujm6eZQG"
      },
      "source": [
        "# Model creation\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OHtun761ekow"
      },
      "source": [
        "\n",
        "## Importing the right plans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjAVW5kEBEtW",
        "outputId": "786b7232-66d3-46fb-90bd-8f191b1dbe37"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[3, 4]\n",
            "{'0': 'sepconv3x3', '1': 'sepconv5x5'}\n"
          ]
        }
      ],
      "source": [
        "input_dict_1 = {\n",
        "    'normal_n2_p0': 'maxpool', \n",
        "    'normal_n2_p1': 'maxpool', \n",
        "    'normal_n3_p0': 'maxpool', \n",
        "    'normal_n3_p1': 'maxpool', \n",
        "    'normal_n3_p2': 'maxpool', \n",
        "    'normal_n4_p0': 'maxpool', \n",
        "    'normal_n4_p1': 'maxpool', \n",
        "    'normal_n4_p2': 'maxpool', \n",
        "    'normal_n4_p3': 'maxpool', \n",
        "    'normal_n5_p0': 'maxpool', \n",
        "    'normal_n5_p1': 'maxpool', \n",
        "    'normal_n5_p2': 'maxpool', \n",
        "    'normal_n5_p3': 'maxpool', \n",
        "    'normal_n5_p4': 'maxpool',\n",
        "    'reduce_n2_p0': 'maxpool', \n",
        "    'reduce_n2_p1': 'maxpool', \n",
        "    'reduce_n3_p0': 'maxpool', \n",
        "    'reduce_n3_p1': 'sepconv5x5', \n",
        "    'reduce_n3_p2': 'maxpool', \n",
        "    'reduce_n4_p0': 'maxpool', \n",
        "    'reduce_n4_p1': 'maxpool', \n",
        "    'reduce_n4_p2': 'dilconv5x5', \n",
        "    'reduce_n4_p3': 'maxpool', \n",
        "    'reduce_n5_p0': 'maxpool', \n",
        "    'reduce_n5_p1': 'sepconv5x5', \n",
        "    'reduce_n5_p2': 'maxpool', \n",
        "    'reduce_n5_p3': 'dilconv5x5', \n",
        "    'reduce_n5_p4': 'maxpool', \n",
        "    'normal_n2_switch': [1, 0], \n",
        "    'normal_n3_switch': [2, 1], \n",
        "    'normal_n4_switch': [3, 2], \n",
        "    'normal_n5_switch': [2, 4], \n",
        "    'reduce_n2_switch': [1, 0], \n",
        "    'reduce_n3_switch': [2, 1], \n",
        "    'reduce_n4_switch': [3, 2], \n",
        "    'reduce_n5_switch': [3, 4] \n",
        "}\n",
        "input_dict_norm_2 = {\"2\": {\"0\": \"sepconv3x3\", \"1\": \"sepconv5x5\"}, \"3\": {\"1\": \"sepconv5x5\", \"0\": \"maxpool\"}, \"4\": {\"0\": \"avgpool\", \"2\": \"dilconv5x5\"}, \"5\": {\"1\": \"sepconv3x3\", \"0\": \"sepconv3x3\"}}\n",
        "input_dict_redu_2 = {\"2\": {\"1\": \"maxpool\", \"0\": \"maxpool\"}, \"3\": {\"2\": \"maxpool\", \"1\": \"maxpool\"}, \"4\": {\"3\": \"maxpool\", \"2\": \"maxpool\"}, \"5\": {\"3\": \"maxpool\", \"2\": \"maxpool\"}}\n",
        "input_dict_norm_1 = {2: {1: 'maxpool', 0: 'maxpool'}, 3: {2: 'maxpool', 1: 'maxpool'}, 4: {3: 'maxpool', 2: 'maxpool'}, 5: {2: 'maxpool', 4: 'maxpool'}}\n",
        "input_dict_redu_1 = {2: {1: 'maxpool', 0: 'maxpool'}, 3: {2: 'maxpool', 1: 'sepconv5x5'}, 4: {3: 'maxpool', 2: 'dilconv5x5'}, 5: {3: 'dilconv5x5', 4: 'maxpool'}}\n",
        "input_dict_norm_test = {2: {0:\"maxpool\", 1: \"avgpool\"}, 3: {0:\"skipconnect\", 1: \"avgpool\"}, 4: {0:\"sepconv3x3\", 1: \"sepconv5x5\"}, 5: {0:\"dilconv3x3\", 1: \"dilconv5x5\"}}\n",
        "input_dict_redu_test = {2: {0:\"maxpool\", 1: \"avgpool\"}, 3: {0:\"skipconnect\", 1: \"avgpool\"}, 4: {0:\"sepconv3x3\", 1: \"sepconv5x5\"}, 5: {0:\"dilconv3x3\", 1: \"dilconv5x5\"}}\n",
        "print(input_dict_1[\"reduce_n5_switch\"])\n",
        "print(input_dict_norm_2[\"2\"])\n",
        "input_dict_norm_3 = {\"2\": {\"1\": \"maxpool\", \"0\": \"maxpool\"}, \"3\": {\"2\": \"maxpool\", \"1\": \"maxpool\"}, \"4\": {\"2\": \"maxpool\", \"3\": \"maxpool\"}, \"5\": {\"4\": \"maxpool\", \"3\": \"maxpool\"}}\n",
        "input_dict_redu_3 = {\"2\": {\"0\": \"maxpool\", \"1\": \"maxpool\"}, \"3\": {\"2\": \"maxpool\", \"0\": \"maxpool\"}, \"4\": {\"3\": \"maxpool\", \"2\": \"maxpool\"}, \"5\": {\"4\": \"dilconv5x5\", \"3\": \"maxpool\"}}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2VXPiYp9em_h"
      },
      "source": [
        "## Classes\n",
        "\n",
        "Creation of the Node, Cell and CNN classes (as layers) to use"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxg6Ay8gIHRk",
        "outputId": "9092a177-7a98-4980-d979-d1ff7940303c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<__main__.Cell at 0x7fdf9133be10>"
            ]
          },
          "metadata": {},
          "execution_count": 18
        }
      ],
      "source": [
        "# for my custom version (cleaner and simpler + conversion already exists)\n",
        "\n",
        "class Node(tf.keras.layers.Layer):\n",
        "  def __init__(self, origin, element, channels, reducing):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    Creates an instance of a node, the smallest base component of the CNN architecture.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    origin: str (can be converted to int) : The id of the node being created\n",
        "    element: dict : The plan of this specific node\n",
        "    channels: int : Number of channels to use (must be the same for each node for compatibility reasons)\n",
        "    reducing: boolean : Whether this node is used for reduction or not\n",
        "    prev_reduction: boolean : Whether the previous node was a reduction node or not\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.origin = origin\n",
        "    self.element = element\n",
        "    self.channels = channels\n",
        "    self.red = reducing\n",
        "    self.models = dict()\n",
        "    \"\"\"\n",
        "    self.preproc = tf.keras.layers.Layer()\n",
        "    if prev_reduction:\n",
        "      self.preproc = FactorizedReduce(channels, channels)\n",
        "    \"\"\"\n",
        "    for i in element:\n",
        "      stride = 2 if reducing and int(i) <=1 else 1\n",
        "      dict_elements = {\n",
        "          \"maxpool\":PoolBN(\"max\", channels, 3, stride, \"same\"),\n",
        "          \"avgpool\":PoolBN(\"avg\", channels, 3, stride, \"same\"),\n",
        "          \"skipconnect\":tf.keras.layers.Layer() if stride == 1 else FactorizedReduce(channels, channels),\n",
        "          \"sepconv3x3\":SepConv(channels, channels, 3, stride, \"same\"),\n",
        "          \"sepconv5x5\":SepConv(channels, channels, 5, stride, \"same\"),\n",
        "          \"dilconv3x3\":DilConv(channels, channels, 3, stride, \"same\", 1),\n",
        "          \"dilconv5x5\":DilConv(channels, channels, 5, stride, \"same\", 1) # Impossible to have a dilation greater than 1 if stride is not 1\n",
        "      }\n",
        "      self.models[int(i)] = dict_elements[element[i]]\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"origin\": self.origin,\n",
        "            \"reduction\": self.red,\n",
        "            \"element\": self.element,\n",
        "            \"channels\": self.channels,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    Function called when using the Node layer.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x: tf.Tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    out: tf.Tensor\n",
        "    \"\"\"\n",
        "    outs = []\n",
        "    inputs = x\n",
        "    i=0\n",
        "    for key in self.models.keys():\n",
        "      outs.append(self.models[key](inputs[key]))\n",
        "      i+=1\n",
        "    out = tf.keras.layers.Add()(outs)\n",
        "    return out \n",
        "\n",
        "class Cell(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Description\n",
        "  ---------------\n",
        "  Creates and returns a cell, according to a plan given as an input.\n",
        "  Input(s)\n",
        "  ---------------\n",
        "  plan : dict containing the plan of the cell\n",
        "  reduction : Model of the previous cell\n",
        "  prev_reduction : Model of the cell before the previous cell\n",
        "  Output(s)\n",
        "  ---------------\n",
        "  output_cell : Model of the normal cell\n",
        "  \"\"\"\n",
        "  def __init__(self, plan, channels, reduction, prev_reduction):\n",
        "    super().__init__()\n",
        "    # Initiate the first two inputs => to add in the call\n",
        "    # inputs = [prev_prev_cell, prev_cell]\n",
        "    self.plan = plan\n",
        "    self.chan = channels\n",
        "    self.red = reduction\n",
        "    self.prev_red = prev_reduction\n",
        "    self.nodes = dict()\n",
        "    if prev_reduction:\n",
        "      self.preproc0 = FactorizedReduce(channels, channels)\n",
        "    else:\n",
        "      self.preproc0 = StdConv(channels, channels, 1, 1, \"same\")\n",
        "    self.preproc1 = StdConv(channels, channels, 1, 1, \"same\")\n",
        "    self.prev_reduction = prev_reduction\n",
        "    for origin in plan.keys():\n",
        "      # print(element)\n",
        "      # print(origin)\n",
        "      node = Node(origin, plan[origin], channels, reduction)\n",
        "      self.nodes[int(origin)] = node\n",
        "\n",
        "  def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"plan\": self.plan,\n",
        "            \"channels\": self.chan,\n",
        "            \"reduction\": self.red,\n",
        "            \"prev_reduction\": self.prev_red,\n",
        "        })\n",
        "        return config\n",
        "\n",
        "  def call(self, x):\n",
        "    # inputs = tf.keras.Input(x)\n",
        "    output = [self.preproc0(x[0]),self.preproc1(x[1])]\n",
        "    for i in self.nodes:\n",
        "      # print(\"node_nb\",i)\n",
        "      # print(\"self.nodes\",self.nodes)\n",
        "      output.append(self.nodes[i](output))\n",
        "    # print(\"cell output\", output[2:])\n",
        "    added = tf.keras.layers.Concatenate(axis = -1)(output[2:])\n",
        "    # print(\"added\", added)\n",
        "    # model = tf.keras.Model(inputs = x, outputs = output)\n",
        "    return added\n",
        "\n",
        "class CNN(tf.keras.layers.Layer):\n",
        "  def __init__(self, channels, n_classes, n_layers, normal_plan, reduction_plan, stem_multiplier=3):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    Creates an instance of a CNN layer.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    channels: int : Number of channels in the internal outputs, must be equal to the number of channels of the input\n",
        "    n_classes: int : number of classes we want to final output to have through a softmax\n",
        "    n_layers: int : number of cells used in the model\n",
        "    normal_plan: dict : plan to construct normal cells\n",
        "    reduction_plan: dict: plan to construct reduction cells\n",
        "    stem_multiplier: int : multiplier for the beginning\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.channels = channels\n",
        "    self.n_classes = n_classes\n",
        "    self.n_layers = n_layers\n",
        "    self.normal_plan = normal_plan\n",
        "    self.red_plan = reduction_plan\n",
        "    self.stem_mul = stem_multiplier\n",
        "    # Start : First layers\n",
        "    cur_chan = stem_multiplier * channels\n",
        "    self.stem = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(cur_chan, 3, strides=(1, 1), padding=\"same\", data_format=\"channels_last\"),\n",
        "        tf.keras.layers.BatchNormalization(axis = -1)]\n",
        "    )\n",
        "    cur_chan = channels\n",
        "    reduction_prev , reduction = False, False\n",
        "    self.cells = []\n",
        "    for i in range(n_layers):\n",
        "      reduction_prev, reduction = reduction, False\n",
        "      if i in [n_layers // 3, 2 * n_layers // 3]:\n",
        "        cur_chan *= 2\n",
        "        reduction=True\n",
        "      \n",
        "      # print(\"i\",i,\"red\", reduction, )\n",
        "      if reduction:\n",
        "        cell = Cell(reduction_plan, cur_chan, reduction = True, prev_reduction = reduction_prev)\n",
        "      else:\n",
        "        cell = Cell(normal_plan, cur_chan, reduction = False, prev_reduction = reduction_prev)\n",
        "      self.cells.append(cell)\n",
        "    self.gap = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    self.linear = tf.keras.layers.Dense(n_classes, activation = \"softmax\")\n",
        "  \n",
        "  def get_config(self):\n",
        "        config = super().get_config()\n",
        "        config.update({\n",
        "            \"n_classes\": self.n_classes,\n",
        "            \"n_layers\": self.n_layers,\n",
        "            \"stem_multiplier\": self.stem_mul,\n",
        "            \"channels\": self.channels,\n",
        "            \"normal_plan\": self.normal_plan,\n",
        "            \"reduction_plan\": self.red_plan,\n",
        "        })\n",
        "        return config\n",
        "  \n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    Function used for forward propagation.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x: Tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    out: Tensor\n",
        "    \"\"\"\n",
        "    # inputs = tf.keras.Input(x)\n",
        "    s0 = s1 = self.stem(x)\n",
        "    out = s1\n",
        "    prev_out = s0\n",
        "    i=0\n",
        "    for cell in self.cells:\n",
        "      # print(\"new cell\",i,\" ----------------------------------------------------------\")\n",
        "      prev_out,out = out,cell([prev_out,out])\n",
        "      i+=1\n",
        "    out = self.gap(out)\n",
        "    # print('left all cells')\n",
        "    out_flat = tf.keras.layers.Flatten()(out)\n",
        "    # print('left flatten')\n",
        "    out = self.linear(out_flat)\n",
        "    # print(\"final stretch\")\n",
        "    return out\n",
        "\n",
        "Cell(input_dict_norm_2, 3, False, False)\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Version with function over classes"
      ],
      "metadata": {
        "id": "-3khRoUlz8Kb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def Node_func(element, channels, reducing, input):\n",
        "  outs = []\n",
        "  inputs = input\n",
        "  for key in element:\n",
        "    stride = 2 if reducing and int(key)<2 else 1\n",
        "    dict_element = {\n",
        "      \"maxpool\":PoolBN(\"max\", channels, 3, stride, \"same\"),\n",
        "      \"avgpool\":PoolBN(\"avg\", channels, 3, stride, \"same\"),\n",
        "      \"skipconnect\":tf.keras.layers.Layer() if stride == 1 else FactorizedReduce(channels, channels),\n",
        "      \"sepconv3x3\":SepConv(channels, channels, 3, stride, \"same\"),\n",
        "      \"sepconv5x5\":SepConv(channels, channels, 5, stride, \"same\"),\n",
        "      \"dilconv3x3\":DilConv(channels, channels, 3, stride, \"same\", 1),\n",
        "      \"dilconv5x5\":DilConv(channels, channels, 5, stride, \"same\", 1) # Impossible to have a dilation greater than 1 if stride is not 1\n",
        "    }\n",
        "    outs.append(dict_element[element[key]](inputs[int(key)]))\n",
        "  # print('outs', outs)\n",
        "  added = tf.keras.layers.Add()(outs)\n",
        "  return added\n",
        "\n",
        "def Cell_func(plan, channels, reduction, prev_reduction, x0, x1):\n",
        "  preproc_1 = StdConv(channels, channels, 1, 1, \"same\")\n",
        "  preproc_0 = StdConv(channels, channels, 1, 1, \"same\")\n",
        "  if prev_reduction:\n",
        "    preproc_0 = FactorizedReduce(channels, channels)  \n",
        "  output = [preproc_0(x0), preproc_1(x1)]\n",
        "  for key in plan.keys():\n",
        "    output.append(Node_func(plan[key], channels, reduction, output))\n",
        "  out = tf.keras.layers.Concatenate(axis = -1)(output[2:])\n",
        "  return out\n",
        "\n",
        "def CNN_creation(channels, n_classes, n_layers, normal_plan, reduction_plan, input_shape, stem_multiplier=3):\n",
        "  in_x = tf.keras.Input(input_shape[1:])\n",
        "  stem = tf.keras.models.Sequential([\n",
        "        tf.keras.layers.Conv2D(channels*stem_multiplier, 3, strides=(1, 1), padding=\"same\", data_format=\"channels_last\"),\n",
        "        tf.keras.layers.BatchNormalization(axis = -1)]\n",
        "    )\n",
        "  out_0 = out_1 = stem(in_x)\n",
        "  red, prev_red = False, False\n",
        "  cur_chans = channels\n",
        "  for i in range(n_layers):\n",
        "    prev_red = red\n",
        "    red = i==n_layers//3 or i==2*n_layers//3\n",
        "    plan = normal_plan\n",
        "    if red :\n",
        "      plan = reduction_plan\n",
        "      cur_chans*=2\n",
        "    out_0, out_1 = out_1, Cell_func(plan, cur_chans, red, prev_red, out_0, out_1)\n",
        "  out = tf.keras.layers.GlobalAveragePooling2D()(out_1)\n",
        "  out = tf.keras.layers.Flatten()(out)\n",
        "  out = tf.keras.layers.Dense(n_classes, activation = \"softmax\")(out)\n",
        "  return tf.keras.Model(inputs = in_x, outputs = out)\n"
      ],
      "metadata": {
        "id": "-6pZ-FZsutfL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wCV_H1hCe4BP"
      },
      "source": [
        "### Example use of the model\n",
        "\n",
        "> The usage of the input_dict_test is to ensure all methods are working correctly\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 397
        },
        "id": "iK9vfoFuZANf",
        "outputId": "975d1382-ee47-42e9-c2ce-34da611500f1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "nb_channels 1 functions\n"
          ]
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-e76ae6567119>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0mtest_model_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimizer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m test_model_2.fit(test_tensors, tf.ones((nb_input,output_size)), \n\u001b[0;32m---> 35\u001b[0;31m           callbacks=[tensorboard_callback_2])\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0mtest_model_2\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msummary\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/utils/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     62\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 64\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     65\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint: disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1382\u001b[0m                 _r=1):\n\u001b[1;32m   1383\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/util/traceback_utils.py\u001b[0m in \u001b[0;36merror_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    148\u001b[0m     \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 150\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    151\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    152\u001b[0m       \u001b[0mfiltered_tb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_process_traceback_frames\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__traceback__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    913\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    914\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_jit_compile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 915\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    916\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    917\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    961\u001b[0m       \u001b[0;31m# This is the first call of __call__, so we have to initialize.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    962\u001b[0m       \u001b[0minitializers\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 963\u001b[0;31m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_initialize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0madd_initializers_to\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minitializers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    964\u001b[0m     \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    965\u001b[0m       \u001b[0;31m# At this point we know that the initialization is complete (or less\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_initialize\u001b[0;34m(self, args, kwds, add_initializers_to)\u001b[0m\n\u001b[1;32m    784\u001b[0m     self._concrete_stateful_fn = (\n\u001b[1;32m    785\u001b[0m         self._stateful_fn._get_concrete_function_internal_garbage_collected(  # pylint: disable=protected-access\n\u001b[0;32m--> 786\u001b[0;31m             *args, **kwds))\n\u001b[0m\u001b[1;32m    787\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0minvalid_creator_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0munused_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0munused_kwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2981\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2982\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2983\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2984\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2985\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   3290\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3291\u001b[0m           \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_call_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3292\u001b[0;31m           \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3293\u001b[0m           self._function_cache.add(cache_key, cache_key_deletion_observer,\n\u001b[1;32m   3294\u001b[0m                                    graph_function)\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   3138\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3139\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3140\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   3141\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3142\u001b[0m         \u001b[0mfunction_spec\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction_spec\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes, acd_record_initial_resource_uses)\u001b[0m\n\u001b[1;32m   1159\u001b[0m         \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moriginal_func\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_decorator\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munwrap\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpython_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1160\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1161\u001b[0;31m       \u001b[0mfunc_outputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpython_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mfunc_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mfunc_kwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1163\u001b[0m       \u001b[0;31m# invariant: `func_outputs` contains only Tensors, CompositeTensors,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36mwrapped_fn\u001b[0;34m(*args, **kwds)\u001b[0m\n\u001b[1;32m    675\u001b[0m         \u001b[0;31m# the function a weak reference to itself to avoid a reference cycle.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mOptionalXlaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcompile_with_xla\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 677\u001b[0;31m           \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mweak_wrapped_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__wrapped__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    678\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mout\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    679\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/func_graph.py\u001b[0m in \u001b[0;36mautograph_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m                     \u001b[0mrecursive\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1142\u001b[0m                     \u001b[0moptional_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mautograph_options\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1143\u001b[0;31m                     \u001b[0muser_requested\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1144\u001b[0m                 ))\n\u001b[1;32m   1145\u001b[0m           \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    437\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    438\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 439\u001b[0;31m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    440\u001b[0m       \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    441\u001b[0m         \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconverted_f\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0meffective_args\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtf__train_function\u001b[0;34m(iterator)\u001b[0m\n\u001b[1;32m     13\u001b[0m                 \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m                     \u001b[0mretval_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep_function\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mag__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mld\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfscope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m                 \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m                     \u001b[0mdo_return\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mstep_function\u001b[0;34m(model, iterator)\u001b[0m\n\u001b[1;32m   1008\u001b[0m             run_step, jit_compile=True, experimental_relax_shapes=True)\n\u001b[1;32m   1009\u001b[0m       \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1010\u001b[0;31m       \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribute_strategy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_step\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1011\u001b[0m       outputs = reduce_per_replica(\n\u001b[1;32m   1012\u001b[0m           outputs, self.distribute_strategy, reduction='first')\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(***failed resolving arguments***)\u001b[0m\n\u001b[1;32m   1310\u001b[0m       fn = autograph.tf_convert(\n\u001b[1;32m   1311\u001b[0m           fn, autograph_ctx.control_status_ctx(), convert_by_default=False)\n\u001b[0;32m-> 1312\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_extended\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1314\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreduce\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36mcall_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   2886\u001b[0m       \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2887\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mscope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2888\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2889\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2890\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/distribute/distribute_lib.py\u001b[0m in \u001b[0;36m_call_for_each_replica\u001b[0;34m(self, fn, args, kwargs)\u001b[0m\n\u001b[1;32m   3687\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_for_each_replica\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3688\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mReplicaContext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_container_strategy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreplica_id_in_sync_group\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3689\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3690\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3691\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_reduce_to\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreduce_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvalue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdestinations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    687\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    688\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mconversion_ctx\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 689\u001b[0;31m           \u001b[0;32mreturn\u001b[0m \u001b[0mconverted_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    690\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=broad-except\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    691\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'ag_error_metadata'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36mconverted_call\u001b[0;34m(f, args, kwargs, caller_fn_scope, options)\u001b[0m\n\u001b[1;32m    375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    376\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0muser_requested\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mconversion\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_allowlisted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 377\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_call_unconverted\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    379\u001b[0m   \u001b[0;31m# internal_convert_user_code is for example turned off when issuing a dynamic\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/autograph/impl/api.py\u001b[0m in \u001b[0;36m_call_unconverted\u001b[0;34m(f, args, kwargs, options, update_cache)\u001b[0m\n\u001b[1;32m    456\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    457\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 458\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    459\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    460\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mrun_step\u001b[0;34m(data)\u001b[0m\n\u001b[1;32m    998\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    999\u001b[0m       \u001b[0;32mdef\u001b[0m \u001b[0mrun_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1000\u001b[0;31m         \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1001\u001b[0m         \u001b[0;31m# Ensure counter is updated only if `train_step` succeeds.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1002\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrol_dependencies\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0m_minimum_control_deps\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(self, data)\u001b[0m\n\u001b[1;32m    861\u001b[0m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_target_and_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    862\u001b[0m     \u001b[0;31m# Run backwards pass.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 863\u001b[0;31m     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mminimize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrainable_variables\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    864\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompute_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    865\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36mminimize\u001b[0;34m(self, loss, var_list, grad_loss, name, tape)\u001b[0m\n\u001b[1;32m    529\u001b[0m     \"\"\"\n\u001b[1;32m    530\u001b[0m     grads_and_vars = self._compute_gradients(\n\u001b[0;32m--> 531\u001b[0;31m         loss, var_list=var_list, grad_loss=grad_loss, tape=tape)\n\u001b[0m\u001b[1;32m    532\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mapply_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads_and_vars\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    533\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_compute_gradients\u001b[0;34m(self, loss, var_list, grad_loss, tape)\u001b[0m\n\u001b[1;32m    581\u001b[0m     \u001b[0mvar_list\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnest\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    582\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/gradients\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 583\u001b[0;31m       \u001b[0mgrads_and_vars\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    584\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    585\u001b[0m     self._assert_valid_dtypes([\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/keras/optimizer_v2/optimizer_v2.py\u001b[0m in \u001b[0;36m_get_gradients\u001b[0;34m(self, tape, loss, var_list, grad_loss)\u001b[0m\n\u001b[1;32m    462\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_get_gradients\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    463\u001b[0m     \u001b[0;34m\"\"\"Called in `minimize` to compute gradients from loss.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 464\u001b[0;31m     \u001b[0mgrads\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtape\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    465\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzip\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgrads\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvar_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    466\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36mgradient\u001b[0;34m(self, target, sources, output_gradients, unconnected_gradients)\u001b[0m\n\u001b[1;32m   1085\u001b[0m         \u001b[0moutput_gradients\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1086\u001b[0m         \u001b[0msources_raw\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mflat_sources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1087\u001b[0;31m         unconnected_gradients=unconnected_gradients)\n\u001b[0m\u001b[1;32m   1088\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1089\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_persistent\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/imperative_grad.py\u001b[0m in \u001b[0;36mimperative_grad\u001b[0;34m(tape, target, sources, output_gradients, sources_raw, unconnected_gradients)\u001b[0m\n\u001b[1;32m     71\u001b[0m       \u001b[0moutput_gradients\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m       \u001b[0msources_raw\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m       compat.as_str(unconnected_gradients.value))\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/eager/backprop.py\u001b[0m in \u001b[0;36m_gradient_function\u001b[0;34m(op_name, attr_tuple, num_inputs, inputs, outputs, out_grads, skip_input_indices, forward_pass_name_scope)\u001b[0m\n\u001b[1;32m    154\u001b[0m       \u001b[0mgradient_name_scope\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mforward_pass_name_scope\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgradient_name_scope\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgrad_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmock_op\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mout_grads\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_FusedBatchNormV3Grad\u001b[0;34m(op, *grad)\u001b[0m\n\u001b[1;32m    919\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mops\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mRegisterGradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"FusedBatchNormV3\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    920\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_FusedBatchNormV3Grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 921\u001b[0;31m   \u001b[0;32mreturn\u001b[0m \u001b[0m_BaseFusedBatchNormGrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    922\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    923\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/nn_grad.py\u001b[0m in \u001b[0;36m_BaseFusedBatchNormGrad\u001b[0;34m(op, version, *grad)\u001b[0m\n\u001b[1;32m    875\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    876\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"reserve_space_3\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 877\u001b[0;31m     \u001b[0mdx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdscale\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdoffset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgrad_fun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    878\u001b[0m   \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    879\u001b[0m     \u001b[0mpop_mean\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m3\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/ops/gen_nn_ops.py\u001b[0m in \u001b[0;36mfused_batch_norm_grad_v3\u001b[0;34m(y_backprop, x, scale, reserve_space_1, reserve_space_2, reserve_space_3, epsilon, data_format, is_training, name)\u001b[0m\n\u001b[1;32m   4067\u001b[0m                                 \u001b[0mreserve_space_3\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mreserve_space_3\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4068\u001b[0m                                 \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdata_format\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4069\u001b[0;31m                                 is_training=is_training, name=name)\n\u001b[0m\u001b[1;32m   4070\u001b[0m   \u001b[0m_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_outputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4071\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0m_execute\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmust_record_gradient\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/tensorflow/python/framework/op_def_library.py\u001b[0m in \u001b[0;36m_apply_op_helper\u001b[0;34m(op_type_name, name, **keywords)\u001b[0m\n\u001b[1;32m    735\u001b[0m     must_colocate_inputs = [val for arg, val in zip(op_def.input_arg, inputs)\n\u001b[1;32m    736\u001b[0m                             if arg.is_ref]\n\u001b[0;32m--> 737\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_MaybeColocateWith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmust_colocate_inputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    738\u001b[0m       \u001b[0;31m# Add Op to graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    739\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "\"\"\"import tensorflow as tf\n",
        "nb_input = 10\n",
        "channels = 16\n",
        "output_size = 10\n",
        "layer_amount = 6\n",
        "test_tensors = tf.ones((nb_input,100,100,3))\n",
        "test_model = tf.keras.models.Sequential()\n",
        "test_model.add(CNN(channels, output_size, layer_amount, input_dict_norm_test, input_dict_redu_test,1))\n",
        "# test_model.summary()\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "loss = tf.keras.losses.categorical_crossentropy\n",
        "test_model.compile(loss = loss, optimizer = optimizer)\n",
        "test_model.fit(test_tensors, tf.ones((nb_input,output_size)))\n",
        "test_model.summary()\"\"\"\n",
        "import datetime\n",
        "log_dir = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\")\n",
        "tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=log_dir, histogram_freq=1)\n",
        "\n",
        "log_dir_2 = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \" (1)\"\n",
        "tensorboard_callback_2 = tf.keras.callbacks.TensorBoard(log_dir=log_dir_2, histogram_freq=1)\n",
        "\n",
        "\n",
        "nb_input = 10\n",
        "channels = 1\n",
        "output_size = 10\n",
        "layer_amount = 5\n",
        "test_tensors = tf.ones((nb_input,100,100,3))\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "loss = tf.keras.losses.categorical_crossentropy\n",
        "\n",
        "print(\"nb_channels\", channels, \"functions\")\n",
        "test_model_2 = CNN_creation(channels, output_size, layer_amount, input_dict_norm_test, input_dict_redu_test, test_tensors.shape)\n",
        "test_model_2.compile(loss = loss, optimizer = optimizer)\n",
        "test_model_2.fit(test_tensors, tf.ones((nb_input,output_size)), \n",
        "          callbacks=[tensorboard_callback_2])\n",
        "test_model_2.summary()\n",
        "\n",
        "print(\"nb_channels\", channels, \"class\")\n",
        "test_model = tf.keras.models.Sequential()\n",
        "test_model.add(CNN(channels, output_size, layer_amount, input_dict_norm_test, input_dict_redu_test))\n",
        "test_model.compile(loss = loss, optimizer = optimizer)\n",
        "test_model.fit(test_tensors, tf.ones((nb_input,output_size)), \n",
        "          callbacks=[tensorboard_callback])\n",
        "test_model.summary()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttPGgO9AfA5O"
      },
      "source": [
        "## Importing the cifar 10 data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NuQvxqs1monI"
      },
      "outputs": [],
      "source": [
        "data = tf.keras.datasets.cifar10.load_data()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7DBgMrZGmv3Z"
      },
      "outputs": [],
      "source": [
        "train_data, test_data = data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QDZr6yUpm0Wo",
        "outputId": "2d147a61-c899-49ca-f9b5-567f312ca16d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(50000, 32, 32, 3)\n",
            "(50000, 1)\n",
            "(10000, 32, 32, 3)\n",
            "(10000, 1)\n"
          ]
        }
      ],
      "source": [
        "print(train_data[0].shape)\n",
        "print(train_data[1].shape)\n",
        "print(test_data[0].shape)\n",
        "print(test_data[1].shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FslFK71SfcPF"
      },
      "source": [
        "## Creation of a model using cifar 10 and the plans"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3WsNGU6MpFfi",
        "outputId": "2a551ed2-a1d8-47df-e444-d81ca7857f43"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/5\n",
            "782/782 [==============================] - 98s 118ms/step - loss: 1.8029 - accuracy: 0.3454\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 25s 32ms/step - loss: 1.5566 - accuracy: 0.4267\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 1.4715 - accuracy: 0.4583\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 1.4039 - accuracy: 0.4875\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 24s 31ms/step - loss: 1.3531 - accuracy: 0.5063\n",
            "Model: \"sequential_3291\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " cnn_2 (CNN)                 (None, 10)                27546     \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 27,546\n",
            "Trainable params: 26,482\n",
            "Non-trainable params: 1,064\n",
            "_________________________________________________________________\n",
            "Time taken class : 3.0 m 17.97048282623291 s\n",
            "Epoch 1/5\n",
            "782/782 [==============================] - 45s 52ms/step - loss: 1.6332 - accuracy: 0.4110\n",
            "Epoch 2/5\n",
            "782/782 [==============================] - 41s 52ms/step - loss: 1.3241 - accuracy: 0.5263\n",
            "Epoch 3/5\n",
            "782/782 [==============================] - 40s 52ms/step - loss: 1.2006 - accuracy: 0.5729\n",
            "Epoch 4/5\n",
            "782/782 [==============================] - 100s 128ms/step - loss: 1.1168 - accuracy: 0.6015\n",
            "Epoch 5/5\n",
            "782/782 [==============================] - 42s 54ms/step - loss: 1.0482 - accuracy: 0.6263\n",
            "Model: \"model_8\"\n",
            "__________________________________________________________________________________________________\n",
            " Layer (type)                   Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            " input_9 (InputLayer)           [(None, 32, 32, 3)]  0           []                               \n",
            "                                                                                                  \n",
            " sequential_3613 (Sequential)   (None, 32, 32, 30)   960         ['input_9[0][0]']                \n",
            "                                                                                                  \n",
            " std_conv_106 (StdConv)         (None, 32, 32, 10)   340         ['sequential_3613[0][0]']        \n",
            "                                                                                                  \n",
            " std_conv_107 (StdConv)         (None, 32, 32, 10)   340         ['sequential_3613[0][0]']        \n",
            "                                                                                                  \n",
            " pool_bn_915 (PoolBN)           (None, 32, 32, 10)   40          ['std_conv_106[0][0]']           \n",
            "                                                                                                  \n",
            " pool_bn_917 (PoolBN)           (None, 32, 32, 10)   40          ['std_conv_107[0][0]']           \n",
            "                                                                                                  \n",
            " add_160 (Add)                  (None, 32, 32, 10)   0           ['pool_bn_915[0][0]',            \n",
            "                                                                  'pool_bn_917[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_919 (PoolBN)           (None, 32, 32, 10)   40          ['add_160[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_921 (PoolBN)           (None, 32, 32, 10)   40          ['std_conv_106[0][0]']           \n",
            "                                                                                                  \n",
            " add_161 (Add)                  (None, 32, 32, 10)   0           ['pool_bn_919[0][0]',            \n",
            "                                                                  'pool_bn_921[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_923 (PoolBN)           (None, 32, 32, 10)   40          ['add_160[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_925 (PoolBN)           (None, 32, 32, 10)   40          ['add_161[0][0]']                \n",
            "                                                                                                  \n",
            " add_162 (Add)                  (None, 32, 32, 10)   0           ['pool_bn_923[0][0]',            \n",
            "                                                                  'pool_bn_925[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_927 (PoolBN)           (None, 32, 32, 10)   40          ['add_162[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_929 (PoolBN)           (None, 32, 32, 10)   40          ['add_161[0][0]']                \n",
            "                                                                                                  \n",
            " add_163 (Add)                  (None, 32, 32, 10)   0           ['pool_bn_927[0][0]',            \n",
            "                                                                  'pool_bn_929[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_138 (Concatenate)  (None, 32, 32, 40)   0           ['add_160[0][0]',                \n",
            "                                                                  'add_161[0][0]',                \n",
            "                                                                  'add_162[0][0]',                \n",
            "                                                                  'add_163[0][0]']                \n",
            "                                                                                                  \n",
            " std_conv_109 (StdConv)         (None, 32, 32, 20)   680         ['sequential_3613[0][0]']        \n",
            "                                                                                                  \n",
            " std_conv_108 (StdConv)         (None, 32, 32, 20)   880         ['concatenate_138[0][0]']        \n",
            "                                                                                                  \n",
            " pool_bn_931 (PoolBN)           (None, 16, 16, 20)   80          ['std_conv_109[0][0]']           \n",
            "                                                                                                  \n",
            " pool_bn_933 (PoolBN)           (None, 16, 16, 20)   80          ['std_conv_108[0][0]']           \n",
            "                                                                                                  \n",
            " add_164 (Add)                  (None, 16, 16, 20)   0           ['pool_bn_931[0][0]',            \n",
            "                                                                  'pool_bn_933[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_935 (PoolBN)           (None, 16, 16, 20)   80          ['add_164[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_937 (PoolBN)           (None, 16, 16, 20)   80          ['std_conv_109[0][0]']           \n",
            "                                                                                                  \n",
            " add_165 (Add)                  (None, 16, 16, 20)   0           ['pool_bn_935[0][0]',            \n",
            "                                                                  'pool_bn_937[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_939 (PoolBN)           (None, 16, 16, 20)   80          ['add_165[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_941 (PoolBN)           (None, 16, 16, 20)   80          ['add_164[0][0]']                \n",
            "                                                                                                  \n",
            " add_166 (Add)                  (None, 16, 16, 20)   0           ['pool_bn_939[0][0]',            \n",
            "                                                                  'pool_bn_941[0][0]']            \n",
            "                                                                                                  \n",
            " dil_conv_2780 (DilConv)        (None, 16, 16, 20)   20120       ['add_166[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_945 (PoolBN)           (None, 16, 16, 20)   80          ['add_165[0][0]']                \n",
            "                                                                                                  \n",
            " add_167 (Add)                  (None, 16, 16, 20)   0           ['dil_conv_2780[0][0]',          \n",
            "                                                                  'pool_bn_945[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_142 (Concatenate)  (None, 16, 16, 80)   0           ['add_164[0][0]',                \n",
            "                                                                  'add_165[0][0]',                \n",
            "                                                                  'add_166[0][0]',                \n",
            "                                                                  'add_167[0][0]']                \n",
            "                                                                                                  \n",
            " std_conv_110 (StdConv)         (None, 16, 16, 20)   1680        ['concatenate_142[0][0]']        \n",
            "                                                                                                  \n",
            " factorized_reduce_101 (Factori  (None, 16, 16, 20)  7280        ['concatenate_138[0][0]']        \n",
            " zedReduce)                                                                                       \n",
            "                                                                                                  \n",
            " pool_bn_947 (PoolBN)           (None, 16, 16, 20)   80          ['std_conv_110[0][0]']           \n",
            "                                                                                                  \n",
            " pool_bn_949 (PoolBN)           (None, 16, 16, 20)   80          ['factorized_reduce_101[0][0]']  \n",
            "                                                                                                  \n",
            " add_168 (Add)                  (None, 16, 16, 20)   0           ['pool_bn_947[0][0]',            \n",
            "                                                                  'pool_bn_949[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_951 (PoolBN)           (None, 16, 16, 20)   80          ['add_168[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_953 (PoolBN)           (None, 16, 16, 20)   80          ['std_conv_110[0][0]']           \n",
            "                                                                                                  \n",
            " add_169 (Add)                  (None, 16, 16, 20)   0           ['pool_bn_951[0][0]',            \n",
            "                                                                  'pool_bn_953[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_955 (PoolBN)           (None, 16, 16, 20)   80          ['add_168[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_957 (PoolBN)           (None, 16, 16, 20)   80          ['add_169[0][0]']                \n",
            "                                                                                                  \n",
            " add_170 (Add)                  (None, 16, 16, 20)   0           ['pool_bn_955[0][0]',            \n",
            "                                                                  'pool_bn_957[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_959 (PoolBN)           (None, 16, 16, 20)   80          ['add_170[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_961 (PoolBN)           (None, 16, 16, 20)   80          ['add_169[0][0]']                \n",
            "                                                                                                  \n",
            " add_171 (Add)                  (None, 16, 16, 20)   0           ['pool_bn_959[0][0]',            \n",
            "                                                                  'pool_bn_961[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_144 (Concatenate)  (None, 16, 16, 80)   0           ['add_168[0][0]',                \n",
            "                                                                  'add_169[0][0]',                \n",
            "                                                                  'add_170[0][0]',                \n",
            "                                                                  'add_171[0][0]']                \n",
            "                                                                                                  \n",
            " std_conv_113 (StdConv)         (None, 16, 16, 40)   3360        ['concatenate_142[0][0]']        \n",
            "                                                                                                  \n",
            " std_conv_112 (StdConv)         (None, 16, 16, 40)   3360        ['concatenate_144[0][0]']        \n",
            "                                                                                                  \n",
            " pool_bn_963 (PoolBN)           (None, 8, 8, 40)     160         ['std_conv_113[0][0]']           \n",
            "                                                                                                  \n",
            " pool_bn_965 (PoolBN)           (None, 8, 8, 40)     160         ['std_conv_112[0][0]']           \n",
            "                                                                                                  \n",
            " add_172 (Add)                  (None, 8, 8, 40)     0           ['pool_bn_963[0][0]',            \n",
            "                                                                  'pool_bn_965[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_967 (PoolBN)           (None, 8, 8, 40)     160         ['add_172[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_969 (PoolBN)           (None, 8, 8, 40)     160         ['std_conv_113[0][0]']           \n",
            "                                                                                                  \n",
            " add_173 (Add)                  (None, 8, 8, 40)     0           ['pool_bn_967[0][0]',            \n",
            "                                                                  'pool_bn_969[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_971 (PoolBN)           (None, 8, 8, 40)     160         ['add_173[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_973 (PoolBN)           (None, 8, 8, 40)     160         ['add_172[0][0]']                \n",
            "                                                                                                  \n",
            " add_174 (Add)                  (None, 8, 8, 40)     0           ['pool_bn_971[0][0]',            \n",
            "                                                                  'pool_bn_973[0][0]']            \n",
            "                                                                                                  \n",
            " dil_conv_2876 (DilConv)        (None, 8, 8, 40)     80240       ['add_174[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_977 (PoolBN)           (None, 8, 8, 40)     160         ['add_173[0][0]']                \n",
            "                                                                                                  \n",
            " add_175 (Add)                  (None, 8, 8, 40)     0           ['dil_conv_2876[0][0]',          \n",
            "                                                                  'pool_bn_977[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_148 (Concatenate)  (None, 8, 8, 160)    0           ['add_172[0][0]',                \n",
            "                                                                  'add_173[0][0]',                \n",
            "                                                                  'add_174[0][0]',                \n",
            "                                                                  'add_175[0][0]']                \n",
            "                                                                                                  \n",
            " std_conv_114 (StdConv)         (None, 8, 8, 40)     6560        ['concatenate_148[0][0]']        \n",
            "                                                                                                  \n",
            " factorized_reduce_105 (Factori  (None, 8, 8, 40)    28960       ['concatenate_144[0][0]']        \n",
            " zedReduce)                                                                                       \n",
            "                                                                                                  \n",
            " pool_bn_979 (PoolBN)           (None, 8, 8, 40)     160         ['std_conv_114[0][0]']           \n",
            "                                                                                                  \n",
            " pool_bn_981 (PoolBN)           (None, 8, 8, 40)     160         ['factorized_reduce_105[0][0]']  \n",
            "                                                                                                  \n",
            " add_176 (Add)                  (None, 8, 8, 40)     0           ['pool_bn_979[0][0]',            \n",
            "                                                                  'pool_bn_981[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_983 (PoolBN)           (None, 8, 8, 40)     160         ['add_176[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_985 (PoolBN)           (None, 8, 8, 40)     160         ['std_conv_114[0][0]']           \n",
            "                                                                                                  \n",
            " add_177 (Add)                  (None, 8, 8, 40)     0           ['pool_bn_983[0][0]',            \n",
            "                                                                  'pool_bn_985[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_987 (PoolBN)           (None, 8, 8, 40)     160         ['add_176[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_989 (PoolBN)           (None, 8, 8, 40)     160         ['add_177[0][0]']                \n",
            "                                                                                                  \n",
            " add_178 (Add)                  (None, 8, 8, 40)     0           ['pool_bn_987[0][0]',            \n",
            "                                                                  'pool_bn_989[0][0]']            \n",
            "                                                                                                  \n",
            " pool_bn_991 (PoolBN)           (None, 8, 8, 40)     160         ['add_178[0][0]']                \n",
            "                                                                                                  \n",
            " pool_bn_993 (PoolBN)           (None, 8, 8, 40)     160         ['add_177[0][0]']                \n",
            "                                                                                                  \n",
            " add_179 (Add)                  (None, 8, 8, 40)     0           ['pool_bn_991[0][0]',            \n",
            "                                                                  'pool_bn_993[0][0]']            \n",
            "                                                                                                  \n",
            " concatenate_150 (Concatenate)  (None, 8, 8, 160)    0           ['add_176[0][0]',                \n",
            "                                                                  'add_177[0][0]',                \n",
            "                                                                  'add_178[0][0]',                \n",
            "                                                                  'add_179[0][0]']                \n",
            "                                                                                                  \n",
            " global_average_pooling2d_11 (G  (None, 160)         0           ['concatenate_150[0][0]']        \n",
            " lobalAveragePooling2D)                                                                           \n",
            "                                                                                                  \n",
            " flatten_8 (Flatten)            (None, 160)          0           ['global_average_pooling2d_11[0][\n",
            "                                                                 0]']                             \n",
            "                                                                                                  \n",
            " dense_11 (Dense)               (None, 10)           1610        ['flatten_8[0][0]']              \n",
            "                                                                                                  \n",
            "==================================================================================================\n",
            "Total params: 160,290\n",
            "Trainable params: 157,630\n",
            "Non-trainable params: 2,660\n",
            "__________________________________________________________________________________________________\n",
            "Time taken function : 4.0 m 32.2917594909668 s\n"
          ]
        }
      ],
      "source": [
        "import tensorflow as tf\n",
        "import time\n",
        "\n",
        "# nb_input = 10\n",
        "batch_size = 64\n",
        "channels = 4\n",
        "output_size = 10\n",
        "n_epochs = 5\n",
        "layer_amount = 5\n",
        "# Dataset creation\n",
        "train_tensors = tf.cast(tf.convert_to_tensor(train_data[0]),tf.float32)\n",
        "# print(train_tensors)\n",
        "result_train_tensor = tf.keras.utils.to_categorical(tf.convert_to_tensor(train_data[1]))\n",
        "\n",
        "\n",
        "\n",
        "import datetime\n",
        "log_dir_3 = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \" new factorized_reduce class \" + str(channels) \n",
        "tensorboard_callback_3 = tf.keras.callbacks.TensorBoard(log_dir=log_dir_3, histogram_freq=1, profile_batch = \"1, 782\")\n",
        "\n",
        "log_dir_4 = \"logs/fit/\" + datetime.datetime.now().strftime(\"%Y%m%d-%H%M%S\") + \" new factorized_reduce function \" + str(channels)\n",
        "tensorboard_callback_4 = tf.keras.callbacks.TensorBoard(log_dir=log_dir_4, histogram_freq=1, profile_batch = \"2347, 3128\")\n",
        "\n",
        "\n",
        "\n",
        "test_tensors = tf.cast(tf.convert_to_tensor(test_data[0]),tf.float32)\n",
        "result_test_tensor = tf.keras.utils.to_categorical(tf.convert_to_tensor(test_data[1]))\n",
        "optimizer = tf.keras.optimizers.SGD()\n",
        "loss = tf.keras.losses.categorical_crossentropy\n",
        "\n",
        "start_time = time.time()\n",
        "test_model = tf.keras.models.Sequential()\n",
        "test_model.add(CNN(channels, output_size, layer_amount, input_dict_norm_3, input_dict_redu_3))\n",
        "test_model.compile(loss = loss, optimizer = optimizer, metrics= ['accuracy'])\n",
        "test_model.fit(train_tensors, result_train_tensor, epochs = n_epochs, batch_size = batch_size, \n",
        "          callbacks=[tensorboard_callback_3])\n",
        "test_model.summary()\n",
        "end_time = time.time()-start_time\n",
        "print(\"Time taken class :\", end_time//60, \"m\", end_time%60, \"s\")\n",
        "\n",
        "channels = 10\n",
        "start_time = time.time()\n",
        "test_model_2 = CNN_creation(channels, output_size, layer_amount, input_dict_norm_3, input_dict_redu_3, train_tensors.shape)\n",
        "test_model_2.compile(loss = loss, optimizer = optimizer, metrics= ['accuracy'])\n",
        "test_model_2.fit(train_tensors, result_train_tensor, epochs = n_epochs, batch_size = batch_size, \n",
        "          callbacks=[tensorboard_callback_4])\n",
        "test_model_2.summary()\n",
        "end_time = time.time()-start_time\n",
        "print(\"Time taken function :\", end_time//60, \"m\", end_time%60, \"s\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Evaluation of the model"
      ],
      "metadata": {
        "id": "I0bXeF5kpnm5"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ETGQOAdBv4CI"
      },
      "outputs": [],
      "source": [
        "test_model.evaluate(test_tensors, result_test_tensor)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "qKZsbt9MbwDu"
      ],
      "name": "Copie de Création modèle via plan du DARTS v7.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}