{
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "import sys\n",
        "print(\"Python version: {}\". format(sys.version))\n",
        "\n",
        "import IPython\n",
        "from IPython import display\n",
        "from IPython.display import Markdown, display, HTML\n",
        "print(\"IPython version: {}\". format(IPython.__version__))\n",
        "\n",
        "import numpy as np\n",
        "print(\"NumPy version: {}\". format(np.__version__))\n",
        "\n",
        "import scipy as sp\n",
        "from scipy import stats\n",
        "print(\"SciPy version: {}\". format(sp.__version__))\n",
        "\n",
        "import pandas as pd\n",
        "print(\"pandas version: {}\". format(pd.__version__))\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
        "\n",
        "import seaborn as sns\n",
        "print(\"seaborn version : {}\". format(sns.__version__))\n",
        "sns.set()\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"tensorflow version : {}\". format(tf.__version__))\n",
        "\n",
        "print(\"keras version : {}\". format(tf.keras.__version__))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eEfGe1C6OqMi",
        "outputId": "3966ebff-9716-409c-c127-d910b53949b3"
      },
      "id": "eEfGe1C6OqMi",
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.7.13 (default, Mar 16 2022, 17:37:17) \n",
            "[GCC 7.5.0]\n",
            "IPython version: 5.5.0\n",
            "NumPy version: 1.21.6\n",
            "SciPy version: 1.4.1\n",
            "pandas version: 1.3.5\n",
            "matplotlib version: 3.2.2\n",
            "seaborn version : 0.11.2\n",
            "tensorflow version : 2.8.0\n",
            "keras version : 2.8.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "id": "08f0a33a",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "08f0a33a",
        "outputId": "c291d65d-d3c7-4a77-f293-a4217a166776"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'nni'...\n",
            "remote: Enumerating objects: 52751, done.\u001b[K\n",
            "remote: Counting objects: 100% (3382/3382), done.\u001b[K\n",
            "remote: Compressing objects: 100% (1891/1891), done.\u001b[K\n",
            "remote: Total 52751 (delta 1743), reused 2634 (delta 1397), pack-reused 49369\u001b[K\n",
            "Receiving objects: 100% (52751/52751), 119.00 MiB | 25.52 MiB/s, done.\n",
            "Resolving deltas: 100% (30458/30458), done.\n"
          ]
        }
      ],
      "source": [
        "# Accés au github\n",
        "from getpass import getpass\n",
        "import os\n",
        "\n",
        "git_public = True\n",
        "\n",
        "if git_public:\n",
        "    !git clone https://github.com/microsoft/nni\n",
        "else:\n",
        "    user = getpass('NAS user')\n",
        "    password = getpass('NAS password')\n",
        "    os.environ['NAS_AUTH'] = user + ':' + password\n",
        "    !git clone https://$NAS_AUTH@github.com/microsoft/nni"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "VjkduHH-Oo7p"
      },
      "id": "VjkduHH-Oo7p",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c478713",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3c478713",
        "outputId": "12ce0b3b-9794-43e7-8c7a-324c72c83712"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/nni/examples/nas/oneshot/darts\n"
          ]
        }
      ],
      "source": [
        "# On se positionne dans le bon réperoire\n",
        "%cd nni/examples/nas/oneshot/darts/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "QLRqYpz8vCbV"
      },
      "id": "QLRqYpz8vCbV",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%ls"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AX07KaUVaL1B",
        "outputId": "20ba84d0-6c0d-40d6-939c-f2550d3765d6"
      },
      "id": "AX07KaUVaL1B",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "datasets.py  ops.py     README_zh_CN.md  search.py\n",
            "model.py     README.md  retrain.py       utils.py\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install nni\n",
        "!pip install graphviz"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kvhhQuhcxoKq",
        "outputId": "f427c383-1366-4c46-b9bb-59d5eb024387"
      },
      "id": "kvhhQuhcxoKq",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting nni\n",
            "  Downloading nni-2.7-py3-none-manylinux1_x86_64.whl (56.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 56.1 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from nni) (2.23.0)\n",
            "Requirement already satisfied: scipy<1.8 in /usr/local/lib/python3.7/dist-packages (from nni) (1.4.1)\n",
            "Requirement already satisfied: numpy<1.22 in /usr/local/lib/python3.7/dist-packages (from nni) (1.21.5)\n",
            "Collecting PythonWebHDFS\n",
            "  Downloading PythonWebHDFS-0.2.3-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: prettytable in /usr/local/lib/python3.7/dist-packages (from nni) (3.2.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from nni) (3.6.0)\n",
            "Collecting pyyaml>=5.4\n",
            "  Downloading PyYAML-6.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (596 kB)\n",
            "\u001b[K     |████████████████████████████████| 596 kB 47.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typeguard in /usr/local/lib/python3.7/dist-packages (from nni) (2.7.1)\n",
            "Requirement already satisfied: hyperopt==0.1.2 in /usr/local/lib/python3.7/dist-packages (from nni) (0.1.2)\n",
            "Collecting json-tricks>=3.15.5\n",
            "  Downloading json_tricks-3.15.5-py2.py3-none-any.whl (26 kB)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from nni) (4.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (from nni) (1.3.5)\n",
            "Requirement already satisfied: scikit-learn>=0.24.1 in /usr/local/lib/python3.7/dist-packages (from nni) (1.0.2)\n",
            "Collecting responses\n",
            "  Downloading responses-0.20.0-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.7/dist-packages (from nni) (5.4.8)\n",
            "Collecting schema\n",
            "  Downloading schema-0.7.5-py2.py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.7/dist-packages (from nni) (1.3.0)\n",
            "Collecting websockets>=10.1\n",
            "  Downloading websockets-10.3-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (112 kB)\n",
            "\u001b[K     |████████████████████████████████| 112 kB 43.6 MB/s \n",
            "\u001b[?25hCollecting colorama\n",
            "  Downloading colorama-0.4.4-py2.py3-none-any.whl (16 kB)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.7/dist-packages (from nni) (21.3)\n",
            "Requirement already satisfied: astor in /usr/local/lib/python3.7/dist-packages (from nni) (0.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->nni) (1.15.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->nni) (0.16.0)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->nni) (4.64.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->nni) (2.6.3)\n",
            "Requirement already satisfied: pymongo in /usr/local/lib/python3.7/dist-packages (from hyperopt==0.1.2->nni) (4.1.0)\n",
            "Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->nni) (1.1.0)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn>=0.24.1->nni) (3.1.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging->nni) (3.0.8)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nni) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2017.3 in /usr/local/lib/python3.7/dist-packages (from pandas->nni) (2018.9)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from prettytable->nni) (4.11.3)\n",
            "Requirement already satisfied: wcwidth in /usr/local/lib/python3.7/dist-packages (from prettytable->nni) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->prettytable->nni) (3.8.0)\n",
            "Collecting simplejson\n",
            "  Downloading simplejson-3.17.6-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (130 kB)\n",
            "\u001b[K     |████████████████████████████████| 130 kB 46.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->nni) (2.10)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->nni) (2021.10.8)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->nni) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->nni) (1.24.3)\n",
            "Collecting urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1\n",
            "  Downloading urllib3-1.25.11-py2.py3-none-any.whl (127 kB)\n",
            "\u001b[K     |████████████████████████████████| 127 kB 46.5 MB/s \n",
            "\u001b[?25hRequirement already satisfied: contextlib2>=0.5.5 in /usr/local/lib/python3.7/dist-packages (from schema->nni) (0.5.5)\n",
            "Installing collected packages: urllib3, simplejson, websockets, schema, responses, pyyaml, PythonWebHDFS, json-tricks, colorama, nni\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 1.24.3\n",
            "    Uninstalling urllib3-1.24.3:\n",
            "      Successfully uninstalled urllib3-1.24.3\n",
            "  Attempting uninstall: pyyaml\n",
            "    Found existing installation: PyYAML 3.13\n",
            "    Uninstalling PyYAML-3.13:\n",
            "      Successfully uninstalled PyYAML-3.13\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\u001b[0m\n",
            "Successfully installed PythonWebHDFS-0.2.3 colorama-0.4.4 json-tricks-3.15.5 nni-2.7 pyyaml-6.0 responses-0.20.0 schema-0.7.5 simplejson-3.17.6 urllib3-1.25.11 websockets-10.3\n",
            "Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch_lightning"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XKsp_ihh3khX",
        "outputId": "15f0336c-b66c-4edb-f402-676a97f244b1"
      },
      "id": "XKsp_ihh3khX",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pytorch_lightning\n",
            "  Downloading pytorch_lightning-1.6.1-py3-none-any.whl (582 kB)\n",
            "\u001b[K     |████████████████████████████████| 582 kB 5.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tensorboard>=2.2.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (2.8.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.64.0)\n",
            "Requirement already satisfied: packaging>=17.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (21.3)\n",
            "Requirement already satisfied: typing-extensions>=4.0.0 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (4.1.1)\n",
            "Collecting fsspec[http]!=2021.06.0,>=2021.05.0\n",
            "  Downloading fsspec-2022.3.0-py3-none-any.whl (136 kB)\n",
            "\u001b[K     |████████████████████████████████| 136 kB 41.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: PyYAML>=5.4 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (6.0)\n",
            "Requirement already satisfied: numpy>=1.17.2 in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.21.5)\n",
            "Collecting torchmetrics>=0.4.1\n",
            "  Downloading torchmetrics-0.8.0-py3-none-any.whl (408 kB)\n",
            "\u001b[K     |████████████████████████████████| 408 kB 41.4 MB/s \n",
            "\u001b[?25hCollecting pyDeprecate<0.4.0,>=0.3.1\n",
            "  Downloading pyDeprecate-0.3.2-py3-none-any.whl (10 kB)\n",
            "Requirement already satisfied: torch>=1.8.* in /usr/local/lib/python3.7/dist-packages (from pytorch_lightning) (1.10.0+cu111)\n",
            "Collecting aiohttp\n",
            "  Downloading aiohttp-3.8.1-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (1.1 MB)\n",
            "\u001b[K     |████████████████████████████████| 1.1 MB 34.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.23.0)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=17.0->pytorch_lightning) (3.0.8)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.4.6)\n",
            "Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.6.1)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.35.0)\n",
            "Requirement already satisfied: grpcio>=1.24.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.44.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.3.6)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.8.1)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (57.4.0)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (0.37.1)\n",
            "Requirement already satisfied: protobuf>=3.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (3.17.3)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.7/dist-packages (from tensorboard>=2.2.0->pytorch_lightning) (1.0.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from absl-py>=0.4->tensorboard>=2.2.0->pytorch_lightning) (1.15.0)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.2.8)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.2.4)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (4.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (1.3.1)\n",
            "Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (4.11.3)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard>=2.2.0->pytorch_lightning) (3.8.0)\n",
            "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard>=2.2.0->pytorch_lightning) (0.4.8)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2021.10.8)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (1.25.11)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (3.0.4)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard>=2.2.0->pytorch_lightning) (3.2.0)\n",
            "Collecting yarl<2.0,>=1.0\n",
            "  Downloading yarl-1.7.2-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_12_x86_64.manylinux2010_x86_64.whl (271 kB)\n",
            "\u001b[K     |████████████████████████████████| 271 kB 42.1 MB/s \n",
            "\u001b[?25hRequirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (21.4.0)\n",
            "Collecting aiosignal>=1.1.2\n",
            "  Downloading aiosignal-1.2.0-py3-none-any.whl (8.2 kB)\n",
            "Collecting async-timeout<5.0,>=4.0.0a3\n",
            "  Downloading async_timeout-4.0.2-py3-none-any.whl (5.8 kB)\n",
            "Requirement already satisfied: charset-normalizer<3.0,>=2.0 in /usr/local/lib/python3.7/dist-packages (from aiohttp->fsspec[http]!=2021.06.0,>=2021.05.0->pytorch_lightning) (2.0.12)\n",
            "Collecting asynctest==0.13.0\n",
            "  Downloading asynctest-0.13.0-py3-none-any.whl (26 kB)\n",
            "Collecting frozenlist>=1.1.1\n",
            "  Downloading frozenlist-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (144 kB)\n",
            "\u001b[K     |████████████████████████████████| 144 kB 38.1 MB/s \n",
            "\u001b[?25hCollecting multidict<7.0,>=4.5\n",
            "  Downloading multidict-6.0.2-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (94 kB)\n",
            "\u001b[K     |████████████████████████████████| 94 kB 2.7 MB/s \n",
            "\u001b[?25hInstalling collected packages: multidict, frozenlist, yarl, asynctest, async-timeout, aiosignal, pyDeprecate, fsspec, aiohttp, torchmetrics, pytorch-lightning\n",
            "Successfully installed aiohttp-3.8.1 aiosignal-1.2.0 async-timeout-4.0.2 asynctest-0.13.0 frozenlist-1.3.0 fsspec-2022.3.0 multidict-6.0.2 pyDeprecate-0.3.2 pytorch-lightning-1.6.1 torchmetrics-0.8.0 yarl-1.7.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "%%shell\n",
        "python search.py --layer=2 --epoch=2 --visualization\n",
        "\"\"\""
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "K-JwcAbvy-Gt",
        "outputId": "81fe65e5-968e-4743-a7cf-99ccc1345962"
      },
      "id": "K-JwcAbvy-Gt",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Traceback (most recent call last):\n",
            "  File \"search.py\", line 32, in <module>\n",
            "    dataset_train, dataset_valid = datasets.get_dataset(\"cifar10\")\n",
            "  File \"/content/nni/examples/nas/oneshot/darts/datasets.py\", line 52, in get_dataset\n",
            "    dataset_train = CIFAR10(root=\"./data\", train=True, download=True, transform=train_transform)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\", line 66, in __init__\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/datasets/cifar.py\", line 144, in download\n",
            "    download_and_extract_archive(self.url, self.root, filename=self.filename, md5=self.tgz_md5)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\", line 427, in download_and_extract_archive\n",
            "    download_url(url, download_root, filename, md5)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\", line 130, in download_url\n",
            "    url = _get_redirect_url(url, max_hops=max_redirect_hops)\n",
            "  File \"/usr/local/lib/python3.7/dist-packages/torchvision/datasets/utils.py\", line 78, in _get_redirect_url\n",
            "    with urllib.request.urlopen(urllib.request.Request(url, headers=headers)) as response:\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 222, in urlopen\n",
            "    return opener.open(url, data, timeout)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 525, in open\n",
            "    response = self._open(req, data)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 543, in _open\n",
            "    '_open', req)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 503, in _call_chain\n",
            "    result = func(*args)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 1393, in https_open\n",
            "    context=self._context, check_hostname=self._check_hostname)\n",
            "  File \"/usr/lib/python3.7/urllib/request.py\", line 1350, in do_open\n",
            "    encode_chunked=req.has_header('Transfer-encoding'))\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 1281, in request\n",
            "    self._send_request(method, url, body, headers, encode_chunked)\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 1327, in _send_request\n",
            "    self.endheaders(body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 1276, in endheaders\n",
            "    self._send_output(message_body, encode_chunked=encode_chunked)\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 1036, in _send_output\n",
            "    self.send(msg)\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 976, in send\n",
            "    self.connect()\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 1443, in connect\n",
            "    super().connect()\n",
            "  File \"/usr/lib/python3.7/http/client.py\", line 948, in connect\n",
            "    (self.host,self.port), self.timeout, self.source_address)\n",
            "  File \"/usr/lib/python3.7/socket.py\", line 716, in create_connection\n",
            "    sock.connect(sa)\n",
            "KeyboardInterrupt\n",
            "\u001b[0m"
          ]
        },
        {
          "output_type": "error",
          "ename": "CalledProcessError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mCalledProcessError\u001b[0m                        Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-6-b4f3c1b6881c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'shell'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m''\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'python search.py --layer=2 --epoch=2 --visualization'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36m_shell_cell_magic\u001b[0;34m(args, cmd)\u001b[0m\n\u001b[1;32m    111\u001b[0m   \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_run_command\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcmd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclear_streamed_output\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mparsed_args\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mignore_errors\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m     \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcheck_returncode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/google/colab/_system_commands.py\u001b[0m in \u001b[0;36mcheck_returncode\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    137\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreturncode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    138\u001b[0m       raise subprocess.CalledProcessError(\n\u001b[0;32m--> 139\u001b[0;31m           returncode=self.returncode, cmd=self.args, output=self.output)\n\u001b[0m\u001b[1;32m    140\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_repr_pretty_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcycle\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m  \u001b[0;31m# pylint:disable=unused-argument\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mCalledProcessError\u001b[0m: Command 'python search.py --layer=2 --epoch=2 --visualization' returned non-zero exit status 1."
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Final_architecture = {'reduce_n2_p0': 'maxpool',\n",
        "                      'reduce_n2_p1': 'maxpool',\n",
        "                      'reduce_n3_p0': 'maxpool',\n",
        "                      'reduce_n3_p1': 'maxpool',\n",
        "                      'reduce_n3_p2': 'maxpool',\n",
        "                      'reduce_n4_p0': 'maxpool',\n",
        "                      'reduce_n4_p1': 'maxpool',\n",
        "                      'reduce_n4_p2': 'maxpool',\n",
        "                      'reduce_n4_p3': 'maxpool',\n",
        "                      'reduce_n5_p0': 'maxpool',\n",
        "                      'reduce_n5_p1': 'maxpool',\n",
        "                      'reduce_n5_p2': 'maxpool',\n",
        "                      'reduce_n5_p3': 'maxpool', \n",
        "                      'reduce_n5_p4': 'maxpool',\n",
        "                      'reduce_n2_switch': [1, 0],\n",
        "                      'reduce_n3_switch': [2, 1],\n",
        "                      'reduce_n4_switch': [3, 2],\n",
        "                      'reduce_n5_switch': [4, 3]}"
      ],
      "metadata": {
        "id": "1lEb1aqNh2VL"
      },
      "id": "1lEb1aqNh2VL"
    },
    {
      "cell_type": "markdown",
      "source": [
        "# convertir un modèle pytorch en un modèle tensorflow"
      ],
      "metadata": {
        "id": "RWREdjfzirhq"
      },
      "id": "RWREdjfzirhq"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## installation des dépendances"
      ],
      "metadata": {
        "id": "gf0MMOSRjIwR"
      },
      "id": "gf0MMOSRjIwR"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install torchvision"
      ],
      "metadata": {
        "id": "YYWiIz3ZikO0"
      },
      "id": "YYWiIz3ZikO0",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "id": "ZgajmHZA3Jzo"
      },
      "id": "ZgajmHZA3Jzo",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install tensorflow"
      ],
      "metadata": {
        "id": "gM9-QzYIizK7"
      },
      "id": "gM9-QzYIizK7",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx_tf"
      ],
      "metadata": {
        "id": "xGIvAPYoi7Yg"
      },
      "id": "xGIvAPYoi7Yg",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## code trouvé permettant d'effectuer la conversion d'un modèle pytorch en un modèle tensorflow"
      ],
      "metadata": {
        "id": "APPneKXyk8B6"
      },
      "id": "APPneKXyk8B6"
    },
    {
      "cell_type": "markdown",
      "source": [
        "La classe correspond à la définition du modèle ainsi que la façon dont il est implémenté "
      ],
      "metadata": {
        "id": "UebFLxr5lxsE"
      },
      "id": "UebFLxr5lxsE"
    },
    {
      "cell_type": "code",
      "source": [
        "\"\"\"\n",
        "import sys\n",
        "import os\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import onnx\n",
        "from collections import OrderedDict\n",
        "import tensorflow as tf\n",
        "from torch.autograd import Variable\n",
        "from onnx_tf.backend import prepare\n",
        "\n",
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_dims, n_hiddens, n_class):\n",
        "        super(MLP, self).__init__()\n",
        "        assert isinstance(input_dims, int), 'Please provide int for input_dims'\n",
        "        self.input_dims = input_dims\n",
        "        current_dims = input_dims\n",
        "        layers = OrderedDict()\n",
        "\n",
        "        if isinstance(n_hiddens, int):\n",
        "            n_hiddens = [n_hiddens]\n",
        "        else:\n",
        "            n_hiddens = list(n_hiddens)\n",
        "        for i, n_hidden in enumerate(n_hiddens):\n",
        "            layers['fc{}'.format(i+1)] = nn.Linear(current_dims, n_hidden)\n",
        "            layers['relu{}'.format(i+1)] = nn.ReLU()\n",
        "            layers['drop{}'.format(i+1)] = nn.Dropout(0.2)\n",
        "            current_dims = n_hidden\n",
        "        layers['out'] = nn.Linear(current_dims, n_class)\n",
        "\n",
        "        self.model= nn.Sequential(layers)\n",
        "        print(self.model)\n",
        "\n",
        "    def forward(self, input):\n",
        "        input = input.view(input.size(0), -1)\n",
        "        assert input.size(1) == self.input_dims\n",
        "        return self.model.forward(input)\n",
        "\n",
        "print(\"%s\" % sys.argv[1])\n",
        "print(\"%s\" % sys.argv[2])\n",
        "\n",
        "\n",
        "# Load the trained model from file\n",
        "trained_dict = torch.load(sys.argv[1], map_location={'cuda:0': 'cpu'})\n",
        "\n",
        "trained_model = MLP(784, [256, 256], 10)\n",
        "trained_model.load_state_dict(trained_dict)\n",
        "\n",
        "if not os.path.exists(\"%s\" % sys.argv[2]):\n",
        "    os.makedirs(\"%s\" % sys.argv[2])\n",
        "\n",
        "# Export the trained model to ONNX\n",
        "dummy_input = Variable(torch.randn(1, 1, 28, 28)) # one black and white 28 x 28 picture will be the input to the model\n",
        "torch.onnx.export(trained_model, dummy_input, \"%s/mnist.onnx\" % sys.argv[2])\n",
        "\n",
        "# Load the ONNX file\n",
        "model = onnx.load(\"%s/mnist.onnx\" % sys.argv[2])\n",
        "\n",
        "# Import the ONNX model to Tensorflow\n",
        "tf_rep = prepare(model)\n",
        "\n",
        "# Input nodes to the model\n",
        "print('inputs:', tf_rep.inputs)\n",
        "\n",
        "# Output nodes from the model\n",
        "print('outputs:', tf_rep.outputs)\n",
        "\n",
        "# All nodes in the model\n",
        "print('tensor_dict:')\n",
        "print(tf_rep.tensor_dict)\n",
        "\n",
        "tf_rep.export_graph(\"%s/mnist.pb\" % sys.argv[2])\n",
        "\n",
        "converter = tf.lite.TFLiteConverter.from_frozen_graph(\n",
        "        \"%s/mnist.pb\" % sys.argv[2], tf_rep.inputs, tf_rep.outputs)\n",
        "tflite_model = converter.convert()\n",
        "open(\"%s/mnist.tflite\" % sys.argv[2], \"wb\").write(tflite_model)\n",
        "\"\"\"\n"
      ],
      "metadata": {
        "id": "mrl8YFf2k6iS"
      },
      "id": "mrl8YFf2k6iS",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "pb: ce code ne semble marcher qu'en local"
      ],
      "metadata": {
        "id": "NdFj3eLJmbPz"
      },
      "id": "NdFj3eLJmbPz"
    },
    {
      "cell_type": "markdown",
      "source": [
        "## utilisation de la librairie pytorch2keras "
      ],
      "metadata": {
        "id": "ZD4hywpCmgWb"
      },
      "id": "ZD4hywpCmgWb"
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install pytorch2keras "
      ],
      "metadata": {
        "id": "zgJ5iFM5k6uC"
      },
      "id": "zgJ5iFM5k6uC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install onnx"
      ],
      "metadata": {
        "id": "20MsyGqNnT-z"
      },
      "id": "20MsyGqNnT-z",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## transformation des opérations du modèle pytorch en modèle tensorflow à la main"
      ],
      "metadata": {
        "id": "76sJ1vu-Dum5"
      },
      "id": "76sJ1vu-Dum5"
    },
    {
      "cell_type": "code",
      "source": [
        " import tensorflow as tf\n",
        " from tensorflow.keras import Sequential"
      ],
      "metadata": {
        "id": "hCqO4HB4mr2j"
      },
      "id": "hCqO4HB4mr2j",
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DilConv(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  This class is the dilated depthwise separable convolution class.\n",
        "  \"\"\"\n",
        "  def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function initializes an instance of the DilConv class.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    C_in: int\n",
        "    C_out: int\n",
        "    kernel_size: int\n",
        "    stride: int\n",
        "    padding: int\n",
        "    dilation: int\n",
        "    affine: boolean\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.net = tf.keras.Sequential()\n",
        "    self.net.add(tf.keras.layers.ReLU())\n",
        "    self.net.add(tf.keras.layers.Conv2D(C_in, kernel_size, strides=stride, dilation_rate=dilation, padding=\"valid\", groups=C_in, data_format=\"channels_last\" ) )\n",
        "    self.net.add(tf.keras.layers.Conv2D(C_out, kernel_size, strides=(1, 1), padding='valid', data_format=\"channels_last\"))\n",
        "    self.net.add(tf.keras.layers.BatchNormalization() )\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function applies the instance of the DilConv class to the data.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x: tf.Tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    return self.net(x)\n"
      ],
      "metadata": {
        "id": "EvopMnyjGUXp"
      },
      "id": "EvopMnyjGUXp",
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class SepConv(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Depthwise separable conv.\n",
        "  DilConv(dilation=1) * 2.\n",
        "  \"\"\"\n",
        "  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function initializes an instance of the SepConv class.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    C_in: int\n",
        "    C_out: int\n",
        "    kernel_size: int\n",
        "    stride: int\n",
        "    padding: int\n",
        "    affine: boolean\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.net = tf.keras.Sequential()\n",
        "    self.net.add(DilConv(C_in, C_in, kernel_size, stride, padding, dilation=1,affine=affine))\n",
        "    self.net.add(DilConv(C_in, C_out, kernel_size, 1, padding, dilation=1, affine=affine))\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function applies the instance of the SepConv class to the data.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x: tf.Tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    return self.net(x)"
      ],
      "metadata": {
        "id": "du3bKAY0gho8"
      },
      "id": "du3bKAY0gho8",
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "layer = tf.keras.layers.ReLU()\n",
        "output = layer([-3.0, -1.0, 0.0, 2.0])\n",
        "output\n",
        "list(output.numpy())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zmA3Sd5WSPWD",
        "outputId": "590f7953-8d0d-486a-fbdc-ad4f1d0af2de"
      },
      "id": "zmA3Sd5WSPWD",
      "execution_count": 58,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.0, 0.0, 0.0, 2.0]"
            ]
          },
          "metadata": {},
          "execution_count": 58
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class FactorizedReduce(tf.keras.layers.Layer):\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  Reduce feature map size by factorized pointwise (stride=2).\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, C_in, C_out, affine=True):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function initializes an instance of the FactorizedReduce class.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    C_in: int\n",
        "    C_out: int\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "    self.conv2d_1 = tf.keras.layers.Conv2D(filters=C_out // 2,\n",
        "                                      strides=2,\n",
        "                                      padding='valid' ,\n",
        "                                      data_format=\"channels_last\",\n",
        "                                      kernel_size=C_in,\n",
        "                                      use_bias=False)\n",
        "    self.conv2d_2 = tf.keras.layers.Conv2D(filters=C_out // 2,\n",
        "                                      strides=2,\n",
        "                                      padding='valid' ,\n",
        "                                      data_format=\"channels_last\",\n",
        "                                      kernel_size=C_in,\n",
        "                                      use_bias=False)\n",
        "    self.concatenation = tf.keras.layers.Concatenate(axis=-1)\n",
        "    self.batch_normalisation = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function applies the instance of the FactorizedReduce class to the data.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x: tf.Tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    x2 = tf.keras.layers.ReLU()(x)\n",
        "    out_1 = self.conv2d_1(x2)\n",
        "    out_2 = self.conv2d_2(x2[:,:,1:,1:])\n",
        "    out = self.concatenation([out_1,out_2])\n",
        "    out = self.batch_normalisation(out)\n",
        "    return out"
      ],
      "metadata": {
        "id": "w7hGJRMaOesE"
      },
      "id": "w7hGJRMaOesE",
      "execution_count": 59,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### test des fonctions créées"
      ],
      "metadata": {
        "id": "DZAqxUZfvDUm"
      },
      "id": "DZAqxUZfvDUm"
    },
    {
      "cell_type": "code",
      "source": [
        "test_tensor = tf.ones((10,10,10,10))\n",
        "model = Sequential()\n",
        "#model.add(tf.keras.layers.ReLU())\n",
        "#model.add(FactorizedReduce(2,1))\n",
        "model.add(DilConv(1,1,1,1,1,1))\n",
        "#model.add(SepConv(1,1,1,1,1))\n",
        "\n",
        "model.compile(loss=\"mse\")\n",
        "model.fit(test_tensor, tf.zeros((10,10,10,10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lEeCdESTvDmF",
        "outputId": "d86daf09-eb4b-4529-a3da-e94f3deb0157"
      },
      "id": "lEeCdESTvDmF",
      "execution_count": 62,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 1s/step - loss: 2.3102e-09\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbacebb52d0>"
            ]
          },
          "metadata": {},
          "execution_count": 62
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "test_tensor = tf.ones((10,10,10,10))\n",
        "model = Sequential()\n",
        "#model.add(tf.keras.layers.ReLU())\n",
        "#model.add(FactorizedReduce(2,1))\n",
        "#model.add(DilConv(1,1,1,1,1,1))\n",
        "model.add(SepConv(1,1,1,1,1))\n",
        "\n",
        "model.compile(loss=\"mse\")\n",
        "model.fit(test_tensor, tf.zeros((10,10,10,10)))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LrMBLDPaY2yc",
        "outputId": "2e7b3ec5-7471-49af-83eb-56feebd8f9dc"
      },
      "id": "LrMBLDPaY2yc",
      "execution_count": 63,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 1s 904ms/step - loss: 0.0000e+00\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.callbacks.History at 0x7fbace9b75d0>"
            ]
          },
          "metadata": {},
          "execution_count": 63
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "nb_input = 10\n",
        "test_tensors = tf.ones((nb_input,12,12,4))\n",
        "test_model = K.models.Sequential()\n",
        "# test_pool = PoolBN(\"max\", 3, 3, 1, \"same\")\n",
        "test_model.add(PoolBN(\"max\", 3, 3, 1, \"same\"))\n",
        "# test_model.add(PoolBN(\"max\", 3, 3, 2, \"same\"))\n",
        "test_model.add(FactorizedReduce(3,3))\n",
        "test_model.add(drop_path(0.3))\n",
        "# test_model.add(SepConv(3,3,3,1,'valid'))\n",
        "\n",
        "# test_model.add(Layer()) # this is apparently a identity layer\n",
        "test_model.compile(loss = \"mse\")\n",
        "test_model.fit(test_tensors, tf.ones((nb_input,5,5,2)))\n",
        "test_model.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 241
        },
        "id": "IkYazi0cXLEM",
        "outputId": "83c7c339-a571-42c2-fb40-f8895dd93d40"
      },
      "id": "IkYazi0cXLEM",
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-60-13abea4ac5dc>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mnb_input\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mtest_tensors\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnb_input\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m12\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mtest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mK\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mSequential\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0;31m# test_pool = PoolBN(\"max\", 3, 3, 1, \"same\")\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mtest_model\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPoolBN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"max\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"same\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'K' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        ""
      ],
      "metadata": {
        "id": "ieUtBJis0P57"
      },
      "id": "ieUtBJis0P57"
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "BihJKQHOOjQz"
      },
      "id": "BihJKQHOOjQz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "uUPx_WjHOk9K"
      },
      "id": "uUPx_WjHOk9K",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow.keras as K\n",
        "\n",
        "\n",
        "\n",
        "class PoolBN(K.layers.Layer):\n",
        "\n",
        "  def __init__(self, pool_type, C, kernel_size, stride, padding):\n",
        "\n",
        "    super().__init__()\n",
        "\n",
        "    self.architecture = K.models.Sequential()\n",
        "\n",
        "    if pool_type == \"max\":\n",
        "\n",
        "      self.architecture.add(K.layers.MaxPooling2D(kernel_size, stride, padding))\n",
        "\n",
        "    elif pool_type == \"avg\":\n",
        "\n",
        "      self.architecture.add(K.layers.AvgPooling2D(kernel_size, stride, padding, count_include_pad=False))\n",
        "\n",
        "    else:\n",
        "\n",
        "      raise ValueError()\n",
        "\n",
        "    self.architecture.add(K.layers.BatchNormalization(C))\n",
        "\n",
        "  \n",
        "\n",
        "  def call(self, x):\n",
        "\n",
        "    return self.architecture(x)"
      ],
      "metadata": {
        "id": "F9XTcy7aCEVB"
      },
      "id": "F9XTcy7aCEVB",
      "execution_count": 62,
      "outputs": []
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.7"
    },
    "toc": {
      "base_numbering": 1,
      "nav_menu": {},
      "number_sections": true,
      "sideBar": true,
      "skip_h1_title": false,
      "title_cell": "Table of Contents",
      "title_sidebar": "Contents",
      "toc_cell": false,
      "toc_position": {},
      "toc_section_display": true,
      "toc_window_display": false
    },
    "colab": {
      "name": "exec_COLAB.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}