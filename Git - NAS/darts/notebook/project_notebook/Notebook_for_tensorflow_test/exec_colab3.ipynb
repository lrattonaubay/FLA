{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "547a821a",
      "metadata": {
        "id": "547a821a"
      },
      "source": [
        "# versions des bibliothèques"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "f9c1fdce",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "f9c1fdce",
        "outputId": "fc1c46e1-cd3b-4222-c202-d0a9af8e8f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Python version: 3.7.13 (default, Apr 24 2022, 01:04:09) \n",
            "[GCC 7.5.0]\n",
            "IPython version: 5.5.0\n",
            "NumPy version: 1.21.6\n",
            "SciPy version: 1.4.1\n",
            "pandas version: 1.3.5\n",
            "matplotlib version: 3.2.2\n",
            "seaborn version : 0.11.2\n",
            "tensorflow version : 2.8.0\n",
            "keras version : 2.8.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "import sys\n",
        "print(\"Python version: {}\". format(sys.version))\n",
        "\n",
        "import IPython\n",
        "from IPython import display\n",
        "from IPython.display import Markdown, display, HTML\n",
        "print(\"IPython version: {}\". format(IPython.__version__))\n",
        "\n",
        "import numpy as np\n",
        "print(\"NumPy version: {}\". format(np.__version__))\n",
        "\n",
        "import scipy as sp\n",
        "from scipy import stats\n",
        "print(\"SciPy version: {}\". format(sp.__version__))\n",
        "\n",
        "import pandas as pd\n",
        "print(\"pandas version: {}\". format(pd.__version__))\n",
        "\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "print(\"matplotlib version: {}\". format(matplotlib.__version__))\n",
        "\n",
        "import seaborn as sns\n",
        "print(\"seaborn version : {}\". format(sns.__version__))\n",
        "sns.set()\n",
        "\n",
        "import tensorflow as tf\n",
        "print(\"tensorflow version : {}\". format(tf.__version__))\n",
        "\n",
        "print(\"keras version : {}\". format(tf.keras.__version__))\n",
        "\n",
        "import logging"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "2325eea0",
      "metadata": {
        "id": "2325eea0"
      },
      "source": [
        "# définition du niveau de trace"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "1aec97d8",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1aec97d8",
        "outputId": "2401fea2-5e99-46c0-ed8a-471f89e2d842"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\n",
            "PYDEV DEBUGGER WARNING:\n",
            "sys.settrace() should not be used when the debugger is being used.\n",
            "This may cause the debugger to stop working correctly.\n",
            "If this is needed, please check: \n",
            "http://pydev.blogspot.com/2007/06/why-cant-pydev-debugger-work-with.html\n",
            "to see how to restore the debug tracing back correctly.\n",
            "Call Location:\n",
            "  File \"/usr/lib/python3.7/doctest.py\", line 1487, in run\n",
            "    sys.settrace(save_trace)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            " +--------------------+\n",
            " |  Tests Unitaires   |\n",
            " +--------------------+\n",
            "1 items had no tests:\n",
            "    __main__\n",
            "0 tests in 1 items.\n",
            "0 passed and 0 failed.\n",
            "Test passed.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import doctest\n",
        "\n",
        "Doc_Fonction = False\n",
        "     \n",
        "# Définition  du niveau de traces : DEBUG, INFO, WARNING, ERROR ou CRITICAL\n",
        "     \n",
        "niveauTrace = logging.DEBUG\n",
        "#niveauTrace = logging.INFO\n",
        "#niveauTrace = logging.WARNING\n",
        "#niveauTrace = logging.ERROR\n",
        "#niveauTrace = logging.CRITICAL\n",
        "    \n",
        "logging.basicConfig(format='%(levelname)s:%(message)s', level=niveauTrace)\n",
        "    \n",
        "if (Doc_Fonction):\n",
        "    #\"\"\"\n",
        "    # Mode d'emploi du Décorateur Trace_appel\n",
        "    print(\"\\n\\n    Mode d'emploi des fonctions\")\n",
        "    print(    \"    ---------------------------\")\n",
        "    help(Trace_appel)\n",
        "    #\"\"\"\n",
        "    \n",
        "# Test unitaire des fonctions\n",
        "print(\"\\n +--------------------+\")\n",
        "print(  \" |  Tests Unitaires   |\")\n",
        "print(  \" +--------------------+\")\n",
        "doctest.testmod(verbose = True)\n",
        "print(\"\")\n",
        "#\"\"\""
      ]
    },
    {
      "cell_type": "markdown",
      "id": "9c51e732",
      "metadata": {
        "id": "9c51e732"
      },
      "source": [
        "# décorateur Trace_appel"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "65dd31b3",
      "metadata": {
        "id": "65dd31b3"
      },
      "outputs": [],
      "source": [
        "import cProfile\n",
        "import io\n",
        "import pstats\n",
        "#from pstats import SortKey\n",
        "import tracemalloc\n",
        "from functools import wraps\n",
        "\n",
        "\n",
        "def Trace_appel(Doc_Fonction=False, Mesure_Temps=False, Mesure_Memoire=False, Valeur_Arguments=True):\n",
        "    \"\"\"\n",
        "    Décorateur qui permet de tracer l'appel des fonctions (niveau INFO)\n",
        "    et la documentation de la fonction, ses arguments d'entree et  \n",
        "    la valeur de retour (niveau DEBUG) sans surcharger le code\n",
        "    En mode DEBUG, on peut aussi tracer le temps d'exécution en utilisant cprofile\n",
        "    - ncalls = nombre d'appels\n",
        "    - tottime = temps passé dans la fonction en excluant le temps passé dans les sous-fonctions appelées\n",
        "    - percall = tottime / ncalls\n",
        "    - cumtime = temps cumulé passé dans la fonction et dans les sous-fonctions appelées\n",
        "    - percall = cumtime / ncalls\n",
        "    \n",
        "    Utilisation :\n",
        "    @Trace_appel(information_a_tracer=True)\n",
        "    def essai(x,y,z):\n",
        "        code de la fonction essai\n",
        "        return codeRetour\n",
        "        \n",
        "    \"\"\"\n",
        "    def Executer_appel(fonction):\n",
        "        \n",
        "        @wraps(fonction)\n",
        "        def wrapper(*args, **kwargs):\n",
        "            logging.info(\"\")\n",
        "            logging.info(\">>>>>> Appel de {} >>>>>>\".format(fonction.__name__))\n",
        "            if (Doc_Fonction): logging.debug(fonction.__doc__)\n",
        "            arguments = \"  ... Arguments :\"        \n",
        "            for indice,valeur in enumerate(args):\n",
        "                arguments += \"  N°{}={} \".format(indice+1, valeur)\n",
        "            for clef, valeur in kwargs.items():\n",
        "                arguments += \"  {}={}\".format(clef, valeur)\n",
        "            if (Valeur_Arguments) : logging.debug(\"{}\".format(arguments))\n",
        "            \n",
        "            if (Mesure_Temps):\n",
        "                # Temps d'execution\n",
        "                pr = cProfile.Profile()\n",
        "                pr.enable()\n",
        "            \n",
        "            if (Mesure_Memoire) : tracemalloc.start()\n",
        "            \n",
        "            retour = fonction(*args, **kwargs)\n",
        "            \n",
        "            if (Mesure_Temps) :\n",
        "                pr.disable()\n",
        "                s = io.StringIO()\n",
        "                #sortby = SortKey.CUMULATIVE  # 'cumulative'\n",
        "                sortby = \"cumulative\"  # 'cumulative'\n",
        "                ps = pstats.Stats(pr, stream=s).sort_stats(sortby)\n",
        "                ps.print_stats(10)\n",
        "            \n",
        "            if (Valeur_Arguments) : logging.debug(\"  ... Resultat : {}\".format(retour))\n",
        "            \n",
        "            if (Mesure_Temps) : \n",
        "                logging.debug(\"\")\n",
        "                logging.debug(\"    >>>>> Temps d'exécution >>>>\")\n",
        "                logging.debug(s.getvalue())\n",
        "            \n",
        "            if (Mesure_Memoire) :\n",
        "                current, peak =  tracemalloc.get_traced_memory()\n",
        "                memoire_en_octets = peak * 1.024\n",
        "                if (memoire_en_octets < 1000) :\n",
        "                    Unite = \"octets\"\n",
        "                else :\n",
        "                    memoire_en_octets /= 1000\n",
        "                    if (memoire_en_octets < 1000) :\n",
        "                        Unite = \"Ko\"\n",
        "                    else:\n",
        "                        memoire_en_octets /= 1000\n",
        "                        if (memoire_en_octets < 1000) :\n",
        "                            Unite = \"Mo\"\n",
        "                        else:\n",
        "                            Unite = \"Go\"\n",
        "                      \n",
        "                #if(peak )\n",
        "                tracemalloc.stop()\n",
        "                logging.debug(\"\")\n",
        "                logging.debug(\"    >>>>> Mémoire allouée : {:.3f} {} >>>>\".format(memoire_en_octets,\n",
        "                                                                              Unite))\n",
        "            \n",
        "            logging.info(\"----- Fin de {} -----\".format(fonction.__name__))\n",
        "        \n",
        "            return retour\n",
        "        \n",
        "        return wrapper\n",
        "    \n",
        "    return Executer_appel"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "23172690",
      "metadata": {
        "id": "23172690"
      },
      "source": [
        "# fichier ops.py transformé en tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "422f20af",
      "metadata": {
        "id": "422f20af"
      },
      "outputs": [],
      "source": [
        "# %load C:\\Users\\cnerin\\Documents\\NAS\\code_tensorflow\\code_branche_gitlab_main\\nas\\darts\\project\\tensorflow\\ops.py\n",
        "import tensorflow as tf\n",
        "from keras import Sequential\n",
        "\n",
        "class DropPath(tf.keras.layers.Layer):\n",
        "  def __init__(self, p=0):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    Creates an instance of the class DropPath.\n",
        "\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    p: float (0 if you don't want, else, between 0 and 1)\n",
        "\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.p=p\n",
        "  \n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    ---------------\n",
        "    Similar to the forward function in tensorflow, this function is used to train the AI.\n",
        "\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x : tf.Tensor\n",
        "\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    if self._trainable and self.p>0:\n",
        "      keep_prob = 1. - self.p\n",
        "      shape = (tf.shape(x)[0],) + (1,) * (len(tf.shape(x)) - 1)\n",
        "      mask = tf.keras.backend.random_bernoulli(shape, keep_prob, dtype = tf.float32)\n",
        "      return x/keep_prob*mask\n",
        "    return x\n",
        "\n",
        "class PoolBN(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Description\n",
        "  ---------------\n",
        "  This class is used to handle both the max pooling layer and the average pooling layer.\n",
        "  In both cases, the pooling is followed by a layer of batch normalization.\n",
        "  \"\"\"\n",
        "  def __init__(self, pool_type, C, kernel_size, stride, padding, affine=False):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    Creates an instance of the class PoolBN\n",
        "\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    pool_type: string (either \"max\" or \"avg\")\n",
        "    C: int, axis on which to perform batch normalization\n",
        "    kernel_size: int or enumerable of ints\n",
        "    stride: int or enumerable of ints\n",
        "    padding: string\n",
        "    affine: bool\n",
        "\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    if pool_type == \"max\":\n",
        "      self.pooling = tf.keras.layers.MaxPooling2D(kernel_size, stride, padding)\n",
        "    elif pool_type == \"avg\":\n",
        "      self.pooling = tf.keras.layers.AveragePooling2D(kernel_size, stride, padding)\n",
        "    else:\n",
        "      raise ValueError()\n",
        "    self.batchnormalisation = tf.keras.layers.BatchNormalization()\n",
        "  \n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    ---------------\n",
        "    Similar to the forward function in tensorflow, this function is used to train the AI.\n",
        "\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x : tf.Tensor\n",
        "\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "\n",
        "    if isinstance(x,tf.Tensor):\n",
        "      # x is a Tensor\n",
        "      out1 = self.pooling(x)\n",
        "      out = self.batchnormalisation(out1)\n",
        "    elif x == ():\n",
        "      out = ()\n",
        "    else:\n",
        "      # x is a list\n",
        "      out = ()\n",
        "      for i in range(len(x)):\n",
        "        if isinstance(len(x[i]),int):\n",
        "          # we want to know if x[i] is a tensor list\n",
        "          if len(x[i]) == 0 :\n",
        "            # x[i] contains no element\n",
        "            print(\"x[i_pool_bn] est vide\")\n",
        "          elif len(x[i]) == 1 :\n",
        "            # x[i] contains only one tensor\n",
        "            entree = x[i][0]\n",
        "            x_i = self.pooling(entree)\n",
        "            out_i = self.batchnormalisation(x_i)\n",
        "            out = out + (out_i,)\n",
        "          else:\n",
        "            # x[i] is a list containing more than one tensor\n",
        "            for j in range(len(x[i])):\n",
        "              entree = x[i][j]\n",
        "              if entree.get_shape()[1] >= 3 and entree.get_shape()[2] >= 3:\n",
        "                # We check that the dimensions of the tensor respect the conditions necessary to apply the layer.\n",
        "                x_i_j = self.pooling(entree)\n",
        "                out_i_j = self.batchnormalisation(x_i_j)\n",
        "                out = out + (out_i_j,)\n",
        "              else:\n",
        "                out_i_j = tf.constant(1,shape=(3, 3, 3, 4))\n",
        "                out = out + (out_i_j,)            \n",
        "        else:\n",
        "          # x[i] is a Tensor\n",
        "          x_i = self.pooling(x[i])\n",
        "          out_i = self.batchnormalisation(x_i)\n",
        "          out = out + (out_i,)\n",
        "    return out\n",
        "    \n",
        "class StdConv(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Description\n",
        "  ---------------\n",
        "  This class is used to do an average convolution.\n",
        "  It is composed of a relu followed by the convolution.\n",
        "  Then a batchnormalisation is applied.\n",
        "  The model used is a sequential keras model.\n",
        "  \"\"\"\n",
        "  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    Creates an instance of the class StdConv\n",
        "\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    C_in: int\n",
        "    C_out: int\n",
        "    kernel_size: int\n",
        "    stride: int\n",
        "    padding: string (\"same\" ou \"valid\")\n",
        "    affine: bool\n",
        "\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "    self.conv2d = tf.keras.layers.Conv2D(filters=C_out,\n",
        "                                        kernel_size=kernel_size,\n",
        "                                        strides=stride,\n",
        "                                        padding=padding, \n",
        "                                        use_bias=False )\n",
        "    self.batch_normalisation = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "  \n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description:\n",
        "    ---------------\n",
        "    Similar to the forward function in tensorflow, this function is used to train the AI.\n",
        "\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x : tf.Tensor\n",
        "\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    if isinstance(x,tf.Tensor):\n",
        "      # x is a Tensor\n",
        "      out_relu = self.relu(x)\n",
        "      out_conv2d = self.conv2d(out_relu)\n",
        "      out_batch_normalisation = self.batch_normalisation(out_conv2d)\n",
        "      out = out_batch_normalisation\n",
        "    else:\n",
        "      # x is a list\n",
        "      out = ()\n",
        "      for i in range(len(x)):\n",
        "        if isinstance(len(x[i]),int):\n",
        "          # we want to know if x[i] is a tensor list\n",
        "          if len(x[i]) == 0 :\n",
        "            # x[i] contains no element\n",
        "            print(\"x[i_std_conv] est vide\")\n",
        "          elif len(x[i]) == 1 :\n",
        "            # x[i] contains only one tensor\n",
        "            entree = x[i][0]\n",
        "            \n",
        "            #print(\"entree.get_shape() = {}\".format(entree.get_shape()))\n",
        "            x_i = self.relu(entree)\n",
        "            out_conv2d_i = self.conv2d(x_i)\n",
        "            \n",
        "            out_i = self.batch_normalisation(x_i)\n",
        "            \n",
        "            out = out + (out_i,)\n",
        "          else:\n",
        "            # x[i] is a list containing more than one tensor\n",
        "            for j in range(len(x[i])):\n",
        "              entree = x[i][j]\n",
        "              x_i_j = self.relu(entree)\n",
        "              out_conv2d_i_j = self.conv2d(x_i_j)\n",
        "              out_i_j = self.batch_normalisation(out_conv2d_i_j)\n",
        "              out = out + (out_i_j,)\n",
        "            \n",
        "        else:\n",
        "          # x[i] is a Tensor\n",
        "\n",
        "          x_i = self.relu(x[i])\n",
        "          out_conv2d_i = self.conv2d(x_i)\n",
        "          out_i = self.batch_normalisation(out_conv2d_i)\n",
        "          out = out + (out_i,)\n",
        "    return out\n",
        "\n",
        "class DilConv(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  This class is the dilated depthwise separable convolution class.\n",
        "  \"\"\"\n",
        "  def __init__(self, C_in, C_out, kernel_size, stride, padding, dilation, affine=True):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function initializes an instance of the DilConv class.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    C_in: int\n",
        "    C_out: int\n",
        "    kernel_size: int\n",
        "    stride: int\n",
        "    padding: int\n",
        "    dilation: int\n",
        "    affine: boolean\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.net = tf.keras.Sequential()\n",
        "    self.net.add(tf.keras.layers.ReLU())\n",
        "    self.net.add(tf.keras.layers.Conv2D(C_in, kernel_size, strides=stride, dilation_rate=dilation, padding=\"valid\", groups=C_in, data_format=\"channels_last\" ) )\n",
        "    self.net.add(tf.keras.layers.Conv2D(C_out, kernel_size, strides=(1, 1), padding='valid', data_format=\"channels_last\"))\n",
        "    self.net.add(tf.keras.layers.BatchNormalization() )\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function applies the instance of the DilConv class to the data.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x: tf.Tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    return self.net(x)\n",
        "\n",
        "\n",
        "\n",
        "class SepConv(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Depthwise separable conv.\n",
        "  DilConv(dilation=1) * 2.\n",
        "  \"\"\"\n",
        "  def __init__(self, C_in, C_out, kernel_size, stride, padding, affine=True):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function initializes an instance of the SepConv class.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    C_in: int\n",
        "    C_out: int\n",
        "    kernel_size: int\n",
        "    stride: int\n",
        "    padding: int\n",
        "    affine: boolean\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.net = tf.keras.Sequential()\n",
        "    self.net.add(DilConv(C_in, C_in, kernel_size, stride, padding, dilation=1,affine=affine))\n",
        "    self.net.add(DilConv(C_in, C_out, kernel_size, 1, padding, dilation=1, affine=affine))\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function applies the instance of the SepConv class to the data.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x: tf.Tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    return self.net(x)\n",
        "\n",
        "class FactorizedReduce(tf.keras.layers.Layer):\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  Reduce feature map size by factorized pointwise (stride=2).\n",
        "\n",
        "  \"\"\"\n",
        "\n",
        "  def __init__(self, C_in, C_out, affine=True):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function initializes an instance of the FactorizedReduce class.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    C_in: int\n",
        "    C_out: int\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    No output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.relu = tf.keras.layers.ReLU()\n",
        "    self.conv2d_1 = tf.keras.layers.Conv2D(filters=C_out // 2,\n",
        "                                      strides=2,\n",
        "                                      padding='same' ,\n",
        "                                      data_format=\"channels_last\",\n",
        "                                      kernel_size=C_in,\n",
        "                                      use_bias=False)\n",
        "    self.conv2d_2 = tf.keras.layers.Conv2D(filters=C_out // 2,\n",
        "                                      strides=2,\n",
        "                                      padding='same' ,\n",
        "                                      data_format=\"channels_last\",\n",
        "                                      kernel_size=C_in,\n",
        "                                      use_bias=False)\n",
        "    self.concatenation = tf.keras.layers.Concatenate(axis=-1)\n",
        "    self.batch_normalisation = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function applies the instance of the FactorizedReduce class to the data.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x: tf.Tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    output: tf.Tensor\n",
        "    \"\"\"\n",
        "    if isinstance(x,tf.Tensor):\n",
        "      # x is a Tensor\n",
        "      x2 = tf.keras.layers.ReLU()(x)\n",
        "      \n",
        "      out_1 = self.conv2d_1(x2)\n",
        "      out_2 = self.conv2d_2(x2[:,:,1:,1:])\n",
        "      out_3 = self.concatenation([out_1,out_2])\n",
        "      \n",
        "      out = self.batch_normalisation(out_3)\n",
        "      \n",
        "    else:\n",
        "      # x is a list\n",
        "      out = ()\n",
        "      for i in range(len(x)):\n",
        "        if isinstance(len(x[i]),int):\n",
        "          # we want to know if x[i] is a tensor list\n",
        "          if len(x[i]) == 0 :\n",
        "            # x[i] contains no element\n",
        "            out = ()\n",
        "          elif len(x[i]) == 1 :\n",
        "            # x[i] contains only one tensor\n",
        "            entree = x[i][0]\n",
        "            #print(\"entree.get_shape() = {}\".format(entree.get_shape()))\n",
        "            x2_i = tf.keras.layers.ReLU()(entree)\n",
        "            out_1_i = self.conv2d_1(x2_i)\n",
        "            out_2_i = self.conv2d_2(x2_i[:,:,1:,1:])\n",
        "            out_3_i = self.concatenation([out_1_i,out_2_i])\n",
        "            out_i = self.batch_normalisation(out_3_i)\n",
        "            out = out + (out_i,)\n",
        "          else:\n",
        "            # x[i] is a list containing more than one tensor\n",
        "            for j in range(len(x[i])):\n",
        "              entree = x[i][j]\n",
        "              x2_i_j = tf.keras.layers.ReLU()(entree)\n",
        "              out_1_i_j = self.conv2d_1(x2_i_j)\n",
        "              out_2_i_j = self.conv2d_2(x2_i_j[:,:,1:,1:])\n",
        "              out_3_i_j = self.concatenation([out_1_i_j,out_2_i_j])\n",
        "              out_i_j = self.batch_normalisation(out_3_i_j)\n",
        "              out = out + (out_i_j,)\n",
        "            \n",
        "        else:\n",
        "          # x[i] is a Tensor\n",
        "          x2_i = tf.keras.layers.ReLU()(x[i])\n",
        "          out_1_i = self.conv2d_1(x2_i)\n",
        "          out_2_i = self.conv2d_2(x2_i[:,:,1:,1:])\n",
        "          out_3_i = self.concatenation([out_1_i,out_2_i])\n",
        "          out_i = self.batch_normalisation(out_3_i)\n",
        "          out = out + (out_i,)\n",
        "    return out"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6865a61f",
      "metadata": {
        "id": "6865a61f"
      },
      "source": [
        "# fichier choices_utils.py transformé en tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c229b611",
      "metadata": {
        "id": "c229b611"
      },
      "outputs": [],
      "source": [
        "# %load C:\\Users\\cnerin\\Documents\\NAS\\code_tensorflow\\code_branche_gitlab_main\\nas\\darts\\project\\tensorflow\\choices_utils.py\n",
        "from typing import Any, Dict, List, Optional\n",
        "from collections import defaultdict\n",
        "\n",
        "\n",
        "def get_fixed_value(label: str) -> Any:\n",
        "    ret = get_current_context('fixed')\n",
        "    try:\n",
        "        return ret[generate_new_label(label)]\n",
        "    except KeyError:\n",
        "        raise KeyError(f'Fixed context with {label} not found. Existing values are: {ret}')\n",
        "\n",
        "\n",
        "def get_current_context(key: str) -> Any:\n",
        "    return ContextStack.top(key)\n",
        "\n",
        "class ContextStack:\n",
        "    _stack: Dict[str, List[Any]] = defaultdict(list)\n",
        "\n",
        "    def __init__(self, key: str, value: Any):\n",
        "        self.key = key\n",
        "        self.value = value\n",
        "\n",
        "    def __enter__(self):\n",
        "        self.push(self.key, self.value)\n",
        "        return self\n",
        "\n",
        "    def __exit__(self, *args, **kwargs):\n",
        "        self.pop(self.key)\n",
        "\n",
        "    @classmethod\n",
        "    def push(cls, key: str, value: Any):\n",
        "        cls._stack[key].append(value)\n",
        "\n",
        "    @classmethod\n",
        "    def pop(cls, key: str) -> None:\n",
        "        cls._stack[key].pop()\n",
        "\n",
        "    @classmethod\n",
        "    def top(cls, key: str) -> Any:\n",
        "        return cls._stack[key][-1]\n",
        "\n",
        "def generate_new_label(label: Optional[str]):\n",
        "    if label is None:\n",
        "        return \"model\"\n",
        "    return label"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "189bfeea",
      "metadata": {
        "id": "189bfeea"
      },
      "source": [
        "# fichier choices.py transformé en tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "32adba1e",
      "metadata": {
        "id": "32adba1e"
      },
      "outputs": [],
      "source": [
        "# %load C:\\Users\\cnerin\\Documents\\NAS\\code_tensorflow\\code_branche_gitlab_main\\nas\\darts\\project\\tensorflow\\choices.py\n",
        "import tensorflow as tf\n",
        "from typing import Union, Dict, List, Optional, OrderedDict\n",
        "import warnings\n",
        "#import choices_utils\n",
        "\n",
        "class LayerChoice(tf.Module):\n",
        "\n",
        "    def create_fixed_module(cls, candidates: Union[Dict[str, tf.Module], List[tf.Module]], *,\n",
        "                            label: Optional[str] = None, **kwargs):\n",
        "        chosen = get_fixed_value(label)\n",
        "        if isinstance(candidates, list):\n",
        "            return candidates[int(chosen)]\n",
        "        else:\n",
        "            return candidates[chosen]\n",
        "\n",
        "    def __init__(self, candidates: Union[Dict[str, tf.Module], List[tf.Module]], *,\n",
        "                 prior: Optional[List[float]] = None, label: Optional[str] = None, **kwargs):\n",
        "                 \n",
        "        super(LayerChoice, self).__init__()\n",
        "        if 'key' in kwargs:\n",
        "            warnings.warn(f'\"key\" is deprecated. Assuming label.')\n",
        "            label = kwargs['key']\n",
        "        if 'return_mask' in kwargs:\n",
        "            warnings.warn(f'\"return_mask\" is deprecated. Ignoring...')\n",
        "        if 'reduction' in kwargs:\n",
        "            warnings.warn(f'\"reduction\" is deprecated. Ignoring...')\n",
        "        self.candidates = candidates\n",
        "        self.prior = prior or [1 / len(candidates) for _ in range(len(candidates))]\n",
        "        assert abs(sum(self.prior) - 1) < 1e-5, 'Sum of prior distribution is not 1.'\n",
        "        self._label = generate_new_label(label)\n",
        "        self._modules: Dict[str, Optional[tf.Module]] = OrderedDict()\n",
        "\n",
        "        self.names = []\n",
        "        if isinstance(candidates, dict):\n",
        "            for name, module in candidates.items():\n",
        "                assert name not in [\"length\", \"reduction\", \"return_mask\", \"_key\", \"key\", \"names\"], \\\n",
        "                    \"Please don't use a reserved name '{}' for your module.\".format(name)\n",
        "                self._modules[name] = module\n",
        "                self.names.append(name)\n",
        "        elif isinstance(candidates, list):\n",
        "            for i, module in enumerate(candidates):\n",
        "                self._modules[name] = module\n",
        "                self.names.append(str(i))\n",
        "        else:\n",
        "            raise TypeError(\"Unsupported candidates type: {}\".format(type(candidates)))\n",
        "        self._first_module = self._modules[self.names[0]]  # to make the dummy forward meaningful\n",
        "\n",
        "    @property\n",
        "    def key(self):\n",
        "        return self._key()\n",
        "\n",
        "    def _key(self):\n",
        "        warnings.warn('Using key to access the identifier of LayerChoice is deprecated. Please use label instead.',\n",
        "                      category=DeprecationWarning)\n",
        "        return self._label\n",
        "\n",
        "    @property\n",
        "    def label(self):\n",
        "        return self._label\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(idx, str):\n",
        "            return self._modules[idx]\n",
        "        return list(self)[idx]\n",
        "\n",
        "    def __setitem__(self, idx, module):\n",
        "        key = idx if isinstance(idx, str) else self.names[idx]\n",
        "        return setattr(self, key, module)\n",
        "\n",
        "    def __delitem__(self, idx):\n",
        "        if isinstance(idx, slice):\n",
        "            for key in self.names[idx]:\n",
        "                delattr(self, key)\n",
        "        else:\n",
        "            if isinstance(idx, str):\n",
        "                key, idx = idx, self.names.index(idx)\n",
        "            else:\n",
        "                key = self.names[idx]\n",
        "            delattr(self, key)\n",
        "        del self.names[idx]\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return map(lambda name: self._modules[name], self.names)\n",
        "\n",
        "    @property\n",
        "    def choices(self):\n",
        "        return self._choices()\n",
        "\n",
        "    def _choices(self):\n",
        "        warnings.warn(\"layer_choice.choices is deprecated. Use `list(layer_choice)` instead.\", category=DeprecationWarning)\n",
        "        return list(self)\n",
        "\n",
        "    def __call__(self, x):\n",
        "        warnings.warn('You should not run forward of this module directly.')\n",
        "        return self._first_module(x)\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'LayerChoice({self.candidates}, label={repr(self.label)})'\n",
        "\n",
        "\n",
        "class InputChoice(tf.Module):\n",
        "  \n",
        "    @classmethod\n",
        "    def create_fixed_module(cls, n_candidates: int, n_chosen: Optional[int] = 1, reduction: str = 'sum', *,\n",
        "                            prior: Optional[List[float]] = None, label: Optional[str] = None, **kwargs):\n",
        "        return ChosenInputs(get_fixed_value(label), reduction=reduction)\n",
        "\n",
        "    def __init__(self, n_candidates: int, n_chosen: Optional[int] = 1,\n",
        "                 reduction: str = 'sum', *,\n",
        "                 prior: Optional[List[float]] = None, label: Optional[str] = None, **kwargs):\n",
        "        super(InputChoice, self).__init__()\n",
        "        if 'key' in kwargs:\n",
        "            warnings.warn(f'\"key\" is deprecated. Assuming label.')\n",
        "            label = kwargs['key']\n",
        "        if 'return_mask' in kwargs:\n",
        "            warnings.warn(f'\"return_mask\" is deprecated. Ignoring...')\n",
        "        if 'choose_from' in kwargs:\n",
        "            warnings.warn(f'\"reduction\" is deprecated. Ignoring...')\n",
        "        self.n_candidates = n_candidates\n",
        "        self.n_chosen = n_chosen\n",
        "        self.reduction = reduction\n",
        "        self.prior = prior or [1 / n_candidates for _ in range(n_candidates)]\n",
        "        assert self.reduction in ['mean', 'concat', 'sum', 'none']\n",
        "        self._label = generate_new_label(label)\n",
        "\n",
        "    @property\n",
        "    def key(self):\n",
        "        return self._key()\n",
        "\n",
        "    def _key(self):\n",
        "        warnings.warn('Using key to access the identifier of InputChoice is deprecated. Please use label instead.',\n",
        "                      category=DeprecationWarning)\n",
        "        return self._label\n",
        "\n",
        "    @property\n",
        "    def label(self):\n",
        "        return self._label\n",
        "\n",
        "    def __call__(self, candidate_inputs: List[tf.Tensor]) -> tf.Tensor:\n",
        "        warnings.warn('You should not run forward of this module directly.')\n",
        "        return candidate_inputs[0]\n",
        "\n",
        "    def __repr__(self):\n",
        "        return f'InputChoice(n_candidates={self.n_candidates}, n_chosen={self.n_chosen}, ' \\\n",
        "            f'reduction={repr(self.reduction)}, label={repr(self.label)})'\n",
        "\n",
        "\n",
        "class ChosenInputs(tf.Module):\n",
        "    \"\"\"\n",
        "    A module that chooses from a tensor list and outputs a reduced tensor.\n",
        "    The already-chosen version of InputChoice.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, chosen: Union[List[int], int], reduction: str):\n",
        "        super().__init__()\n",
        "        self.chosen = chosen if isinstance(chosen, list) else [chosen]\n",
        "        self.reduction = reduction\n",
        "\n",
        "    def __call__(self, candidate_inputs):\n",
        "        return self._tensor_reduction(self.reduction, [candidate_inputs[i] for i in self.chosen])\n",
        "\n",
        "    def _tensor_reduction(self, reduction_type, tensor_list):\n",
        "        if reduction_type == 'none':\n",
        "            return tensor_list\n",
        "        if not tensor_list:\n",
        "            return None  # empty. return None for now\n",
        "        if len(tensor_list) == 1:\n",
        "            return tensor_list[0]\n",
        "        if reduction_type == 'sum':\n",
        "            return sum(tensor_list)\n",
        "        if reduction_type == 'mean':\n",
        "            return sum(tensor_list) / len(tensor_list)\n",
        "        if reduction_type == 'concat':\n",
        "            return tf.cat(tensor_list, dim=1)\n",
        "        raise ValueError(f'Unrecognized reduction policy: \"{reduction_type}\"')"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1b3c271a",
      "metadata": {
        "id": "1b3c271a"
      },
      "source": [
        "# fichier model.py transformé en tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "8714288b",
      "metadata": {
        "id": "8714288b"
      },
      "outputs": [],
      "source": [
        "# %load C:\\Users\\cnerin\\Documents\\NAS\\code_tensorflow\\code_branche_gitlab_main\\nas\\darts\\project\\tensorflow\\model.py\n",
        "from collections import OrderedDict\n",
        "#import ops\n",
        "#from ops import DropPath\n",
        "#from choices import LayerChoice, InputChoice\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "\n",
        "class AuxiliaryHead(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "  Description\n",
        "  ---------------\n",
        "  This class allows the creation of an auxiliary head that takes 2/3 of the\n",
        "  place of network to let the gradient flow well .\n",
        "  \"\"\"\n",
        "  def __init__(self, input_size, C, n_classes):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function creates an instance of the AuxiliaryHead class.\n",
        "    We are assuming that the input size is either 7x7 or 8x8.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    input_size: int\n",
        "    C: int\n",
        "    n_classes: int\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    no output\n",
        "    \"\"\"\n",
        "    assert input_size in [7, 8]\n",
        "    super().__init__()\n",
        "    self.net = tf.keras.Sequential()\n",
        "    self.net.add(tf.keras.layers.ReLU())\n",
        "    self.net.add(tf.keras.AvgPooling2D(5, \n",
        "                                       stride=input_size - 5,\n",
        "                                       padding=0,\n",
        "                                       count_include_pad=False)) # 2x2 out\n",
        "    self.net.add(tf.keras.layers.Conv2D(filters=128, \n",
        "                                        kernel_size=C, \n",
        "                                        use_bias=False, \n",
        "                                        data_format=\"channels_last\"))\n",
        "    self.net.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
        "    self.net.add(tf.keras.layers.ReLU())\n",
        "    self.net.add(tf.keras.layers.Conv2D(filters=768, \n",
        "                                        kernel_size= 128,\n",
        "                                        data_format=\"channels_last\",\n",
        "                                        use_bias=False)) # 1x1 out\n",
        "    self.net.add(tf.keras.layers.BatchNormalization(axis=-1))\n",
        "    self.net.add(tf.keras.layers.ReLU())\n",
        "    self.linear = tf.layers.Linear(768, n_classes)\n",
        "\n",
        "\n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function applies the instance of the AuxiliaryHead class to the data.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x: tf.Tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    logits : tf.Tensor\n",
        "    \"\"\"\n",
        "    out = self.net(x)\n",
        "    out = out.view(out.size(0), -1)  # flatten\n",
        "    logits = self.linear(out)\n",
        "    return logits\n",
        "\n",
        "    \n",
        "class Node(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, node_id, num_prev_nodes, channels, num_downsample_connect):\n",
        "        \"\"\"\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.ops = []\n",
        "        choice_keys = []\n",
        "\n",
        "        for i in range(num_prev_nodes):\n",
        "            stride = 2 if i < num_downsample_connect else 1\n",
        "            choice_keys.append(\"{}_p{}\".format(node_id, i))\n",
        "            self.ops.append(\n",
        "                LayerChoice(OrderedDict([\n",
        "                                        (\"maxpool\", PoolBN('max', channels, 3, stride, \"same\", affine=False)),\n",
        "                                        (\"avgpool\", PoolBN('avg', channels, 3, stride, \"same\", affine=False)),\n",
        "                                        (\"skipconnect\",tf.identity(channels) if stride == 1 else FactorizedReduce(channels,channels,affine=False)),\n",
        "                                        (\"sepconv3x3\", SepConv(channels, channels, 3, stride, 1, affine=False)),\n",
        "                                        (\"sepconv5x5\", SepConv(channels, channels, 5, stride, 2, affine=False)),\n",
        "                                        (\"dilconv3x3\", DilConv(channels, channels, 3, stride, 2, 2, affine=False)),\n",
        "                                        (\"dilconv5x5\", DilConv(channels, channels, 5, stride, 4, 2, affine=False))\n",
        "                                        ]), label=choice_keys[-1]))\n",
        "        self.drop_path = DropPath()\n",
        "        self.input_switch = InputChoice(n_candidates=len(choice_keys),n_chosen=2,label=\"{}_switch\".format(node_id))\n",
        "        \n",
        "    def call(self, prev_nodes):\n",
        "\n",
        "        \"\"\"\n",
        "        - prev_nodes est une liste des sorties de tous les noeuds précédents  :\n",
        "                -> au noeud n°1, prev_nodes = [[s0][s1]] où s0 et s1 sont les sorties cell n-2 et n-1\n",
        "                -> au noeud n°2, prev_nodes = [[s0][s1][n1]] où s0 et s1 sont les sorties cell n-2 et n-1, et n1 est la sortie du noeud n°1\n",
        "                -> au noeud n°3, prev_nodes = [[s0][s1][n1][n2]] où s0 et s1 sont les sorties cell n-2 et n-1, n1 est la sortie du noeud n°1 et n2 est la sortie du noeud n°2\n",
        "                ...\n",
        "        - self.ops fait référence au paramètre self de la fonction call, donc concerne le noeud courant :\n",
        "                -> au noeud n°1, self.ops = [LayerChoice(OrderedDict[\"toutes les sorties des opérations listées dans ce paramètre\"], \"noeud de provenance\"=s0), \n",
        "                                            LayerChoice(OrderedDict[\"toutes les sorties des opérations listées dans ce paramètre\"], \"noeud de provenance\"=s1)]\n",
        "                -> au noeud n°2, self.ops = [LayerChoice(OrderedDict[\"toutes les sorties des opérations listées dans ce paramètre\"], \"noeud de provenance\"=s0), \n",
        "                                            LayerChoice(OrderedDict[\"toutes les sorties des opérations listées dans ce paramètre\"], \"noeud de provenance\"=s1),\n",
        "                                            LayerChoice(OrderedDict[\"toutes les sorties des opérations listées dans ce paramètre\"], \"noeud de provenance\"=n1)\n",
        "                ...\n",
        "        \"\"\"\n",
        "        assert len(self.ops) == len(prev_nodes)\n",
        "\n",
        "        \"\"\"\n",
        "                    ======================= ANALYSE DES ACTION DE LA FONCTION=======================\n",
        "        Propositions basées sur le fait que l'entrée dans le Node est une liste d'entrées uniques pour chaque noeud précédent, et non une liste d'entrée pour chaque \n",
        "                                opération de chaque noeud précédent -> cela implique que la sortie out du Node elle aussi est unique:\n",
        "\n",
        "\n",
        "        Vérifier ce que fait le call de la classe LayerChoice \n",
        "                -> proposition : liste des valeurs de sortie de chaque opération appliquée à la sortie du noeud courrant, node serait égal à la somme de toutes les \n",
        "                                    entrées du noeud, et op(node) retournerait un tensor d'une dimension d'une logueur égale au nombre d'opérations dans le LayerChoice\n",
        "        \"\"\"        \n",
        "\n",
        "        out = [op(node) for op, node in zip(self.ops, prev_nodes)]\n",
        "\n",
        "\n",
        "        \"\"\"\n",
        "        Vérifier ce que fait le DropPath \n",
        "                -> proposition en partant de la proposition précédente : out devient une atténuation de lui-même, en fonction de la proba de performance de l'opération o, le \n",
        "                                    but étant de mettre plus en évidence les opérations donnant les meilleurs résultats\n",
        "        \"\"\"\n",
        "        out = [self.drop_path(o) if o is not None else None for o in out] \n",
        "\n",
        "        \"\"\"\n",
        "        Vérifier ce que fait le InputChoice \n",
        "                -> proposition en partant de la proposition précédente : les 2 meilleurs éléments du out sont combinés (sum, mean, ...) pour n'avoir qu'une seule sortie du noeud \n",
        "        \"\"\"\n",
        "        return self.input_switch(out)\n",
        "\n",
        "\n",
        "class Cell(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, n_nodes, channels_pp, channels_p, channels, reduction_p, reduction):\n",
        "        super().__init__()\n",
        "        self.reduction = reduction\n",
        "        self.n_nodes = n_nodes\n",
        "        # If previous cell is reduction cell, current input size does not match with\n",
        "        # output size of cell[k-2]. So the output[k-2] should be reduced by preprocessing.\n",
        "        if reduction_p:\n",
        "            self.preproc0 = FactorizedReduce(channels_pp, channels, affine=False)\n",
        "        else:\n",
        "            self.preproc0 = StdConv(channels_pp, channels, 1, 1, \"valid\", affine=False)\n",
        "        self.preproc1 = StdConv(channels_p, channels, 1, 1, \"valid\", affine=False)\n",
        "\n",
        "        # generate dag\n",
        "\n",
        "        self.mutable_ops = []\n",
        "\n",
        "        for depth in range(2, self.n_nodes + 2):\n",
        "            self.mutable_ops.append(Node(\"{}_n{}\".format(\"reduce\" if reduction else \"normal\", depth), \n",
        "                                    depth, channels, 2 if reduction else 0))\n",
        "\n",
        "    def call(self, s0, s1):\n",
        "    # s0, s1 are the outputs of previous previous cell and previous cell, respectively.\n",
        "        tensors = [self.preproc0(s0), self.preproc1(s1)]\n",
        "        for node in self.mutable_ops:\n",
        "            cur_tensor = node(tensors)\n",
        "            tensors.append(cur_tensor)\n",
        "\n",
        "        output = tf.concat(tensors[2:], axis=-1)\n",
        "        return output\n",
        "\n",
        "class CNN(tf.keras.layers.Layer):\n",
        "  \"\"\"\n",
        "   Description\n",
        "   ---------------\n",
        "   This class allows the creation of a CNN model.\n",
        "  \"\"\"\n",
        "  def __init__(self, input_size, in_channels, channels, n_classes, n_layers, n_nodes=4,\n",
        "               stem_multiplier=3, auxiliary=False):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function creates an instance of the CNN class.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    input_size: int\n",
        "    in_channels: int\n",
        "    channels: int\n",
        "    n_classes: int\n",
        "    n_layers: int\n",
        "    n_nodes: int\n",
        "    stem_multiplier: int\n",
        "    auxiliary: bool\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    no output\n",
        "    \"\"\"\n",
        "    super().__init__()\n",
        "    self.in_channels = in_channels\n",
        "    self.channels = channels\n",
        "    self.n_classes = n_classes\n",
        "    self.n_layers = n_layers\n",
        "    self.aux_pos = 2 * n_layers // 3 if auxiliary else -1\n",
        "    c_cur = stem_multiplier * self.channels\n",
        "    self.conv2d = tf.keras.layers.Conv2D(filters=c_cur,\n",
        "                                         strides=1,\n",
        "                                         padding=\"same\",\n",
        "                                         data_format=\"channels_last\",\n",
        "                                         kernel_size=1,\n",
        "                                         use_bias=False )\n",
        "    self.batch_normalisation = tf.keras.layers.BatchNormalization(axis=-1)\n",
        "    # for the first cell, stem is used for both s0 and s1\n",
        "    # [!] channels_pp and channels_p is output channel size, but c_cur is input channel size.\n",
        "    channels_pp, channels_p, c_cur = c_cur, c_cur, channels\n",
        "    self.cells = []\n",
        "    reduction_p, reduction = False, False\n",
        "    for i in range(n_layers):\n",
        "      reduction_p, reduction = reduction, False\n",
        "      # Reduce featuremap size and double channels in 1/3 and 2/3 layer.\n",
        "      if i in [n_layers // 3, 2 * n_layers // 3]:\n",
        "        c_cur *= 2\n",
        "        reduction = True\n",
        "      cell = Cell(n_nodes, channels_pp, channels_p, c_cur, reduction_p, reduction)\n",
        "      self.cells.append(cell)\n",
        "      c_cur_out = c_cur * n_nodes\n",
        "      channels_pp, channels_p = channels_p, c_cur_out\n",
        "      if i == self.aux_pos:\n",
        "        self.aux_head = AuxiliaryHead(input_size // 4, channels_p, n_classes)\n",
        "    self.gap = tf.keras.layers.GlobalAveragePooling2D()\n",
        "    self.linear = tf.keras.layers.Dense( n_classes)\n",
        "  \n",
        "  def GetLogits(self, x):\n",
        "    \"\"\"\n",
        "\tDescription\n",
        "    ---------------\n",
        "    This function creates a list of tensor that is the result\n",
        "\tof the self.linear operation applied on x.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x : Tensor or list of tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    out : list of Tensor\n",
        "    \"\"\"\n",
        "    out = []\n",
        "    if isinstance(x,tf.Tensor):\n",
        "        # x is a Tensor\n",
        "        out = self.linear(x)\n",
        "    elif x == ():\n",
        "         out = []\n",
        "    else:\n",
        "        # x is a list\n",
        "        for i in range(len(x)):\n",
        "            if isinstance(len(x[i]),int):\n",
        "                # we want to know if x[i] is a tensor list\n",
        "                if len(x[i]) == 0 :\n",
        "                    # x[i] contains no element\n",
        "                    out = out\n",
        "                elif len(x[i]) == 1 :\n",
        "                    # x[i] contains only one tensor\n",
        "                    entree = x[i][0]\n",
        "                    out_i = self.linear(entree)\n",
        "                    out.append(out_i)\n",
        "                else:\n",
        "                    # x[i] is a list containing more than one tensor\n",
        "                    for j in range(len(x[i])):\n",
        "                        entree = x[i][j]\n",
        "\n",
        "                        if entree.get_shape()[1] >= 3 and \\\n",
        "                           entree.get_shape()[2] >= 3:\n",
        "                            # We check that the dimensions of the tensor \n",
        "                            # respect the conditions necessary to apply the layer.\n",
        "                            out_i_j = self.linear(entree)\n",
        "                            out.append(out_i_j)\n",
        "                        else:\n",
        "                            out_i_j = tf.constant(1,shape=(3, 3, 3, 4))\n",
        "                            out.append(out_i_j)\n",
        "            else:\n",
        "                # x[i] is a Tensor\n",
        "                out_i = self.flatten(x[i])\n",
        "                out.append(out_i)\n",
        "    \n",
        "    return out\n",
        "  \n",
        "  def ApplyLayer(self, fonction, x):\n",
        "    \"\"\"\n",
        "\tDescription\n",
        "    ---------------\n",
        "    This function creates a tuple of tensor that is the result\n",
        "\tof the self.linear operation applied on x.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x : Tensor or list of tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    out : tuple of Tensor\n",
        "    \"\"\"\n",
        "    if isinstance(x,tf.Tensor):\n",
        "        # x is a Tensor\n",
        "        out = fonction(x)\n",
        "    elif x == ():\n",
        "         out = ()\n",
        "    else:\n",
        "        # x is a list\n",
        "        out = ()\n",
        "        x = tf.convert_to_tensor(x)\n",
        "\n",
        "        for i in range(len(x)):\n",
        "            if isinstance(len(x[i]),int):\n",
        "                # we want to know if x[i] is a tensor list\n",
        "                if len(x[i]) == 0 :\n",
        "                    # x[i] contains no element\n",
        "                    out = out + ()\n",
        "                elif len(x[i]) == 1 :\n",
        "                    # x[i] contains only one tensor\n",
        "                    entree = x[i][0]\n",
        "                    out_i = fonction(entree)\n",
        "                    out = out + (out_i,)\n",
        "                else:\n",
        "                    # x[i] is a list containing more than one tensor\n",
        "                    for j in range(len(x[i])):\n",
        "                        entree = x[i][j]\n",
        "\n",
        "                        if entree.get_shape()[1] >= 3 and \\\n",
        "                           entree.get_shape()[2] >= 3:\n",
        "                            # We check that the dimensions of the tensor \n",
        "                            # respect the conditions necessary to apply the layer.\n",
        "                            out_i_j = fonction(entree)\n",
        "                            out = out + (out_i_j,)\n",
        "                        else:\n",
        "                            out_i_j = tf.constant(1,shape=(3, 3, 3, 4))\n",
        "                            out = out + (out_i_j,)            \n",
        "            else:\n",
        "                # x[i] is a Tensor\n",
        "                out_i = fonction(x[i])\n",
        "                out = out + (out_i,)\n",
        "    \n",
        "    return out\n",
        "\n",
        "\n",
        "\n",
        "  \n",
        "  def call(self, x):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function applies the instance of the CNN class to the data.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    x: tf.Tensor\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    logits : tf.Tensor\n",
        "    \"\"\"\n",
        "    out1 = self.conv2d(x)\n",
        "    s0 = self.batch_normalisation(out1)\n",
        "    s1 = self.batch_normalisation(out1)\n",
        "    aux_logits = None\n",
        "    for i, cell in enumerate(self.cells):\n",
        "      s0, s1 = s1, cell(s0, s1)\n",
        "      if i == self.aux_pos and self.training:\n",
        "        aux_logits = self.aux_head(s1)\n",
        "    out = self.ApplyLayer(self.gap, s1)\n",
        "    self.flatten = tf.keras.layers.Flatten()\n",
        "    out2 = self.ApplyLayer(self.flatten, out)\n",
        "    logits = self.GetLogits(out2)\n",
        "    if aux_logits is not None:\n",
        "      return logits, aux_logits\n",
        "    return logits\n",
        "\n",
        "  def drop_path_prob(self, p):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    This function drops some paths between neurons of the CNN model\n",
        "    with a probability p.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    p: float\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    no output\n",
        "    \"\"\"\n",
        "    for module in self.modules():\n",
        "      if isinstance(module, DropPath):\n",
        "        module.p = p"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "6da843e5",
      "metadata": {
        "id": "6da843e5"
      },
      "source": [
        "# fichier darts_mutable.py transformé en tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "c91e9fcc",
      "metadata": {
        "id": "c91e9fcc"
      },
      "outputs": [],
      "source": [
        "# %load C:\\Users\\cnerin\\Documents\\NAS\\code_tensorflow\\code_branche_gitlab_main\\nas\\darts\\project\\tensorflow\\darts_mutables.py\n",
        "import warnings\n",
        "from collections import OrderedDict\n",
        "import tensorflow as tf\n",
        "\n",
        "\n",
        "def global_mutable_counting():\n",
        "    \"\"\"\n",
        "    A program level counter starting from 1.\n",
        "    \"\"\"\n",
        "    global _counter\n",
        "    _counter += 1\n",
        "    return _counter\n",
        "\n",
        "class Mutable(tf.Module):\n",
        "    \"\"\"\n",
        "    Mutable is designed to function as a normal layer, with all necessary operators' weights.\n",
        "    States and weights of architectures should be included in mutator, instead of the layer itself.\n",
        "\n",
        "    Mutable has a key, which marks the identity of the mutable. This key can be used by users to share\n",
        "    decisions among different mutables. In mutator's implementation, mutators should use the key to\n",
        "    distinguish different mutables. Mutables that share the same key should be \"similar\" to each other.\n",
        "\n",
        "    Currently the default scope for keys is global. By default, the keys uses a global counter from 1 to\n",
        "    produce unique ids.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    key : str\n",
        "        The key of mutable.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    The counter is program level, but mutables are model level. In case multiple models are defined, and\n",
        "    you want to have `counter` starting from 1 in the second model, it's recommended to assign keys manually\n",
        "    instead of using automatic keys.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, key=None):\n",
        "        super().__init__()\n",
        "        if key is not None:\n",
        "            if not isinstance(key, str):\n",
        "                key = str(key)\n",
        "                print(\"Warning: key \\\"%s\\\" is not string, converted to string.\", key)\n",
        "            self._key = key\n",
        "        else:\n",
        "            self._key = self.__class__.__name__ + str(global_mutable_counting())\n",
        "        self.init_hook = self.forward_hook = None\n",
        "\n",
        "    def __deepcopy__(self, memodict=None):\n",
        "        raise NotImplementedError(\"Deep copy doesn't work for mutables.\")\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        self._check_built()\n",
        "        return super().__call__(*args, **kwargs)\n",
        "\n",
        "    def set_mutator(self, mutator):\n",
        "        if \"mutator\" in self.__dict__:\n",
        "            raise RuntimeError(\"`set_mutator` is called more than once. Did you parse the search space multiple times? \"\n",
        "                               \"Or did you apply multiple fixed architectures?\")\n",
        "        self.__dict__[\"mutator\"] = mutator\n",
        "\n",
        "    @property\n",
        "    def key(self):\n",
        "        \"\"\"\n",
        "        Read-only property of key.\n",
        "        \"\"\"\n",
        "        return self._key\n",
        "\n",
        "    @property\n",
        "    def name(self):\n",
        "        \"\"\"\n",
        "        After the search space is parsed, it will be the module name of the mutable.\n",
        "        \"\"\"\n",
        "        return self._name if hasattr(self, \"_name\") else self._key\n",
        "\n",
        "    @name.setter\n",
        "    def name(self, name):\n",
        "        self._name = name\n",
        "\n",
        "    def _check_built(self):\n",
        "        if not hasattr(self, \"mutator\"):\n",
        "            raise ValueError(\n",
        "                \"Mutator not set for {}. You might have forgotten to initialize and apply your mutator. \"\n",
        "                \"Or did you initialize a mutable on the fly in forward pass? Move to `__init__` \"\n",
        "                \"so that trainer can locate all your mutables. See NNI docs for more details.\".format(self))\n",
        "\n",
        "class MutableScope(Mutable):\n",
        "    \"\"\"\n",
        "    Mutable scope marks a subgraph/submodule to help mutators make better decisions.\n",
        "\n",
        "    If not annotated with mutable scope, search space will be flattened as a list. However, some mutators might\n",
        "    need to leverage the concept of a \"cell\". So if a module is defined as a mutable scope, everything in it will\n",
        "    look like \"sub-search-space\" in the scope. Scopes can be nested.\n",
        "\n",
        "    There are two ways mutators can use mutable scope. One is to traverse the search space as a tree during initialization\n",
        "    and reset. The other is to implement `enter_mutable_scope` and `exit_mutable_scope`. They are called before and after\n",
        "    the forward method of the class inheriting mutable scope.\n",
        "\n",
        "    Mutable scopes are also mutables that are listed in the mutator.mutables (search space), but they are not supposed\n",
        "    to appear in the dict of choices.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    key : str\n",
        "        Key of mutable scope.\n",
        "    \"\"\"\n",
        "    def __init__(self, key):\n",
        "        super().__init__(key=key)\n",
        "\n",
        "    def _check_built(self):\n",
        "        return True  # bypass the test because it's deprecated\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        if not hasattr(self, 'mutator'):\n",
        "            return super().__call__(*args, **kwargs)\n",
        "        warnings.warn(\"`MutableScope` is deprecated in Retiarii.\", DeprecationWarning)\n",
        "        try:\n",
        "            self._check_built()\n",
        "            self.mutator.enter_mutable_scope(self)\n",
        "            return super().__call__(*args, **kwargs)\n",
        "        finally:\n",
        "            self.mutator.exit_mutable_scope(self)\n",
        "\n",
        "\n",
        "class LayerChoice_darts_mutable(Mutable):\n",
        "    \"\"\"\n",
        "    Layer choice selects one of the ``op_candidates``, then apply it on inputs and return results.\n",
        "    In rare cases, it can also select zero or many.\n",
        "\n",
        "    Layer choice does not allow itself to be nested.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    op_candidates : list of nn.Module or OrderedDict\n",
        "        A module list to be selected from.\n",
        "    reduction : str\n",
        "        ``mean``, ``concat``, ``sum`` or ``none``. Policy if multiples are selected.\n",
        "        If ``none``, a list is returned. ``mean`` returns the average. ``sum`` returns the sum.\n",
        "        ``concat`` concatenate the list at dimension 1.\n",
        "    return_mask : bool\n",
        "        If ``return_mask``, return output tensor and a mask. Otherwise return tensor only.\n",
        "    key : str\n",
        "        Key of the input choice.\n",
        "\n",
        "    Attributes\n",
        "    ----------\n",
        "    length : int\n",
        "        Deprecated. Number of ops to choose from. ``len(layer_choice)`` is recommended.\n",
        "    names : list of str\n",
        "        Names of candidates.\n",
        "    choices : list of Module\n",
        "        Deprecated. A list of all candidate modules in the layer choice module.\n",
        "        ``list(layer_choice)`` is recommended, which will serve the same purpose.\n",
        "\n",
        "    Notes\n",
        "    -----\n",
        "    ``op_candidates`` can be a list of modules or a ordered dict of named modules, for example,\n",
        "\n",
        "    .. code-block:: python\n",
        "\n",
        "        self.op_choice = LayerChoice(OrderedDict([\n",
        "            (\"conv3x3\", nn.Conv2d(3, 16, 128)),\n",
        "            (\"conv5x5\", nn.Conv2d(5, 16, 128)),\n",
        "            (\"conv7x7\", nn.Conv2d(7, 16, 128))\n",
        "        ]))\n",
        "\n",
        "    Elements in layer choice can be modified or deleted. Use ``del self.op_choice[\"conv5x5\"]`` or\n",
        "    ``self.op_choice[1] = nn.Conv3d(...)``. Adding more choices is not supported yet.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, op_candidates, reduction=\"sum\", return_mask=False, key=None):\n",
        "        super().__init__(key=key)\n",
        "        self.names = []\n",
        "        if isinstance(op_candidates, OrderedDict):\n",
        "            for name, module in op_candidates.items():\n",
        "                assert name not in [\"length\", \"reduction\", \"return_mask\", \"_key\", \"key\", \"names\"], \\\n",
        "                    \"Please don't use a reserved name '{}' for your module.\".format(name)\n",
        "                self.add_module(name, module)\n",
        "                self.names.append(name)\n",
        "        elif isinstance(op_candidates, list):\n",
        "            for i, module in enumerate(op_candidates):\n",
        "                self.add_module(str(i), module)\n",
        "                self.names.append(str(i))\n",
        "        else:\n",
        "            raise TypeError(\"Unsupported op_candidates type: {}\".format(type(op_candidates)))\n",
        "        self.reduction = reduction\n",
        "        self.return_mask = return_mask\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        if isinstance(idx, str):\n",
        "            return self._modules[idx]\n",
        "        return list(self)[idx]\n",
        "\n",
        "    def __setitem__(self, idx, module):\n",
        "        key = idx if isinstance(idx, str) else self.names[idx]\n",
        "        return setattr(self, key, module)\n",
        "\n",
        "    def __delitem__(self, idx):\n",
        "        if isinstance(idx, slice):\n",
        "            for key in self.names[idx]:\n",
        "                delattr(self, key)\n",
        "        else:\n",
        "            if isinstance(idx, str):\n",
        "                key, idx = idx, self.names.index(idx)\n",
        "            else:\n",
        "                key = self.names[idx]\n",
        "            delattr(self, key)\n",
        "        del self.names[idx]\n",
        "\n",
        "    @property\n",
        "    def length(self):\n",
        "        warnings.warn(\"layer_choice.length is deprecated. Use `len(layer_choice)` instead.\", DeprecationWarning)\n",
        "        return len(self)\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.names)\n",
        "\n",
        "    def __iter__(self):\n",
        "        return map(lambda name: self._modules[name], self.names)\n",
        "\n",
        "    @property\n",
        "    def choices(self):\n",
        "        warnings.warn(\"layer_choice.choices is deprecated. Use `list(layer_choice)` instead.\", DeprecationWarning)\n",
        "        return list(self)\n",
        "\n",
        "    def __call__(self, *args, **kwargs):\n",
        "        \"\"\"\n",
        "        Returns\n",
        "        -------\n",
        "        tuple of tensors\n",
        "            Output and selection mask. If ``return_mask`` is ``False``, only output is returned.\n",
        "        \"\"\"\n",
        "        out, mask = self.mutator.on_forward_layer_choice(self, *args, **kwargs)\n",
        "        if self.return_mask:\n",
        "            return out, mask\n",
        "        return out\n",
        "\n",
        "class InputChoice_darts_mutable(Mutable):\n",
        "    \"\"\"\n",
        "    Input choice selects ``n_chosen`` inputs from ``choose_from`` (contains ``n_candidates`` keys). For beginners,\n",
        "    use ``n_candidates`` instead of ``choose_from`` is a safe option. To get the most power out of it, you might want to\n",
        "    know about ``choose_from``.\n",
        "\n",
        "    The keys in ``choose_from`` can be keys that appear in past mutables, or ``NO_KEY`` if there are no suitable ones.\n",
        "    The keys are designed to be the keys of the sources. To help mutators make better decisions,\n",
        "    mutators might be interested in how the tensors to choose from come into place. For example, the tensor is the\n",
        "    output of some operator, some node, some cell, or some module. If this operator happens to be a mutable (e.g.,\n",
        "    ``LayerChoice`` or ``InputChoice``), it has a key naturally that can be used as a source key. If it's a\n",
        "    module/submodule, it needs to be annotated with a key: that's where a :class:`MutableScope` is needed.\n",
        "\n",
        "    In the example below, ``input_choice`` is a 4-choose-any. The first 3 is semantically output of cell1, output of cell2,\n",
        "    output of cell3 with respectively. Notice that an extra max pooling is followed by cell1, indicating x1 is not\n",
        "    \"actually\" the direct output of cell1.\n",
        "\n",
        "    .. code-block:: python\n",
        "\n",
        "        class Cell(MutableScope):\n",
        "            pass\n",
        "\n",
        "        class Net(nn.Module):\n",
        "            def __init__(self):\n",
        "                self.cell1 = Cell(\"cell1\")\n",
        "                self.cell2 = Cell(\"cell2\")\n",
        "                self.op = LayerChoice([conv3x3(), conv5x5()], key=\"op\")\n",
        "                self.input_choice = InputChoice(choose_from=[\"cell1\", \"cell2\", \"op\", InputChoice.NO_KEY])\n",
        "\n",
        "            def forward(self, x):\n",
        "                x1 = max_pooling(self.cell1(x))\n",
        "                x2 = self.cell2(x)\n",
        "                x3 = self.op(x)\n",
        "                x4 = torch.zeros_like(x)\n",
        "                return self.input_choice([x1, x2, x3, x4])\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    n_candidates : int\n",
        "        Number of inputs to choose from.\n",
        "    choose_from : list of str\n",
        "        List of source keys to choose from. At least of one of ``choose_from`` and ``n_candidates`` must be fulfilled.\n",
        "        If ``n_candidates`` has a value but ``choose_from`` is None, it will be automatically treated as ``n_candidates``\n",
        "        number of empty string.\n",
        "    n_chosen : int\n",
        "        Recommended inputs to choose. If None, mutator is instructed to select any.\n",
        "    reduction : str\n",
        "        ``mean``, ``concat``, ``sum`` or ``none``. See :class:`LayerChoice`.\n",
        "    return_mask : bool\n",
        "        If ``return_mask``, return output tensor and a mask. Otherwise return tensor only.\n",
        "    key : str\n",
        "        Key of the input choice.\n",
        "    \"\"\"\n",
        "\n",
        "    NO_KEY = \"\"\n",
        "\n",
        "    def __init__(self, n_candidates=None, choose_from=None, n_chosen=None,\n",
        "                 reduction=\"sum\", return_mask=False, key=None):\n",
        "        super().__init__(key=key)\n",
        "        # precondition check\n",
        "        assert n_candidates is not None or choose_from is not None, \"At least one of `n_candidates` and `choose_from`\" \\\n",
        "                                                                    \"must be not None.\"\n",
        "        if choose_from is not None and n_candidates is None:\n",
        "            n_candidates = len(choose_from)\n",
        "        elif choose_from is None and n_candidates is not None:\n",
        "            choose_from = [self.NO_KEY] * n_candidates\n",
        "        assert n_candidates == len(choose_from), \"Number of candidates must be equal to the length of `choose_from`.\"\n",
        "        assert n_candidates > 0, \"Number of candidates must be greater than 0.\"\n",
        "        assert n_chosen is None or 0 <= n_chosen <= n_candidates, \"Expected selected number must be None or no more \" \\\n",
        "                                                                  \"than number of candidates.\"\n",
        "\n",
        "        self.n_candidates = n_candidates\n",
        "        self.choose_from = choose_from.copy()\n",
        "        self.n_chosen = n_chosen\n",
        "        self.reduction = reduction\n",
        "        self.return_mask = return_mask\n",
        "\n",
        "    def __call__(self, optional_inputs):\n",
        "        \"\"\"\n",
        "        Forward method of LayerChoice.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        optional_inputs : list or dict\n",
        "            Recommended to be a dict. As a dict, inputs will be converted to a list that follows the order of\n",
        "            ``choose_from`` in initialization. As a list, inputs must follow the semantic order that is the same as\n",
        "            ``choose_from``.\n",
        "\n",
        "        Returns\n",
        "        -------\n",
        "        tuple of tensors\n",
        "            Output and selection mask. If ``return_mask`` is ``False``, only output is returned.\n",
        "        \"\"\"\n",
        "        optional_input_list = optional_inputs\n",
        "        if isinstance(optional_inputs, dict):\n",
        "            optional_input_list = [optional_inputs[tag] for tag in self.choose_from]\n",
        "        assert isinstance(optional_input_list, list), \\\n",
        "            \"Optional input list must be a list, not a {}.\".format(type(optional_input_list))\n",
        "        assert len(optional_inputs) == self.n_candidates, \\\n",
        "            \"Length of the input list must be equal to number of candidates.\"\n",
        "        out, mask = self.mutator.on_forward_input_choice(self, optional_input_list)\n",
        "        if self.return_mask:\n",
        "            return out, mask\n",
        "        return out\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0423635f",
      "metadata": {
        "id": "0423635f"
      },
      "source": [
        "# fichier darts_utils.py transformé en tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "05cf1fef",
      "metadata": {
        "id": "05cf1fef"
      },
      "outputs": [],
      "source": [
        "# %load C:\\Users\\cnerin\\Documents\\NAS\\code_tensorflow\\code_branche_gitlab_main\\nas\\darts\\project\\tensorflow\\darts_utils.py\n",
        "import numpy as np\n",
        "from collections import OrderedDict\n",
        "import tensorflow as tf\n",
        "#import choices\n",
        "#from darts_mutables import LayerChoice, InputChoice\n",
        "\n",
        "\n",
        "def _reset_global_mutable_counting():\n",
        "    \"\"\"\n",
        "    Reset the global mutable counting to count from 1. Useful when defining multiple models with default keys.\n",
        "    \"\"\"\n",
        "    global _counter\n",
        "    _counter = 0\n",
        "\n",
        "\n",
        "def to_device(obj, device):\n",
        "    \"\"\"\n",
        "    Move a tensor, tuple, list, or dict onto device.\n",
        "    \"\"\"\n",
        "    if tf.is_tensor(obj):\n",
        "        return obj.to(device)\n",
        "    if isinstance(obj, tuple):\n",
        "        return tuple(to_device(t, device) for t in obj)\n",
        "    if isinstance(obj, list):\n",
        "        return [to_device(t, device) for t in obj]\n",
        "    if isinstance(obj, dict):\n",
        "        return {k: to_device(v, device) for k, v in obj.items()}\n",
        "    if isinstance(obj, (int, float, str)):\n",
        "        return obj\n",
        "    raise ValueError(\"'%s' has unsupported type '%s'\" % (obj, type(obj)))\n",
        "\n",
        "\n",
        "def to_list(arr):\n",
        "    if tf.is_tensor(arr):\n",
        "        return arr.cpu().numpy().tolist()\n",
        "    if isinstance(arr, np.ndarray):\n",
        "        return arr.tolist()\n",
        "    if isinstance(arr, (list, tuple)):\n",
        "        return list(arr)\n",
        "    return arr\n",
        "\n",
        "\n",
        "class AverageMeterGroup:\n",
        "    \"\"\"\n",
        "    Average meter group for multiple average meters.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.meters = OrderedDict()\n",
        "\n",
        "    def update(self, data):\n",
        "        \"\"\"\n",
        "        Update the meter group with a dict of metrics.\n",
        "        Non-exist average meters will be automatically created.\n",
        "        \"\"\"\n",
        "        for k, v in data.items():\n",
        "            if k not in self.meters:\n",
        "                self.meters[k] = AverageMeter(k, \":4f\")\n",
        "            self.meters[k].update(v)\n",
        "\n",
        "    def __getattr__(self, item):\n",
        "        return self.meters[item]\n",
        "\n",
        "    def __getitem__(self, item):\n",
        "        return self.meters[item]\n",
        "\n",
        "    def __str__(self):\n",
        "        return \"  \".join(str(v) for v in self.meters.values())\n",
        "\n",
        "    def summary(self):\n",
        "        \"\"\"\n",
        "        Return a summary string of group data.\n",
        "        \"\"\"\n",
        "        return \"  \".join(v.summary() for v in self.meters.values())\n",
        "\n",
        "\n",
        "class AverageMeter:\n",
        "    \"\"\"\n",
        "    Computes and stores the average and current value.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    name : str\n",
        "        Name to display.\n",
        "    fmt : str\n",
        "        Format string to print the values.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, name, fmt=':f'):\n",
        "        self.name = name\n",
        "        self.fmt = fmt\n",
        "        self.reset()\n",
        "\n",
        "\n",
        "    def reset(self):\n",
        "        \"\"\"\n",
        "        Reset the meter.\n",
        "        \"\"\"\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        \"\"\"\n",
        "        Update with value and weight.\n",
        "\n",
        "        Parameters\n",
        "        ----------\n",
        "        val : float or int\n",
        "            The new value to be accounted in.\n",
        "        n : int\n",
        "            The weight of the new value.\n",
        "        \"\"\"\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = self.sum / self.count\n",
        "\n",
        "    def __str__(self):\n",
        "        fmtstr = '{name} {val' + self.fmt + '} ({avg' + self.fmt + '})'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "    def summary(self):\n",
        "        fmtstr = '{name}: {avg' + self.fmt + '}'\n",
        "        return fmtstr.format(**self.__dict__)\n",
        "\n",
        "def _replace_module_with_type(root_module, init_fn, type_name, modules):\n",
        "    if modules is None:\n",
        "        modules = []\n",
        "    for child in root_module.submodules:\n",
        "        if isinstance(child, type_name):\n",
        "            modules.append([child.label, init_fn(child)])\n",
        "    return modules\n",
        "\n",
        "\n",
        "def replace_layer_choice(root_module, init_fn, modules=None):\n",
        "    \"\"\"\n",
        "    Replace layer choice modules with modules that are initiated with init_fn.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    root_module : nn.Module\n",
        "        Root module to traverse.\n",
        "    init_fn : Callable\n",
        "        Initializing function.\n",
        "    modules : dict, optional\n",
        "        Update the replaced modules into the dict and check duplicate if provided.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[Tuple[str, nn.Module]]\n",
        "        A list from layer choice keys (names) and replaced modules.\n",
        "    \"\"\"\n",
        "    # First LayerChoice is from darts_utils.py and second is from choices.py\n",
        "    return _replace_module_with_type(root_module, init_fn, (LayerChoice_darts_mutable, LayerChoice), modules)\n",
        "\n",
        "\n",
        "def replace_input_choice(root_module, init_fn, modules=None):\n",
        "    \"\"\"\n",
        "    Replace input choice modules with modules that are initiated with init_fn.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    root_module : nn.Module\n",
        "        Root module to traverse.\n",
        "    init_fn : Callable\n",
        "        Initializing function.\n",
        "    modules : dict, optional\n",
        "        Update the replaced modules into the dict and check duplicate if provided.\n",
        "\n",
        "    Returns\n",
        "    -------\n",
        "    List[Tuple[str, nn.Module]]\n",
        "        A list from layer choice keys (names) and replaced modules.\n",
        "    \"\"\"\n",
        "    return _replace_module_with_type(root_module, init_fn, (InputChoice_darts_mutable, InputChoice), modules)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "43b45792",
      "metadata": {
        "id": "43b45792"
      },
      "source": [
        "# fichier darts.py transformé en tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "87136071",
      "metadata": {
        "id": "87136071"
      },
      "outputs": [],
      "source": [
        "# %load C:\\Users\\cnerin\\Documents\\NAS\\code_tensorflow\\code_branche_gitlab_main\\nas\\darts\\project\\tensorflow\\darts.py\n",
        "from collections import OrderedDict\n",
        "import tensorflow as tf\n",
        "#from darts_utils import replace_layer_choice, replace_input_choice\n",
        "import abc\n",
        "from typing import Any\n",
        "\n",
        "\n",
        "class DartsLayerChoice(tf.keras.layers.Layer):\n",
        "\n",
        "    def __init__(self, layer_choice):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        self.op_choices = OrderedDict([(name, layer_choice[name]) for name in layer_choice.names])\n",
        "        size_normal = []\n",
        "        size_normal.append(len(self.op_choices))\n",
        "        self.alpha = tf.random.normal(size_normal) * 1e-3\n",
        "\n",
        "\n",
        "    def call(self, *args, **kwargs):\n",
        "        op_results = tf.keras.layers.Concatenate([op(*args, **kwargs) for op in self.op_choices.values()])\n",
        "        alpha_shape = [-1] + [1] * (len(op_results.size()) - 1)\n",
        "        return tf.sum(op_results * tf.keras.layers.Softmax(self.alpha, -1).reshape(*alpha_shape), 0)\n",
        "   \n",
        "    def parameters(self):\n",
        "        for _, p in self.named_parameters():\n",
        "            yield p\n",
        "\n",
        "    def named_parameters(self):\n",
        "        for name, p in super(DartsLayerChoice, self).named_parameters():\n",
        "            if name == 'alpha':\n",
        "                continue\n",
        "            yield name, p\n",
        "\n",
        "    def export(self):\n",
        "        return list(self.op_choices.keys())[tf.math.argmax(self.alpha)]   \n",
        "\n",
        "\n",
        "class DartsInputChoice(tf.keras.layers.Layer):\n",
        "    def __init__(self, input_choice):\n",
        "\n",
        "        super().__init__()\n",
        "\n",
        "        size_normal = []\n",
        "        size_normal.append(input_choice.n_candidates)\n",
        "        self.alpha = tf.convert_to_tensor(tf.random.normal(size_normal) * 1e-3)\n",
        "        self.n_chosen = input_choice.n_chosen or 1\n",
        "        \n",
        "\n",
        "    def call(self, inputs):\n",
        "        inputs =  tf.keras.layers.Concatenate(inputs)\n",
        "        alpha_shape = [-1] + [1] * (len(inputs.size()) - 1)\n",
        "        return tf.sum(inputs * tf.keras.layers.Softmax(self.alpha, -1).reshape(*alpha_shape), 0)\n",
        "\n",
        "    def parameters(self):\n",
        "        for _, p in self.named_parameters():\n",
        "            yield p\n",
        "\n",
        "    def named_parameters(self):\n",
        "        for name, p in super(DartsInputChoice, self).named_parameters():\n",
        "            if name == 'alpha':\n",
        "                continue\n",
        "            yield name, p\n",
        "\n",
        "    def export(self):\n",
        "        print(\"n_chosen = \",self.n_chosen)\n",
        "        print(\"alpha = \", self.alpha)\n",
        "        return tf.argsort(-self.alpha).cpu().numpy().tolist()[:self.n_chosen]    \n",
        "          \n",
        "# Param BaseOneShotTrainer\n",
        "class BaseOneShotTrainer(abc.ABC):\n",
        "    \"\"\"\n",
        "    Build many (possibly all) architectures into a full graph, search (with train) and export the best.\n",
        "\n",
        "    One-shot trainer has a ``fit`` function with no return value. Trainers should fit and search for the best architecture.\n",
        "    Currently, all the inputs of trainer needs to be manually set before fit (including the search space, data loader\n",
        "    to use training epochs, and etc.).\n",
        "\n",
        "    It has an extra ``export`` function that exports an object representing the final searched architecture.\n",
        "    \"\"\"\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def fit(self) -> None:\n",
        "        pass\n",
        "\n",
        "    @abc.abstractmethod\n",
        "    def export(self) -> Any:\n",
        "        pass\n",
        "\n",
        "class DartsTrainer(BaseOneShotTrainer):\n",
        " \n",
        "    \"\"\"\"\"   \n",
        "    DARTS trainer.\n",
        "\n",
        "    Parameters\n",
        "    ----------\n",
        "    model : nn.Module\n",
        "        PyTorch model to be trained.\n",
        "    loss : callable\n",
        "        Receives logits and ground truth label, return a loss tensor.\n",
        "    metrics : callable\n",
        "        Receives logits and ground truth label, return a dict of metrics.\n",
        "    optimizer : Optimizer\n",
        "        The optimizer used for optimizing the model.\n",
        "    num_epochs : int\n",
        "        Number of epochs planned for training.\n",
        "    dataset : Dataset\n",
        "        Dataset for training. Will be split for training weights and architecture weights.\n",
        "    grad_clip : float\n",
        "        Gradient clipping. Set to 0 to disable. Default: 5.\n",
        "    learning_rate : float\n",
        "        Learning rate to optimize the model.\n",
        "    batch_size : int\n",
        "        Batch size.\n",
        "    workers : int\n",
        "        Workers for data loading.\n",
        "    device : torch.device\n",
        "        ``torch.device(\"cpu\")`` or ``torch.device(\"cuda\")``.\n",
        "    log_frequency : int\n",
        "        Step count per logging.\n",
        "    arc_learning_rate : float\n",
        "        Learning rate of architecture parameters.\n",
        "    unrolled : float\n",
        "        ``True`` if using second order optimization, else first order optimization.\n",
        "    \"\"\"\n",
        "    def __init__(self, model, loss, metrics, optimizer,\n",
        "                 num_epochs, dataset, grad_clip=5.,\n",
        "                 learning_rate=2.5E-3, batch_size=64, workers=4,\n",
        "                 device=None, log_frequency=None,\n",
        "                 arc_learning_rate=3.0E-4, unrolled=False):\n",
        "        self.model = model\n",
        "        self.loss = loss\n",
        "        self.metrics = metrics\n",
        "        self.num_epochs = num_epochs\n",
        "        self.dataset = dataset\n",
        "        self.batch_size = batch_size\n",
        "        self.workers = workers\n",
        "        self.log_frequency = log_frequency\n",
        "        \n",
        "        self.nas_modules = []\n",
        "        # Modifier utils\n",
        "        replace_layer_choice(self.model, DartsLayerChoice, self.nas_modules)\n",
        "        replace_input_choice(self.model, DartsInputChoice, self.nas_modules)\n",
        "\n",
        "        self.model_optim = optimizer\n",
        "        # use the same architecture weight for modules with duplicated names\n",
        "\n",
        "        self.ctrl_optim = tf.keras.optimizers.Adam(learning_rate=arc_learning_rate, beta_1=0.5, beta_2=0.999, clipvalue=3.)\n",
        "        self.grad_clip = 5.\n",
        "\n",
        "\n",
        "        self._init_dataloader()\n",
        "\n",
        "    def _init_dataloader(self):\n",
        "\n",
        "        # Split the dataset for training and validation.\n",
        "        x, y = self.dataset\n",
        "        split = len(x) // 2\n",
        "        x_train = x[:split]\n",
        "        x_train = tf.cast(x_train, float)\n",
        "        y_train = y[:split]\n",
        "        y_train = tf.cast(y_train, float)\n",
        "        x_val = x[split:]\n",
        "        x_val = tf.cast(x_val, float)\n",
        "        y_val = y[split:]\n",
        "        y_val = tf.cast(y_val, float)\n",
        "\n",
        "        # Prepare the training dataset with batch_size\n",
        "        self.train_dataset = tf.data.Dataset.from_tensor_slices((x_train, y_train))\n",
        "        self.train_dataset = self.train_dataset.batch(self.batch_size)\n",
        "\n",
        "        # Prepare the validation dataset with batch_size\n",
        "        self.val_dataset = tf.data.Dataset.from_tensor_slices((x_val, y_val))\n",
        "        self.val_dataset = self.val_dataset.batch(self.batch_size)\n",
        "    \n",
        "    def _train_one_epoch(self, epoch):\n",
        "        print(\"Start epoch n°\",epoch)\n",
        "        for step, ((trn_X, trn_y), (val_X, val_y)) in enumerate(zip(self.train_dataset, self.val_dataset)):\n",
        "\n",
        "            # phase 1. architecture step\n",
        "            with tf.GradientTape() as tape:\n",
        "                logits = self.model(val_X, training=True)\n",
        "                loss_value = self.loss(val_y, logits)\n",
        "\n",
        "            grads = tape.gradient(loss_value, self.model.trainable_weights)\n",
        "            self.ctrl_optim.apply_gradients((grad, var) for (grad, var) in zip(grads, self.model.trainable_variables) if grad is not None)            \n",
        "\n",
        "            # phase 2: child network step\n",
        "            with tf.GradientTape() as tape2:\n",
        "\n",
        "                logits = self.model(trn_X, training=True)\n",
        "                loss_value = self.loss(trn_y, logits)\n",
        "\n",
        "            grads = tape2.gradient(loss_value, self.model.trainable_weights)\n",
        "            self.model_optim.apply_gradients((grad, var) for (grad, var) in zip(grads, self.model.trainable_variables) if grad is not None)\n",
        "\n",
        "\n",
        "            if self.log_frequency is not None and step % self.log_frequency == 0:\n",
        "                print(\n",
        "                    \"Training loss (for one batch) at step %d: %.4f\"\n",
        "                    % (step, float(loss_value))\n",
        "                )\n",
        "                print(\"Seen so far: %s samples\" % ((step + 1) * self.batch_size))\n",
        "\n",
        "\n",
        "    def fit(self):\n",
        "        for i in range(self.num_epochs):\n",
        "            self._train_one_epoch(i)\n",
        "\n",
        "   \n",
        "    def export(self):\n",
        "        result = dict()\n",
        "        for name, module in self.nas_modules:\n",
        "            if name not in result:\n",
        "                result[name] = module.export()\n",
        "        return result\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "36155bbb",
      "metadata": {
        "id": "36155bbb"
      },
      "source": [
        "# fichier dataset.py transformé en tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "ce50800c",
      "metadata": {
        "id": "ce50800c"
      },
      "outputs": [],
      "source": [
        "# %load C:\\Users\\cnerin\\Documents\\NAS\\code_tensorflow\\code_branche_gitlab_main\\nas\\darts\\project\\tensorflow\\datasets.py\n",
        "import tensorflow as tf\n",
        "\n",
        "def get_dataset(cls):\n",
        "\n",
        "    if cls == \"cifar10\":\n",
        "        dataset_train, dataset_valid = tf.keras.datasets.cifar10.load_data()\n",
        "        return 3, dataset_train, dataset_valid\n",
        "\n",
        "    elif cls == \"mnist\":\n",
        "        dataset_train, dataset_valid = tf.keras.datasets.mnist.load_data()\n",
        "        return 1, dataset_train, dataset_valid\n",
        "\n",
        "    else:\n",
        "        print(\"Dataset Error ------- \" + cls + \" is unknown !\")\n",
        "        return 0"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "45889d0d",
      "metadata": {
        "id": "45889d0d"
      },
      "source": [
        "# fichier utils.py transformé en tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "0d21fe64",
      "metadata": {
        "id": "0d21fe64"
      },
      "outputs": [],
      "source": [
        "# %load C:\\Users\\cnerin\\Documents\\NAS\\code_tensorflow\\code_branche_gitlab_main\\nas\\darts\\project\\tensorflow\\utils.py\n",
        "from graphviz import Digraph\n",
        "\n",
        "def accuracy(output, target, topk=(1,)):\n",
        "    \"\"\" Computes the precision@k for the specified values of k \"\"\"\n",
        "    maxk = max(topk)\n",
        "    batch_size = target.size(0)\n",
        "\n",
        "    _, pred = output.topk(maxk, 1, True, True)\n",
        "    pred = pred.t()\n",
        "    # one-hot case\n",
        "    if target.ndimension() > 1:\n",
        "        target = target.max(1)[1]\n",
        "\n",
        "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
        "\n",
        "    res = dict()\n",
        "    for k in topk:\n",
        "        correct_k = correct[:k].reshape(-1).float().sum(0)\n",
        "        res[\"acc{}\".format(k)] = correct_k.mul_(1.0 / batch_size).item()\n",
        "    return res\n",
        "\n",
        "def visualize(final_architecture):\n",
        "    arc_normal, arc_reduce = split_normal_reduce(final_architecture)\n",
        "    values_normal = convert_to_simple(arc_normal)\n",
        "    values_reduce = convert_to_simple(arc_reduce)\n",
        "\n",
        "    if values_normal and len(values_normal)>0:\n",
        "        normal_graph = generate_graph(values_normal, 'normal')\n",
        "        normal_graph.render(directory='img', view=False)\n",
        "    \n",
        "    if values_reduce and len(values_reduce)>0:\n",
        "        reduce_graph = generate_graph(values_reduce, 'reduce')\n",
        "        reduce_graph.render(directory='img', view=False)\n",
        "\n",
        "def generate_graph(model, name):\n",
        "\n",
        "    graph = Digraph(name)\n",
        "\n",
        "    graph.node('0', 'c[k-2]')\n",
        "    graph.node('1', 'c[k-1]')\n",
        "\n",
        "    for i in range(len(model)) :\n",
        "        graph.node(str(i+2),str(i))\n",
        "\n",
        "    graph.node(str(len(model)+2), 'c[k]')\n",
        "\n",
        "    for key, values in model.items() :\n",
        "        for node, value in values.items() :\n",
        "            graph.edge(str(node), str(key), label=str(value))\n",
        "    \n",
        "    graph.edge(str(len(model)+1),str(len(model)+2))\n",
        "\n",
        "    return graph\n",
        "\n",
        "\n",
        "def split_normal_reduce(arc):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    Splits the models into normal and reduce dictionaries.\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    arc: dictionary\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    arc_normal: dictionary\n",
        "    arc_reduce: dictionary\n",
        "    \"\"\"\n",
        "    arc_normal, arc_reduce = dict(),dict()\n",
        "    for key in arc.keys():\n",
        "        if \"normal\" in key:\n",
        "            arc_normal[key]=arc[key]\n",
        "        elif \"reduce\" in key:\n",
        "            arc_reduce[key]=arc[key]\n",
        "        else:\n",
        "            print(\"Issue encountered : the following value is neither\", key)\n",
        "    return arc_normal, arc_reduce\n",
        "\n",
        "def convert_to_simple(arc):\n",
        "    \"\"\"\n",
        "    Description\n",
        "    ---------------\n",
        "    Converts an architecture into a more understandable dictionary.\n",
        "    Output shape : {n:{p:<type of link>}}\n",
        "    Output example : {2: {1: 'maxpool', 0: 'maxpool'}, 3: {2: 'maxpool', 1: 'maxpool'}, 4: {3: 'maxpool', 2: 'maxpool'}, 5: {3: 'maxpool', 4: 'maxpool'}}\n",
        "    Input(s)\n",
        "    ---------------\n",
        "    arc: dictionary\n",
        "    Output(s)\n",
        "    ---------------\n",
        "    kept_arc: dictionary\n",
        "    \"\"\"\n",
        "    kept_arc_index = []\n",
        "    kept_arc = dict()\n",
        "    for value in arc.values():\n",
        "        if type(value)==type([1,2]):\n",
        "            kept_arc_index.append(value)\n",
        "    prev_inc = 0\n",
        "    increment = 2\n",
        "    j=2\n",
        "    for pair in kept_arc_index:\n",
        "        keys_available = list(arc.keys())[prev_inc:increment]\n",
        "        values = dict()\n",
        "        for i in pair:\n",
        "            values[i]= arc[keys_available[i]]\n",
        "        kept_arc[j] = values\n",
        "        prev_inc= increment\n",
        "        increment += j+1\n",
        "        j+=1\n",
        "    \n",
        "    return kept_arc\n",
        "\n",
        "if __name__==\"__main__\":\n",
        "    arc = {'normal_n2_p0': 'maxpool', 'normal_n2_p1': 'maxpool', 'normal_n3_p0': 'maxpool', 'normal_n3_p1': 'maxpool', 'normal_n3_p2': 'maxpool', 'normal_n4_p0': 'maxpool', 'normal_n4_p1': 'maxpool', 'normal_n4_p2': 'maxpool', 'normal_n4_p3': 'maxpool', 'normal_n5_p0': 'maxpool', 'normal_n5_p1': 'maxpool', 'normal_n5_p2': 'maxpool', 'normal_n5_p3': 'maxpool', 'normal_n5_p4': 'dilconv3x3', 'reduce_n2_p0': 'maxpool', 'reduce_n2_p1': 'maxpool', 'reduce_n3_p0': 'maxpool', 'reduce_n3_p1': 'maxpool', 'reduce_n3_p2': 'maxpool', 'reduce_n4_p0': 'maxpool', 'reduce_n4_p1': 'maxpool', 'reduce_n4_p2': 'maxpool', 'reduce_n4_p3': 'maxpool', 'reduce_n5_p0': 'maxpool', 'reduce_n5_p1': 'maxpool', 'reduce_n5_p2': 'maxpool', 'reduce_n5_p3': 'maxpool', 'reduce_n5_p4': 'maxpool', 'normal_n2_switch': [1, 0], 'normal_n3_switch': [2, 1], 'normal_n4_switch': [2, 1], 'normal_n5_switch': [4, 1], 'reduce_n2_switch': [1, 0], 'reduce_n3_switch': [2, 1], 'reduce_n4_switch': [3, 2], 'reduce_n5_switch': [3, 4]}\n",
        "    visualize(arc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7ac940b8",
      "metadata": {
        "id": "7ac940b8"
      },
      "source": [
        "# fichier search.py transformé en tensorflow"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "id": "e49dbe3b",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "e49dbe3b",
        "outputId": "32310fd5-e910-4338-8deb-c9a59b90e232"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start epoch n° 0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:97: UserWarning: You should not run forward of this module directly.\n",
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:143: UserWarning: You should not run forward of this module directly.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training loss (for one batch) at step 0: 3.6274\n",
            "Seen so far: 64 samples\n",
            "Training loss (for one batch) at step 1: 2.9985\n",
            "Seen so far: 128 samples\n",
            "Training loss (for one batch) at step 2: 2.3003\n",
            "Seen so far: 192 samples\n",
            "Training loss (for one batch) at step 3: 2.0625\n",
            "Seen so far: 256 samples\n",
            "Training loss (for one batch) at step 4: 2.0459\n",
            "Seen so far: 320 samples\n",
            "Training loss (for one batch) at step 5: 2.0641\n",
            "Seen so far: 384 samples\n",
            "Training loss (for one batch) at step 6: 2.1518\n",
            "Seen so far: 448 samples\n",
            "Training loss (for one batch) at step 7: 2.1761\n",
            "Seen so far: 512 samples\n",
            "Training loss (for one batch) at step 8: 1.9165\n",
            "Seen so far: 576 samples\n",
            "Training loss (for one batch) at step 9: 2.0343\n",
            "Seen so far: 640 samples\n",
            "Training loss (for one batch) at step 10: 1.9542\n",
            "Seen so far: 704 samples\n",
            "Training loss (for one batch) at step 11: 1.9806\n",
            "Seen so far: 768 samples\n",
            "Training loss (for one batch) at step 12: 2.2125\n",
            "Seen so far: 832 samples\n",
            "Training loss (for one batch) at step 13: 2.1904\n",
            "Seen so far: 896 samples\n",
            "Training loss (for one batch) at step 14: 1.8546\n",
            "Seen so far: 960 samples\n",
            "Training loss (for one batch) at step 15: 1.9265\n",
            "Seen so far: 1024 samples\n",
            "Training loss (for one batch) at step 16: 1.9527\n",
            "Seen so far: 1088 samples\n",
            "Training loss (for one batch) at step 17: 2.0022\n",
            "Seen so far: 1152 samples\n",
            "Training loss (for one batch) at step 18: 2.0187\n",
            "Seen so far: 1216 samples\n",
            "Training loss (for one batch) at step 19: 1.8817\n",
            "Seen so far: 1280 samples\n",
            "Training loss (for one batch) at step 20: 1.9247\n",
            "Seen so far: 1344 samples\n",
            "Training loss (for one batch) at step 21: 1.8370\n",
            "Seen so far: 1408 samples\n",
            "Training loss (for one batch) at step 22: 1.8494\n",
            "Seen so far: 1472 samples\n",
            "Training loss (for one batch) at step 23: 2.0435\n",
            "Seen so far: 1536 samples\n",
            "Training loss (for one batch) at step 24: 1.9298\n",
            "Seen so far: 1600 samples\n",
            "Training loss (for one batch) at step 25: 1.8362\n",
            "Seen so far: 1664 samples\n",
            "Training loss (for one batch) at step 26: 1.9664\n",
            "Seen so far: 1728 samples\n",
            "Training loss (for one batch) at step 27: 1.8620\n",
            "Seen so far: 1792 samples\n",
            "Training loss (for one batch) at step 28: 1.9903\n",
            "Seen so far: 1856 samples\n",
            "Training loss (for one batch) at step 29: 1.8983\n",
            "Seen so far: 1920 samples\n",
            "Training loss (for one batch) at step 30: 2.0732\n",
            "Seen so far: 1984 samples\n",
            "Training loss (for one batch) at step 31: 2.1639\n",
            "Seen so far: 2048 samples\n",
            "Training loss (for one batch) at step 32: 2.1058\n",
            "Seen so far: 2112 samples\n",
            "Training loss (for one batch) at step 33: 1.7280\n",
            "Seen so far: 2176 samples\n",
            "Training loss (for one batch) at step 34: 1.8767\n",
            "Seen so far: 2240 samples\n",
            "Training loss (for one batch) at step 35: 1.9841\n",
            "Seen so far: 2304 samples\n",
            "Training loss (for one batch) at step 36: 1.9266\n",
            "Seen so far: 2368 samples\n",
            "Training loss (for one batch) at step 37: 1.7438\n",
            "Seen so far: 2432 samples\n",
            "Training loss (for one batch) at step 38: 2.0339\n",
            "Seen so far: 2496 samples\n",
            "Training loss (for one batch) at step 39: 1.9453\n",
            "Seen so far: 2560 samples\n",
            "Training loss (for one batch) at step 40: 1.8811\n",
            "Seen so far: 2624 samples\n",
            "Training loss (for one batch) at step 41: 1.9959\n",
            "Seen so far: 2688 samples\n",
            "Training loss (for one batch) at step 42: 1.8335\n",
            "Seen so far: 2752 samples\n",
            "Training loss (for one batch) at step 43: 1.8790\n",
            "Seen so far: 2816 samples\n",
            "Training loss (for one batch) at step 44: 1.7713\n",
            "Seen so far: 2880 samples\n",
            "Training loss (for one batch) at step 45: 1.8081\n",
            "Seen so far: 2944 samples\n",
            "Training loss (for one batch) at step 46: 2.0737\n",
            "Seen so far: 3008 samples\n",
            "Training loss (for one batch) at step 47: 1.7672\n",
            "Seen so far: 3072 samples\n",
            "Training loss (for one batch) at step 48: 1.9049\n",
            "Seen so far: 3136 samples\n",
            "Training loss (for one batch) at step 49: 2.0334\n",
            "Seen so far: 3200 samples\n",
            "Training loss (for one batch) at step 50: 1.8803\n",
            "Seen so far: 3264 samples\n",
            "Training loss (for one batch) at step 51: 1.8472\n",
            "Seen so far: 3328 samples\n",
            "Training loss (for one batch) at step 52: 1.8524\n",
            "Seen so far: 3392 samples\n",
            "Training loss (for one batch) at step 53: 1.8930\n",
            "Seen so far: 3456 samples\n",
            "Training loss (for one batch) at step 54: 1.7518\n",
            "Seen so far: 3520 samples\n",
            "Training loss (for one batch) at step 55: 1.7925\n",
            "Seen so far: 3584 samples\n",
            "Training loss (for one batch) at step 56: 1.9027\n",
            "Seen so far: 3648 samples\n",
            "Training loss (for one batch) at step 57: 1.8641\n",
            "Seen so far: 3712 samples\n",
            "Training loss (for one batch) at step 58: 1.6171\n",
            "Seen so far: 3776 samples\n",
            "Training loss (for one batch) at step 59: 1.7978\n",
            "Seen so far: 3840 samples\n",
            "Training loss (for one batch) at step 60: 1.9688\n",
            "Seen so far: 3904 samples\n",
            "Training loss (for one batch) at step 61: 1.9267\n",
            "Seen so far: 3968 samples\n",
            "Training loss (for one batch) at step 62: 1.7728\n",
            "Seen so far: 4032 samples\n",
            "Training loss (for one batch) at step 63: 1.8457\n",
            "Seen so far: 4096 samples\n",
            "Training loss (for one batch) at step 64: 1.8444\n",
            "Seen so far: 4160 samples\n",
            "Training loss (for one batch) at step 65: 1.7068\n",
            "Seen so far: 4224 samples\n",
            "Training loss (for one batch) at step 66: 1.8004\n",
            "Seen so far: 4288 samples\n",
            "Training loss (for one batch) at step 67: 1.8896\n",
            "Seen so far: 4352 samples\n",
            "Training loss (for one batch) at step 68: 1.8209\n",
            "Seen so far: 4416 samples\n",
            "Training loss (for one batch) at step 69: 1.7782\n",
            "Seen so far: 4480 samples\n",
            "Training loss (for one batch) at step 70: 1.9154\n",
            "Seen so far: 4544 samples\n",
            "Training loss (for one batch) at step 71: 1.7833\n",
            "Seen so far: 4608 samples\n",
            "Training loss (for one batch) at step 72: 1.8372\n",
            "Seen so far: 4672 samples\n",
            "Training loss (for one batch) at step 73: 1.6818\n",
            "Seen so far: 4736 samples\n",
            "Training loss (for one batch) at step 74: 1.7193\n",
            "Seen so far: 4800 samples\n",
            "Training loss (for one batch) at step 75: 1.9528\n",
            "Seen so far: 4864 samples\n",
            "Training loss (for one batch) at step 76: 1.7830\n",
            "Seen so far: 4928 samples\n",
            "Training loss (for one batch) at step 77: 1.9010\n",
            "Seen so far: 4992 samples\n",
            "Training loss (for one batch) at step 78: 1.7287\n",
            "Seen so far: 5056 samples\n",
            "Training loss (for one batch) at step 79: 1.9870\n",
            "Seen so far: 5120 samples\n",
            "Training loss (for one batch) at step 80: 2.0918\n",
            "Seen so far: 5184 samples\n",
            "Training loss (for one batch) at step 81: 1.9738\n",
            "Seen so far: 5248 samples\n",
            "Training loss (for one batch) at step 82: 1.8376\n",
            "Seen so far: 5312 samples\n",
            "Training loss (for one batch) at step 83: 1.5987\n",
            "Seen so far: 5376 samples\n",
            "Training loss (for one batch) at step 84: 1.9180\n",
            "Seen so far: 5440 samples\n",
            "Training loss (for one batch) at step 85: 1.9880\n",
            "Seen so far: 5504 samples\n",
            "Training loss (for one batch) at step 86: 1.8116\n",
            "Seen so far: 5568 samples\n",
            "Training loss (for one batch) at step 87: 1.6029\n",
            "Seen so far: 5632 samples\n",
            "Training loss (for one batch) at step 88: 1.7332\n",
            "Seen so far: 5696 samples\n",
            "Training loss (for one batch) at step 89: 1.7902\n",
            "Seen so far: 5760 samples\n",
            "Training loss (for one batch) at step 90: 1.8032\n",
            "Seen so far: 5824 samples\n",
            "Training loss (for one batch) at step 91: 1.7252\n",
            "Seen so far: 5888 samples\n",
            "Training loss (for one batch) at step 92: 1.7408\n",
            "Seen so far: 5952 samples\n",
            "Training loss (for one batch) at step 93: 1.7236\n",
            "Seen so far: 6016 samples\n",
            "Training loss (for one batch) at step 94: 1.6976\n",
            "Seen so far: 6080 samples\n",
            "Training loss (for one batch) at step 95: 1.8830\n",
            "Seen so far: 6144 samples\n",
            "Training loss (for one batch) at step 96: 1.6163\n",
            "Seen so far: 6208 samples\n",
            "Training loss (for one batch) at step 97: 1.7784\n",
            "Seen so far: 6272 samples\n",
            "Training loss (for one batch) at step 98: 1.7467\n",
            "Seen so far: 6336 samples\n",
            "Training loss (for one batch) at step 99: 1.7066\n",
            "Seen so far: 6400 samples\n",
            "Training loss (for one batch) at step 100: 1.7000\n",
            "Seen so far: 6464 samples\n",
            "Training loss (for one batch) at step 101: 1.6850\n",
            "Seen so far: 6528 samples\n",
            "Training loss (for one batch) at step 102: 1.7474\n",
            "Seen so far: 6592 samples\n",
            "Training loss (for one batch) at step 103: 1.7611\n",
            "Seen so far: 6656 samples\n",
            "Training loss (for one batch) at step 104: 1.7592\n",
            "Seen so far: 6720 samples\n",
            "Training loss (for one batch) at step 105: 1.6119\n",
            "Seen so far: 6784 samples\n",
            "Training loss (for one batch) at step 106: 1.7377\n",
            "Seen so far: 6848 samples\n",
            "Training loss (for one batch) at step 107: 1.8908\n",
            "Seen so far: 6912 samples\n",
            "Training loss (for one batch) at step 108: 1.7330\n",
            "Seen so far: 6976 samples\n",
            "Training loss (for one batch) at step 109: 1.5992\n",
            "Seen so far: 7040 samples\n",
            "Training loss (for one batch) at step 110: 1.6968\n",
            "Seen so far: 7104 samples\n",
            "Training loss (for one batch) at step 111: 1.5599\n",
            "Seen so far: 7168 samples\n",
            "Training loss (for one batch) at step 112: 1.7645\n",
            "Seen so far: 7232 samples\n",
            "Training loss (for one batch) at step 113: 1.6915\n",
            "Seen so far: 7296 samples\n",
            "Training loss (for one batch) at step 114: 1.7858\n",
            "Seen so far: 7360 samples\n",
            "Training loss (for one batch) at step 115: 1.9582\n",
            "Seen so far: 7424 samples\n",
            "Training loss (for one batch) at step 116: 1.6822\n",
            "Seen so far: 7488 samples\n",
            "Training loss (for one batch) at step 117: 1.7503\n",
            "Seen so far: 7552 samples\n",
            "Training loss (for one batch) at step 118: 1.7975\n",
            "Seen so far: 7616 samples\n",
            "Training loss (for one batch) at step 119: 1.8940\n",
            "Seen so far: 7680 samples\n",
            "Training loss (for one batch) at step 120: 1.7345\n",
            "Seen so far: 7744 samples\n",
            "Training loss (for one batch) at step 121: 1.7817\n",
            "Seen so far: 7808 samples\n",
            "Training loss (for one batch) at step 122: 1.7828\n",
            "Seen so far: 7872 samples\n",
            "Training loss (for one batch) at step 123: 1.7551\n",
            "Seen so far: 7936 samples\n",
            "Training loss (for one batch) at step 124: 1.7422\n",
            "Seen so far: 8000 samples\n",
            "Training loss (for one batch) at step 125: 1.7079\n",
            "Seen so far: 8064 samples\n",
            "Training loss (for one batch) at step 126: 1.8280\n",
            "Seen so far: 8128 samples\n",
            "Training loss (for one batch) at step 127: 1.5498\n",
            "Seen so far: 8192 samples\n",
            "Training loss (for one batch) at step 128: 1.7276\n",
            "Seen so far: 8256 samples\n",
            "Training loss (for one batch) at step 129: 1.8917\n",
            "Seen so far: 8320 samples\n",
            "Training loss (for one batch) at step 130: 1.8553\n",
            "Seen so far: 8384 samples\n",
            "Training loss (for one batch) at step 131: 1.7133\n",
            "Seen so far: 8448 samples\n",
            "Training loss (for one batch) at step 132: 1.9951\n",
            "Seen so far: 8512 samples\n",
            "Training loss (for one batch) at step 133: 1.6826\n",
            "Seen so far: 8576 samples\n",
            "Training loss (for one batch) at step 134: 1.6758\n",
            "Seen so far: 8640 samples\n",
            "Training loss (for one batch) at step 135: 1.6245\n",
            "Seen so far: 8704 samples\n",
            "Training loss (for one batch) at step 136: 1.8766\n",
            "Seen so far: 8768 samples\n",
            "Training loss (for one batch) at step 137: 1.7075\n",
            "Seen so far: 8832 samples\n",
            "Training loss (for one batch) at step 138: 1.7142\n",
            "Seen so far: 8896 samples\n",
            "Training loss (for one batch) at step 139: 2.0633\n",
            "Seen so far: 8960 samples\n",
            "Training loss (for one batch) at step 140: 1.9323\n",
            "Seen so far: 9024 samples\n",
            "Training loss (for one batch) at step 141: 1.7465\n",
            "Seen so far: 9088 samples\n",
            "Training loss (for one batch) at step 142: 1.7190\n",
            "Seen so far: 9152 samples\n",
            "Training loss (for one batch) at step 143: 1.6354\n",
            "Seen so far: 9216 samples\n",
            "Training loss (for one batch) at step 144: 1.6754\n",
            "Seen so far: 9280 samples\n",
            "Training loss (for one batch) at step 145: 1.6707\n",
            "Seen so far: 9344 samples\n",
            "Training loss (for one batch) at step 146: 1.5696\n",
            "Seen so far: 9408 samples\n",
            "Training loss (for one batch) at step 147: 1.8724\n",
            "Seen so far: 9472 samples\n",
            "Training loss (for one batch) at step 148: 1.6944\n",
            "Seen so far: 9536 samples\n",
            "Training loss (for one batch) at step 149: 1.7233\n",
            "Seen so far: 9600 samples\n",
            "Training loss (for one batch) at step 150: 1.6945\n",
            "Seen so far: 9664 samples\n",
            "Training loss (for one batch) at step 151: 1.8254\n",
            "Seen so far: 9728 samples\n",
            "Training loss (for one batch) at step 152: 1.7740\n",
            "Seen so far: 9792 samples\n",
            "Training loss (for one batch) at step 153: 1.4701\n",
            "Seen so far: 9856 samples\n",
            "Training loss (for one batch) at step 154: 1.6123\n",
            "Seen so far: 9920 samples\n",
            "Training loss (for one batch) at step 155: 1.7887\n",
            "Seen so far: 9984 samples\n",
            "Training loss (for one batch) at step 156: 1.7630\n",
            "Seen so far: 10048 samples\n",
            "Training loss (for one batch) at step 157: 1.6758\n",
            "Seen so far: 10112 samples\n",
            "Training loss (for one batch) at step 158: 1.9030\n",
            "Seen so far: 10176 samples\n",
            "Training loss (for one batch) at step 159: 2.0108\n",
            "Seen so far: 10240 samples\n",
            "Training loss (for one batch) at step 160: 1.8005\n",
            "Seen so far: 10304 samples\n",
            "Training loss (for one batch) at step 161: 1.7013\n",
            "Seen so far: 10368 samples\n",
            "Training loss (for one batch) at step 162: 1.7656\n",
            "Seen so far: 10432 samples\n",
            "Training loss (for one batch) at step 163: 2.0083\n",
            "Seen so far: 10496 samples\n",
            "Training loss (for one batch) at step 164: 1.6766\n",
            "Seen so far: 10560 samples\n",
            "Training loss (for one batch) at step 165: 1.8033\n",
            "Seen so far: 10624 samples\n",
            "Training loss (for one batch) at step 166: 1.6330\n",
            "Seen so far: 10688 samples\n",
            "Training loss (for one batch) at step 167: 1.6005\n",
            "Seen so far: 10752 samples\n",
            "Training loss (for one batch) at step 168: 1.7219\n",
            "Seen so far: 10816 samples\n",
            "Training loss (for one batch) at step 169: 1.5521\n",
            "Seen so far: 10880 samples\n",
            "Training loss (for one batch) at step 170: 1.8550\n",
            "Seen so far: 10944 samples\n",
            "Training loss (for one batch) at step 171: 1.5663\n",
            "Seen so far: 11008 samples\n",
            "Training loss (for one batch) at step 172: 1.8642\n",
            "Seen so far: 11072 samples\n",
            "Training loss (for one batch) at step 173: 1.6767\n",
            "Seen so far: 11136 samples\n",
            "Training loss (for one batch) at step 174: 1.6355\n",
            "Seen so far: 11200 samples\n",
            "Training loss (for one batch) at step 175: 1.6533\n",
            "Seen so far: 11264 samples\n",
            "Training loss (for one batch) at step 176: 1.5329\n",
            "Seen so far: 11328 samples\n",
            "Training loss (for one batch) at step 177: 1.8148\n",
            "Seen so far: 11392 samples\n",
            "Training loss (for one batch) at step 178: 1.8121\n",
            "Seen so far: 11456 samples\n",
            "Training loss (for one batch) at step 179: 1.6836\n",
            "Seen so far: 11520 samples\n",
            "Training loss (for one batch) at step 180: 1.4429\n",
            "Seen so far: 11584 samples\n",
            "Training loss (for one batch) at step 181: 1.6157\n",
            "Seen so far: 11648 samples\n",
            "Training loss (for one batch) at step 182: 1.6661\n",
            "Seen so far: 11712 samples\n",
            "Training loss (for one batch) at step 183: 1.6646\n",
            "Seen so far: 11776 samples\n",
            "Training loss (for one batch) at step 184: 1.8320\n",
            "Seen so far: 11840 samples\n",
            "Training loss (for one batch) at step 185: 1.8801\n",
            "Seen so far: 11904 samples\n",
            "Training loss (for one batch) at step 186: 1.7498\n",
            "Seen so far: 11968 samples\n",
            "Training loss (for one batch) at step 187: 1.6983\n",
            "Seen so far: 12032 samples\n",
            "Training loss (for one batch) at step 188: 1.8267\n",
            "Seen so far: 12096 samples\n",
            "Training loss (for one batch) at step 189: 1.7820\n",
            "Seen so far: 12160 samples\n",
            "Training loss (for one batch) at step 190: 1.6497\n",
            "Seen so far: 12224 samples\n",
            "Training loss (for one batch) at step 191: 1.8737\n",
            "Seen so far: 12288 samples\n",
            "Training loss (for one batch) at step 192: 1.8340\n",
            "Seen so far: 12352 samples\n",
            "Training loss (for one batch) at step 193: 1.6429\n",
            "Seen so far: 12416 samples\n",
            "Training loss (for one batch) at step 194: 1.5632\n",
            "Seen so far: 12480 samples\n",
            "Training loss (for one batch) at step 195: 1.6484\n",
            "Seen so far: 12544 samples\n",
            "Training loss (for one batch) at step 196: 1.7501\n",
            "Seen so far: 12608 samples\n",
            "Training loss (for one batch) at step 197: 1.7073\n",
            "Seen so far: 12672 samples\n",
            "Training loss (for one batch) at step 198: 1.6715\n",
            "Seen so far: 12736 samples\n",
            "Training loss (for one batch) at step 199: 1.6179\n",
            "Seen so far: 12800 samples\n",
            "Training loss (for one batch) at step 200: 1.4751\n",
            "Seen so far: 12864 samples\n",
            "Training loss (for one batch) at step 201: 1.7058\n",
            "Seen so far: 12928 samples\n",
            "Training loss (for one batch) at step 202: 1.7595\n",
            "Seen so far: 12992 samples\n",
            "Training loss (for one batch) at step 203: 1.8337\n",
            "Seen so far: 13056 samples\n",
            "Training loss (for one batch) at step 204: 1.7260\n",
            "Seen so far: 13120 samples\n",
            "Training loss (for one batch) at step 205: 1.8208\n",
            "Seen so far: 13184 samples\n",
            "Training loss (for one batch) at step 206: 1.8109\n",
            "Seen so far: 13248 samples\n",
            "Training loss (for one batch) at step 207: 1.8291\n",
            "Seen so far: 13312 samples\n",
            "Training loss (for one batch) at step 208: 1.6699\n",
            "Seen so far: 13376 samples\n",
            "Training loss (for one batch) at step 209: 1.7274\n",
            "Seen so far: 13440 samples\n",
            "Training loss (for one batch) at step 210: 1.5805\n",
            "Seen so far: 13504 samples\n",
            "Training loss (for one batch) at step 211: 1.6529\n",
            "Seen so far: 13568 samples\n",
            "Training loss (for one batch) at step 212: 1.6203\n",
            "Seen so far: 13632 samples\n",
            "Training loss (for one batch) at step 213: 1.7627\n",
            "Seen so far: 13696 samples\n",
            "Training loss (for one batch) at step 214: 1.6809\n",
            "Seen so far: 13760 samples\n",
            "Training loss (for one batch) at step 215: 1.6917\n",
            "Seen so far: 13824 samples\n",
            "Training loss (for one batch) at step 216: 1.5134\n",
            "Seen so far: 13888 samples\n",
            "Training loss (for one batch) at step 217: 1.6108\n",
            "Seen so far: 13952 samples\n",
            "Training loss (for one batch) at step 218: 1.6531\n",
            "Seen so far: 14016 samples\n",
            "Training loss (for one batch) at step 219: 1.7384\n",
            "Seen so far: 14080 samples\n",
            "Training loss (for one batch) at step 220: 1.8218\n",
            "Seen so far: 14144 samples\n",
            "Training loss (for one batch) at step 221: 1.8225\n",
            "Seen so far: 14208 samples\n",
            "Training loss (for one batch) at step 222: 1.6074\n",
            "Seen so far: 14272 samples\n",
            "Training loss (for one batch) at step 223: 1.8503\n",
            "Seen so far: 14336 samples\n",
            "Training loss (for one batch) at step 224: 1.7174\n",
            "Seen so far: 14400 samples\n",
            "Training loss (for one batch) at step 225: 1.8191\n",
            "Seen so far: 14464 samples\n",
            "Training loss (for one batch) at step 226: 1.5381\n",
            "Seen so far: 14528 samples\n",
            "Training loss (for one batch) at step 227: 1.4391\n",
            "Seen so far: 14592 samples\n",
            "Training loss (for one batch) at step 228: 1.6716\n",
            "Seen so far: 14656 samples\n",
            "Training loss (for one batch) at step 229: 1.7401\n",
            "Seen so far: 14720 samples\n",
            "Training loss (for one batch) at step 230: 1.7131\n",
            "Seen so far: 14784 samples\n",
            "Training loss (for one batch) at step 231: 1.7715\n",
            "Seen so far: 14848 samples\n",
            "Training loss (for one batch) at step 232: 1.4825\n",
            "Seen so far: 14912 samples\n",
            "Training loss (for one batch) at step 233: 1.8005\n",
            "Seen so far: 14976 samples\n",
            "Training loss (for one batch) at step 234: 1.7160\n",
            "Seen so far: 15040 samples\n",
            "Training loss (for one batch) at step 235: 1.9593\n",
            "Seen so far: 15104 samples\n",
            "Training loss (for one batch) at step 236: 1.7868\n",
            "Seen so far: 15168 samples\n",
            "Training loss (for one batch) at step 237: 1.5219\n",
            "Seen so far: 15232 samples\n",
            "Training loss (for one batch) at step 238: 1.4037\n",
            "Seen so far: 15296 samples\n",
            "Training loss (for one batch) at step 239: 1.7019\n",
            "Seen so far: 15360 samples\n",
            "Training loss (for one batch) at step 240: 1.8381\n",
            "Seen so far: 15424 samples\n",
            "Training loss (for one batch) at step 241: 1.7211\n",
            "Seen so far: 15488 samples\n",
            "Training loss (for one batch) at step 242: 1.7925\n",
            "Seen so far: 15552 samples\n",
            "Training loss (for one batch) at step 243: 2.0986\n",
            "Seen so far: 15616 samples\n",
            "Training loss (for one batch) at step 244: 1.5335\n",
            "Seen so far: 15680 samples\n",
            "Training loss (for one batch) at step 245: 1.6524\n",
            "Seen so far: 15744 samples\n",
            "Training loss (for one batch) at step 246: 1.6174\n",
            "Seen so far: 15808 samples\n",
            "Training loss (for one batch) at step 247: 1.7696\n",
            "Seen so far: 15872 samples\n",
            "Training loss (for one batch) at step 248: 1.8738\n",
            "Seen so far: 15936 samples\n",
            "Training loss (for one batch) at step 249: 1.8306\n",
            "Seen so far: 16000 samples\n",
            "Training loss (for one batch) at step 250: 1.6138\n",
            "Seen so far: 16064 samples\n",
            "Training loss (for one batch) at step 251: 1.6524\n",
            "Seen so far: 16128 samples\n",
            "Training loss (for one batch) at step 252: 1.7694\n",
            "Seen so far: 16192 samples\n",
            "Training loss (for one batch) at step 253: 1.5738\n",
            "Seen so far: 16256 samples\n",
            "Training loss (for one batch) at step 254: 1.8465\n",
            "Seen so far: 16320 samples\n",
            "Training loss (for one batch) at step 255: 1.5596\n",
            "Seen so far: 16384 samples\n",
            "Training loss (for one batch) at step 256: 1.6776\n",
            "Seen so far: 16448 samples\n",
            "Training loss (for one batch) at step 257: 1.5928\n",
            "Seen so far: 16512 samples\n",
            "Training loss (for one batch) at step 258: 1.9505\n",
            "Seen so far: 16576 samples\n",
            "Training loss (for one batch) at step 259: 1.8437\n",
            "Seen so far: 16640 samples\n",
            "Training loss (for one batch) at step 260: 1.4556\n",
            "Seen so far: 16704 samples\n",
            "Training loss (for one batch) at step 261: 1.7719\n",
            "Seen so far: 16768 samples\n",
            "Training loss (for one batch) at step 262: 1.6584\n",
            "Seen so far: 16832 samples\n",
            "Training loss (for one batch) at step 263: 1.8373\n",
            "Seen so far: 16896 samples\n",
            "Training loss (for one batch) at step 264: 1.4102\n",
            "Seen so far: 16960 samples\n",
            "Training loss (for one batch) at step 265: 1.5783\n",
            "Seen so far: 17024 samples\n",
            "Training loss (for one batch) at step 266: 1.6888\n",
            "Seen so far: 17088 samples\n",
            "Training loss (for one batch) at step 267: 1.6709\n",
            "Seen so far: 17152 samples\n",
            "Training loss (for one batch) at step 268: 1.7493\n",
            "Seen so far: 17216 samples\n",
            "Training loss (for one batch) at step 269: 1.6711\n",
            "Seen so far: 17280 samples\n",
            "Training loss (for one batch) at step 270: 1.6008\n",
            "Seen so far: 17344 samples\n",
            "Training loss (for one batch) at step 271: 1.6952\n",
            "Seen so far: 17408 samples\n",
            "Training loss (for one batch) at step 272: 1.5962\n",
            "Seen so far: 17472 samples\n",
            "Training loss (for one batch) at step 273: 1.6065\n",
            "Seen so far: 17536 samples\n",
            "Training loss (for one batch) at step 274: 1.6787\n",
            "Seen so far: 17600 samples\n",
            "Training loss (for one batch) at step 275: 1.5305\n",
            "Seen so far: 17664 samples\n",
            "Training loss (for one batch) at step 276: 1.8989\n",
            "Seen so far: 17728 samples\n",
            "Training loss (for one batch) at step 277: 1.7981\n",
            "Seen so far: 17792 samples\n",
            "Training loss (for one batch) at step 278: 1.7884\n",
            "Seen so far: 17856 samples\n",
            "Training loss (for one batch) at step 279: 1.3133\n",
            "Seen so far: 17920 samples\n",
            "Training loss (for one batch) at step 280: 1.6060\n",
            "Seen so far: 17984 samples\n",
            "Training loss (for one batch) at step 281: 1.7372\n",
            "Seen so far: 18048 samples\n",
            "Training loss (for one batch) at step 282: 1.4367\n",
            "Seen so far: 18112 samples\n",
            "Training loss (for one batch) at step 283: 1.6955\n",
            "Seen so far: 18176 samples\n",
            "Training loss (for one batch) at step 284: 1.5197\n",
            "Seen so far: 18240 samples\n",
            "Training loss (for one batch) at step 285: 1.6237\n",
            "Seen so far: 18304 samples\n",
            "Training loss (for one batch) at step 286: 1.7008\n",
            "Seen so far: 18368 samples\n",
            "Training loss (for one batch) at step 287: 1.5335\n",
            "Seen so far: 18432 samples\n",
            "Training loss (for one batch) at step 288: 1.6688\n",
            "Seen so far: 18496 samples\n",
            "Training loss (for one batch) at step 289: 1.6191\n",
            "Seen so far: 18560 samples\n",
            "Training loss (for one batch) at step 290: 1.6487\n",
            "Seen so far: 18624 samples\n",
            "Training loss (for one batch) at step 291: 1.7345\n",
            "Seen so far: 18688 samples\n",
            "Training loss (for one batch) at step 292: 1.6076\n",
            "Seen so far: 18752 samples\n",
            "Training loss (for one batch) at step 293: 1.5370\n",
            "Seen so far: 18816 samples\n",
            "Training loss (for one batch) at step 294: 1.7058\n",
            "Seen so far: 18880 samples\n",
            "Training loss (for one batch) at step 295: 1.5498\n",
            "Seen so far: 18944 samples\n",
            "Training loss (for one batch) at step 296: 1.8395\n",
            "Seen so far: 19008 samples\n",
            "Training loss (for one batch) at step 297: 1.7452\n",
            "Seen so far: 19072 samples\n",
            "Training loss (for one batch) at step 298: 1.4792\n",
            "Seen so far: 19136 samples\n",
            "Training loss (for one batch) at step 299: 1.5885\n",
            "Seen so far: 19200 samples\n",
            "Training loss (for one batch) at step 300: 1.6452\n",
            "Seen so far: 19264 samples\n",
            "Training loss (for one batch) at step 301: 1.6389\n",
            "Seen so far: 19328 samples\n",
            "Training loss (for one batch) at step 302: 1.5287\n",
            "Seen so far: 19392 samples\n",
            "Training loss (for one batch) at step 303: 1.3991\n",
            "Seen so far: 19456 samples\n",
            "Training loss (for one batch) at step 304: 1.5748\n",
            "Seen so far: 19520 samples\n",
            "Training loss (for one batch) at step 305: 1.8185\n",
            "Seen so far: 19584 samples\n",
            "Training loss (for one batch) at step 306: 1.3789\n",
            "Seen so far: 19648 samples\n",
            "Training loss (for one batch) at step 307: 1.6580\n",
            "Seen so far: 19712 samples\n",
            "Training loss (for one batch) at step 308: 1.6823\n",
            "Seen so far: 19776 samples\n",
            "Training loss (for one batch) at step 309: 1.5226\n",
            "Seen so far: 19840 samples\n",
            "Training loss (for one batch) at step 310: 1.6228\n",
            "Seen so far: 19904 samples\n",
            "Training loss (for one batch) at step 311: 1.5874\n",
            "Seen so far: 19968 samples\n",
            "Training loss (for one batch) at step 312: 1.7353\n",
            "Seen so far: 20032 samples\n",
            "Training loss (for one batch) at step 313: 1.5979\n",
            "Seen so far: 20096 samples\n",
            "Training loss (for one batch) at step 314: 1.8006\n",
            "Seen so far: 20160 samples\n",
            "Training loss (for one batch) at step 315: 1.5014\n",
            "Seen so far: 20224 samples\n",
            "Training loss (for one batch) at step 316: 1.4854\n",
            "Seen so far: 20288 samples\n",
            "Training loss (for one batch) at step 317: 1.5261\n",
            "Seen so far: 20352 samples\n",
            "Training loss (for one batch) at step 318: 1.4538\n",
            "Seen so far: 20416 samples\n",
            "Training loss (for one batch) at step 319: 1.6261\n",
            "Seen so far: 20480 samples\n",
            "Training loss (for one batch) at step 320: 1.6465\n",
            "Seen so far: 20544 samples\n",
            "Training loss (for one batch) at step 321: 1.6593\n",
            "Seen so far: 20608 samples\n",
            "Training loss (for one batch) at step 322: 1.5530\n",
            "Seen so far: 20672 samples\n",
            "Training loss (for one batch) at step 323: 1.7404\n",
            "Seen so far: 20736 samples\n",
            "Training loss (for one batch) at step 324: 1.4734\n",
            "Seen so far: 20800 samples\n",
            "Training loss (for one batch) at step 325: 1.5078\n",
            "Seen so far: 20864 samples\n",
            "Training loss (for one batch) at step 326: 1.4805\n",
            "Seen so far: 20928 samples\n",
            "Training loss (for one batch) at step 327: 1.5565\n",
            "Seen so far: 20992 samples\n",
            "Training loss (for one batch) at step 328: 1.5640\n",
            "Seen so far: 21056 samples\n",
            "Training loss (for one batch) at step 329: 1.4792\n",
            "Seen so far: 21120 samples\n",
            "Training loss (for one batch) at step 330: 1.5179\n",
            "Seen so far: 21184 samples\n",
            "Training loss (for one batch) at step 331: 1.7354\n",
            "Seen so far: 21248 samples\n",
            "Training loss (for one batch) at step 332: 1.5475\n",
            "Seen so far: 21312 samples\n",
            "Training loss (for one batch) at step 333: 1.6885\n",
            "Seen so far: 21376 samples\n",
            "Training loss (for one batch) at step 334: 1.7732\n",
            "Seen so far: 21440 samples\n",
            "Training loss (for one batch) at step 335: 1.5722\n",
            "Seen so far: 21504 samples\n",
            "Training loss (for one batch) at step 336: 1.6168\n",
            "Seen so far: 21568 samples\n",
            "Training loss (for one batch) at step 337: 1.3055\n",
            "Seen so far: 21632 samples\n",
            "Training loss (for one batch) at step 338: 1.5666\n",
            "Seen so far: 21696 samples\n",
            "Training loss (for one batch) at step 339: 1.5429\n",
            "Seen so far: 21760 samples\n",
            "Training loss (for one batch) at step 340: 1.6851\n",
            "Seen so far: 21824 samples\n",
            "Training loss (for one batch) at step 341: 1.4994\n",
            "Seen so far: 21888 samples\n",
            "Training loss (for one batch) at step 342: 1.6563\n",
            "Seen so far: 21952 samples\n",
            "Training loss (for one batch) at step 343: 1.7329\n",
            "Seen so far: 22016 samples\n",
            "Training loss (for one batch) at step 344: 1.5477\n",
            "Seen so far: 22080 samples\n",
            "Training loss (for one batch) at step 345: 1.3970\n",
            "Seen so far: 22144 samples\n",
            "Training loss (for one batch) at step 346: 1.5536\n",
            "Seen so far: 22208 samples\n",
            "Training loss (for one batch) at step 347: 1.6007\n",
            "Seen so far: 22272 samples\n",
            "Training loss (for one batch) at step 348: 1.5633\n",
            "Seen so far: 22336 samples\n",
            "Training loss (for one batch) at step 349: 1.4942\n",
            "Seen so far: 22400 samples\n",
            "Training loss (for one batch) at step 350: 1.6612\n",
            "Seen so far: 22464 samples\n",
            "Training loss (for one batch) at step 351: 1.6208\n",
            "Seen so far: 22528 samples\n",
            "Training loss (for one batch) at step 352: 1.4661\n",
            "Seen so far: 22592 samples\n",
            "Training loss (for one batch) at step 353: 1.6327\n",
            "Seen so far: 22656 samples\n",
            "Training loss (for one batch) at step 354: 1.6068\n",
            "Seen so far: 22720 samples\n",
            "Training loss (for one batch) at step 355: 1.5726\n",
            "Seen so far: 22784 samples\n",
            "Training loss (for one batch) at step 356: 1.6916\n",
            "Seen so far: 22848 samples\n",
            "Training loss (for one batch) at step 357: 1.7134\n",
            "Seen so far: 22912 samples\n",
            "Training loss (for one batch) at step 358: 1.3325\n",
            "Seen so far: 22976 samples\n",
            "Training loss (for one batch) at step 359: 1.5769\n",
            "Seen so far: 23040 samples\n",
            "Training loss (for one batch) at step 360: 1.6613\n",
            "Seen so far: 23104 samples\n",
            "Training loss (for one batch) at step 361: 1.4827\n",
            "Seen so far: 23168 samples\n",
            "Training loss (for one batch) at step 362: 1.7386\n",
            "Seen so far: 23232 samples\n",
            "Training loss (for one batch) at step 363: 1.4863\n",
            "Seen so far: 23296 samples\n",
            "Training loss (for one batch) at step 364: 1.3413\n",
            "Seen so far: 23360 samples\n",
            "Training loss (for one batch) at step 365: 1.6441\n",
            "Seen so far: 23424 samples\n",
            "Training loss (for one batch) at step 366: 1.7397\n",
            "Seen so far: 23488 samples\n",
            "Training loss (for one batch) at step 367: 1.6029\n",
            "Seen so far: 23552 samples\n",
            "Training loss (for one batch) at step 368: 1.4628\n",
            "Seen so far: 23616 samples\n",
            "Training loss (for one batch) at step 369: 1.4852\n",
            "Seen so far: 23680 samples\n",
            "Training loss (for one batch) at step 370: 1.3613\n",
            "Seen so far: 23744 samples\n",
            "Training loss (for one batch) at step 371: 1.8217\n",
            "Seen so far: 23808 samples\n",
            "Training loss (for one batch) at step 372: 1.8250\n",
            "Seen so far: 23872 samples\n",
            "Training loss (for one batch) at step 373: 1.6192\n",
            "Seen so far: 23936 samples\n",
            "Training loss (for one batch) at step 374: 1.6448\n",
            "Seen so far: 24000 samples\n",
            "Training loss (for one batch) at step 375: 1.4532\n",
            "Seen so far: 24064 samples\n",
            "Training loss (for one batch) at step 376: 1.4892\n",
            "Seen so far: 24128 samples\n",
            "Training loss (for one batch) at step 377: 1.4710\n",
            "Seen so far: 24192 samples\n",
            "Training loss (for one batch) at step 378: 1.5235\n",
            "Seen so far: 24256 samples\n",
            "Training loss (for one batch) at step 379: 1.4787\n",
            "Seen so far: 24320 samples\n",
            "Training loss (for one batch) at step 380: 1.5067\n",
            "Seen so far: 24384 samples\n",
            "Training loss (for one batch) at step 381: 1.3751\n",
            "Seen so far: 24448 samples\n",
            "Training loss (for one batch) at step 382: 1.5108\n",
            "Seen so far: 24512 samples\n",
            "Training loss (for one batch) at step 383: 1.4765\n",
            "Seen so far: 24576 samples\n",
            "Training loss (for one batch) at step 384: 1.5696\n",
            "Seen so far: 24640 samples\n",
            "Training loss (for one batch) at step 385: 1.5743\n",
            "Seen so far: 24704 samples\n",
            "Training loss (for one batch) at step 386: 1.6227\n",
            "Seen so far: 24768 samples\n",
            "Training loss (for one batch) at step 387: 1.7309\n",
            "Seen so far: 24832 samples\n",
            "Training loss (for one batch) at step 388: 1.6653\n",
            "Seen so far: 24896 samples\n",
            "Training loss (for one batch) at step 389: 1.4599\n",
            "Seen so far: 24960 samples\n",
            "Training loss (for one batch) at step 390: 1.4490\n",
            "Seen so far: 25024 samples\n",
            "n_chosen =  2\n",
            "alpha =  tf.Tensor([-0.00143605  0.00139522], shape=(2,), dtype=float32)\n",
            "n_chosen =  2\n",
            "alpha =  tf.Tensor([-0.00202773 -0.00181885 -0.00083759], shape=(3,), dtype=float32)\n",
            "n_chosen =  2\n",
            "alpha =  tf.Tensor([-0.00150842  0.00086431  0.00169512  0.00048355], shape=(4,), dtype=float32)\n",
            "n_chosen =  2\n",
            "alpha =  tf.Tensor(\n",
            "[ 8.8767393e-04 -7.1062252e-04 -4.1492944e-04 -7.9704194e-05\n",
            " -3.4158278e-04], shape=(5,), dtype=float32)\n",
            "Final architecture: {'reduce_n2_p0': 'maxpool', 'reduce_n2_p1': 'avgpool', 'reduce_n3_p0': 'sepconv5x5', 'reduce_n3_p1': 'skipconnect', 'reduce_n3_p2': 'sepconv5x5', 'reduce_n4_p0': 'skipconnect', 'reduce_n4_p1': 'dilconv5x5', 'reduce_n4_p2': 'dilconv3x3', 'reduce_n4_p3': 'dilconv3x3', 'reduce_n5_p0': 'avgpool', 'reduce_n5_p1': 'dilconv5x5', 'reduce_n5_p2': 'skipconnect', 'reduce_n5_p3': 'skipconnect', 'reduce_n5_p4': 'dilconv5x5', 'reduce_n2_switch': [1, 0], 'reduce_n3_switch': [2, 1], 'reduce_n4_switch': [2, 1], 'reduce_n5_switch': [0, 3]}\n"
          ]
        }
      ],
      "source": [
        "# %load C:\\Users\\cnerin\\Documents\\NAS\\code_tensorflow\\code_branche_gitlab_main\\nas\\darts\\project\\tensorflow\\search.py\n",
        "from argparse import ArgumentParser\n",
        "\n",
        "import tensorflow as tf\n",
        "\n",
        "#import datasets\n",
        "#from model import CNN\n",
        "#from utils import accuracy, visualize\n",
        "#from darts import DartsTrainer\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    \"\"\"\n",
        "\n",
        "    parser = ArgumentParser(\"darts\")\n",
        "    parser.add_argument(\"--layers\", default=8, type=int)\n",
        "    parser.add_argument(\"--batch-size\", default=64, type=int)\n",
        "    parser.add_argument(\"--log-frequency\", default=10, type=int)\n",
        "    parser.add_argument(\"--epochs\", default=50, type=int)\n",
        "    parser.add_argument(\"--channels\", default=16, type=int)\n",
        "    parser.add_argument(\"--unrolled\", default=False, action=\"store_true\")\n",
        "    parser.add_argument(\"--visualization\", default=False, action=\"store_true\")\n",
        "    args = parser.parse_args()\n",
        "    \"\"\"\n",
        "    args = {}\n",
        "    args[\"layers\"] = 2\n",
        "    args[\"batch_size\"] =64\n",
        "    args[\"log_frequency\"] = 1\n",
        "    args[\"epochs\"] = 1\n",
        "    args[\"channels\"] = 16\n",
        "    args[\"unrolled\"] = False\n",
        "    args[\"visualization\"] = False\n",
        "\n",
        "        # Récupérer les données du dataset\n",
        "    n_channel, dataset_train, dataset_valid = get_dataset(\"cifar10\")\n",
        "\n",
        "        \n",
        "        # construction du modèle entier + appel des fonction d'entrainement (forward)\n",
        "\n",
        "\n",
        "    \n",
        "    channels_arg = args[\"channels\"]\n",
        "    layers_arg = args[\"layers\"]\n",
        "    model = CNN(\n",
        "        input_size=32, \n",
        "        in_channels=n_channel, \n",
        "        channels=channels_arg, \n",
        "        n_classes=10, \n",
        "        n_layers=layers_arg\n",
        "        )\n",
        "\n",
        "        # calcule la cross entropy loss entre l'entrée et la cible\n",
        "    criterion = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
        "\n",
        "        # Implémente la descente de gradient stochastique\n",
        "    optim = tf.keras.optimizers.SGD(learning_rate= 0.025, momentum=0.9)\n",
        "\n",
        "    \"\"\"\n",
        "    trainer = DartsTrainer(\n",
        "        model=model,\n",
        "        loss=criterion,\n",
        "        metrics=lambda output, target: accuracy(output, target, topk=(1,)),\n",
        "        optimizer=optim,\n",
        "        num_epochs=args.epochs,\n",
        "        dataset=dataset_train,\n",
        "        batch_size=args.batch_size,\n",
        "        log_frequency=args.log_frequency,\n",
        "        unrolled=args.unrolled\n",
        "    )\n",
        "    \"\"\"\n",
        "    epochs_arg = args[\"epochs\"]\n",
        "    batch_size_arg = args[\"batch_size\"]\n",
        "    log_frequency_arg = args[\"log_frequency\"]\n",
        "    unrolled_arg = args[\"unrolled\"]\n",
        "    trainer = DartsTrainer(\n",
        "        model=model,\n",
        "        loss=criterion,\n",
        "        metrics=lambda output, target: accuracy(output, target, topk=(1,)),\n",
        "        optimizer=optim,\n",
        "        num_epochs=epochs_arg,\n",
        "        dataset=dataset_train,\n",
        "        batch_size=batch_size_arg,\n",
        "        log_frequency=log_frequency_arg,\n",
        "        unrolled=unrolled_arg\n",
        "    )\n",
        "\n",
        "    trainer.fit()\n",
        "    final_architecture = trainer.export()\n",
        "    print('Final architecture:', final_architecture)\n",
        "    \"\"\"\n",
        "    if args.visualization :\n",
        "        visualize(final_architecture)\n",
        "    \"\"\"\n",
        "    #json.dump(trainer.export(), open('checkpoint.json', 'w'))\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dccd7830",
      "metadata": {
        "id": "dccd7830"
      },
      "outputs": [],
      "source": [
        ""
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.12"
    },
    "colab": {
      "name": "exec_colab3.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 5
}