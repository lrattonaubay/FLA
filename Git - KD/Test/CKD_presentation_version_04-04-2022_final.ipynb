{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1dea5343",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "84a207ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CKD():\n",
    "    def __init__(\n",
    "        self,\n",
    "        teacher: tf.keras.Model,\n",
    "        student: tf.keras.Model,\n",
    "        alreadySoftmax: bool = True,\n",
    "        optimizer: tf.keras.optimizers = tf.keras.optimizers.Adam(),\n",
    "        studentLoss: tf.keras.losses = tf.keras.losses.CategoricalCrossentropy(),\n",
    "        distilLoss: tf.keras.losses = tf.keras.losses.KLDivergence(),\n",
    "        metrics = [tf.keras.metrics.CategoricalAccuracy()]\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        Initialize the teacher model, the student model and their last layer index.\n",
    "\n",
    "        Input(s)\n",
    "        ---------------\n",
    "        teacher: A trained Keras Sequential or Functional model (Sub-class models are not supported).\n",
    "        student: An untrained Keras Sequential or Functional model (Sub-class models are not supported).\n",
    "        alreadySoftmax : If the last layer is softmax it must be true, else it must be false (for teacher and student). By default true.\n",
    "        optimizer: Optimizer instance. By default Adam.\n",
    "        distilLoss: Loss instance. By default KLDivergence.\n",
    "        metrics: List of metrics to be evaluated by the model during training and testing. By default accuracy.\n",
    "        \"\"\"\n",
    "        self.teacher = teacher\n",
    "        self.student = student\n",
    "        self.alreadySoftmax = alreadySoftmax\n",
    "        self.metrics = metrics\n",
    "        self.optimizer = optimizer\n",
    "        self.distilLoss = distilLoss\n",
    "        self.studentLoss = studentLoss\n",
    "\n",
    "    def distil(\n",
    "        self,\n",
    "        trainData: tf.data.Dataset,\n",
    "        valData: tf.data.Dataset,\n",
    "        epochs: int = 1,\n",
    "        trainBatchSize: int = 32,\n",
    "        valBatchSize: int = 32,\n",
    "        alpha: float = 0.1,\n",
    "        temperature: int = 3\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Description\n",
    "        ---------------\n",
    "        Distil the knowledge of the teacher to the student.\n",
    "\n",
    "        Input(s)\n",
    "        ---------------\n",
    "        trainData: TensorFlow Dataset with training images.\n",
    "        valData: TensorFlow Dataset with validation images.\n",
    "        epochs: Number of epochs to distil the model. By default 1.\n",
    "        trainBatchSize: Number of samples per gradient update. By default 32.\n",
    "        valBatchSize: Number of samples per validation batch. By default 32.\n",
    "        alpha: Loss weighting factor. By default 0.1 (10% student's loss, 90% distillation's loss).\n",
    "        temperature: Temperature for softening probability distributions. Larger temperature gives softer distributions. By default 3.\n",
    "\n",
    "        Output(s)\n",
    "        ---------------\n",
    "        distilled_model: Distilled Keras Sequential or Functional student model.\n",
    "        \"\"\"\n",
    "        # Compiling student model\n",
    "        self.student.compile(\n",
    "            optimizer=self.optimizer,\n",
    "            loss=self.studentLoss,\n",
    "            metrics=self.metrics\n",
    "        )\n",
    "        # Prepare the training dataset\n",
    "        trainData = trainData.shuffle(1024).batch(batch_size=trainBatchSize)\n",
    "        batchNbTrain = trainData.cardinality().numpy()\n",
    "        # Prepare the validation dataset\n",
    "        valData = valData.batch(batch_size=valBatchSize)\n",
    "        batchNbVal = valData.cardinality().numpy()\n",
    "        # Getting metrics\n",
    "        metricsHandlerTruth = []\n",
    "        metricsHandlerTeacher = []\n",
    "        for metric in self.metrics:\n",
    "            metricsHandlerTruth.append(tf.keras.metrics.get(metric))\n",
    "            metricsHandlerTeacher.append(tf.keras.metrics.get(metric))\n",
    "        # Training\n",
    "        for epoch in range(epochs):\n",
    "            print(\"Distillation Epoch {}/{}\".format(epoch+1, epochs))\n",
    "            pb_train = tf.keras.utils.Progbar(batchNbTrain)\n",
    "            for step, (x_batch_train, y_batch_train) in enumerate(trainData):\n",
    "                # Teacher's forward pass\n",
    "                teacherPredsTrain = self.teacher(x_batch_train, training=False)\n",
    "                with tf.GradientTape() as tape:\n",
    "                    # Student's forward pass\n",
    "                    studentPredsTrain = self.student(x_batch_train, training=True)\n",
    "                    # Computing distillation and student losses\n",
    "                    if self.alreadySoftmax == False:\n",
    "                        distilLossTrain = self.distilLoss(\n",
    "                            tf.keras.activations.softmax(teacherPredsTrain) / temperature,\n",
    "                            tf.keras.activations.softmax(studentPredsTrain) / temperature\n",
    "                        )\n",
    "                        studentLossTrain = self.studentLoss(\n",
    "                            y_batch_train,\n",
    "                            tf.keras.activations.softmax(studentPredsTrain)\n",
    "                        )\n",
    "                    else:\n",
    "                        distilLossTrain = self.distilLoss(\n",
    "                            teacherPredsTrain / temperature,\n",
    "                            studentPredsTrain / temperature\n",
    "                        )\n",
    "                        studentLossTrain = self.studentLoss(\n",
    "                            y_batch_train,\n",
    "                            tf.keras.activations.softmax(studentPredsTrain)\n",
    "                        )\n",
    "                    # Computing loss\n",
    "                    lossTrain = (alpha * studentLossTrain) + ((1 - alpha) * distilLossTrain)\n",
    "                # Computing metrics\n",
    "                metricsTuplesTrain = []\n",
    "                # Comparing to ground truth values\n",
    "                for handler in metricsHandlerTruth:\n",
    "                    handler.reset_state()\n",
    "                    handler.update_state(y_batch_train.numpy(), studentPredsTrain.numpy())\n",
    "                    metricsTuplesTrain.append((handler.name, handler.result().numpy()))\n",
    "                # Comparing to teacher\n",
    "                for handler in metricsHandlerTeacher:\n",
    "                    handler.reset_state()\n",
    "                    handler.update_state(teacherPredsTrain.numpy(), studentPredsTrain.numpy())\n",
    "                    metricsTuplesTrain.append((\"Distillation_\" + handler.name, handler.result().numpy()))\n",
    "                # Updating progress bar losses and metrics\n",
    "                lossesTuplesTrain = [\n",
    "                    ('Loss', lossTrain),\n",
    "                    ('DistilLoss', distilLossTrain),\n",
    "                    ('StudentLoss', studentLossTrain)\n",
    "                ]\n",
    "                globalTuplesTrain = lossesTuplesTrain + metricsTuplesTrain\n",
    "                pb_train.add(1, values=globalTuplesTrain)\n",
    "                # Computing gradient\n",
    "                gradients = tape.gradient(lossTrain, self.student.trainable_variables)\n",
    "                # Update weights\n",
    "                self.student.optimizer.apply_gradients(zip(gradients, self.student.trainable_variables))\n",
    "            # Validation\n",
    "            pb_val = tf.keras.utils.Progbar(batchNbVal)\n",
    "            for step, (x_batch_val, y_batch_val) in enumerate(valData):\n",
    "                teacherPredsVal = self.teacher(x_batch_val, training=False)\n",
    "                studentPredsVal = self.student(x_batch_val, training=False)\n",
    "                # Computing losses and metrics\n",
    "                # Computing losses\n",
    "                if self.alreadySoftmax == False:\n",
    "                    distilLossVal = self.distilLoss(\n",
    "                        tf.keras.activations.softmax(teacherPredsVal) / temperature,\n",
    "                        tf.keras.activations.softmax(studentPredsVal) / temperature\n",
    "                    )\n",
    "                    studentLossVal = self.studentLoss(\n",
    "                        y_batch_val,\n",
    "                        tf.keras.activations.softmax(studentPredsVal)\n",
    "                    )\n",
    "                else:\n",
    "                    distilLossVal = self.distilLoss(\n",
    "                        teacherPredsVal / temperature,\n",
    "                        studentPredsVal / temperature\n",
    "                    )\n",
    "                    studentLossVal = self.studentLoss(\n",
    "                        y_batch_val,\n",
    "                        tf.keras.activations.softmax(studentPredsVal)\n",
    "                    )\n",
    "                lossVal = (alpha * studentLossVal) + ((1 - alpha) * distilLossVal)\n",
    "                # Updating validation losses and metrics\n",
    "                lossesTuplesVal = [\n",
    "                    ('Val_loss', lossVal),\n",
    "                    ('Val_distilLoss', distilLossVal),\n",
    "                    ('Val_studentLoss', studentLossVal)\n",
    "                ]\n",
    "                # Computing metrics\n",
    "                metricsTuplesVal = []\n",
    "                # Comparing to ground truth values\n",
    "                for handler in metricsHandlerTruth:\n",
    "                    if step == 0:\n",
    "                        handler.reset_state()\n",
    "                    handler.update_state(y_batch_val.numpy(), studentPredsVal.numpy())\n",
    "                    metricsTuplesVal.append(('Val_' + handler.name, handler.result().numpy()))\n",
    "                # Comparing to teacher\n",
    "                for handler in metricsHandlerTeacher:\n",
    "                    if step == 0:\n",
    "                        handler.reset_state()\n",
    "                    handler.update_state(teacherPredsVal.numpy(), studentPredsVal.numpy())\n",
    "                    metricsTuplesVal.append((\"Val_distillation_\" + handler.name, handler.result().numpy()))\n",
    "                globalTuplesVal = lossesTuplesVal + metricsTuplesVal\n",
    "                pb_val.add(1, values=globalTuplesVal)\n",
    "        # Returning distilled student\n",
    "        return self.student\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3cb11575",
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = tf.keras.datasets.cifar10.load_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "46a2af2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_shape = x_train.shape[1::]\n",
    "y_train_cat = tf.keras.utils.to_categorical(y_train, num_classes=10)\n",
    "y_test_cat = tf.keras.utils.to_categorical(y_test, num_classes=10)\n",
    "x_train = x_train.astype('float32')\n",
    "x_test = x_test.astype('float32')\n",
    "x_train = x_train / 255.0\n",
    "x_test = x_test / 255.0\n",
    "\n",
    "nb_classes = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2f6af43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sparse = tf.data.Dataset.from_tensor_slices((x_train, y_train)).batch(32)\n",
    "train_cat = tf.data.Dataset.from_tensor_slices((x_train, y_train_cat))\n",
    "train_cat_batch = tf.data.Dataset.from_tensor_slices((x_train, y_train_cat)).batch(32)\n",
    "test_sparse = tf.data.Dataset.from_tensor_slices((x_test, y_test)).batch(32)\n",
    "test_cat = tf.data.Dataset.from_tensor_slices((x_test, y_test_cat))\n",
    "test_cat_batch = tf.data.Dataset.from_tensor_slices((x_test, y_test_cat)).batch(32)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3199f45b",
   "metadata": {},
   "outputs": [],
   "source": [
    "teacher = tf.keras.models.load_model('teacher_model_CKD_presentation.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "d6fc2d99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"teacher\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 16, 32)        896       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 32)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 16, 16, 32)        9248      \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 8, 8, 32)         0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 8, 8, 64)          18496     \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 64)          0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 8, 8, 64)          36928     \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 4, 4, 64)         0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 4, 4, 128)         73856     \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 4, 4, 128)         0         \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 4, 4, 128)         147584    \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPooling  (None, 2, 2, 128)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 512)               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1024)              525312    \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              1049600   \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 10)                10250     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,872,170\n",
      "Trainable params: 1,872,170\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "teacher.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7da1f03f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 2s 3ms/step - loss: 0.7514 - sparse_categorical_accuracy: 0.7638 - accuracy: 0.7638\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[0.7514100074768066, 0.7638000249862671, 0.7638000249862671]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "teacher.evaluate(test_sparse)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "9561b924",
   "metadata": {},
   "outputs": [],
   "source": [
    "student = tf.keras.Sequential(\n",
    "        [\n",
    "            tf.keras.Input(shape=input_shape),\n",
    "            tf.keras.layers.Conv2D(16, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.LeakyReLU(alpha=0.2),\n",
    "            tf.keras.layers.MaxPooling2D(pool_size=(2, 2), strides=(1, 1), padding=\"same\"),\n",
    "            tf.keras.layers.Conv2D(32, (3, 3), strides=(2, 2), padding=\"same\"),\n",
    "            tf.keras.layers.Dropout(0.2),\n",
    "            tf.keras.layers.Flatten(),\n",
    "            tf.keras.layers.Dense(10)\n",
    "        ],\n",
    "        name=\"student\",\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5bb2d39d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"student\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 16, 16, 16)        448       \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 16, 16, 16)        0         \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 16, 16, 16)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 32)          4640      \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 8, 8, 32)          0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 10)                20490     \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 25,578\n",
      "Trainable params: 25,578\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "student.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3c7373ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "dist = CKD(\n",
    "    teacher,\n",
    "    student,\n",
    "    alreadySoftmax=False,\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    studentLoss=tf.keras.losses.CategoricalCrossentropy()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "b08ba991",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distillation Epoch 1/10\n",
      "1563/1563 [==============================] - 33s 21ms/step - Loss: 0.4873 - DistilLoss: 0.3690 - StudentLoss: 1.5520 - categorical_accuracy: 0.4492 - Distillation_categorical_accuracy: 0.4618\n",
      "313/313 [==============================] - 4s 12ms/step - Val_loss: 0.4076 - Val_distilLoss: 0.2943 - Val_studentLoss: 1.4276 - Val_categorical_accuracy: 0.5019 - Val_distillation_categorical_accuracy: 0.5017\n",
      "Distillation Epoch 2/10\n",
      "1563/1563 [==============================] - 33s 21ms/step - Loss: 0.4110 - DistilLoss: 0.3066 - StudentLoss: 1.3509 - categorical_accuracy: 0.5256 - Distillation_categorical_accuracy: 0.5413\n",
      "313/313 [==============================] - 4s 12ms/step - Val_loss: 0.3722 - Val_distilLoss: 0.2647 - Val_studentLoss: 1.3394 - Val_categorical_accuracy: 0.5452 - Val_distillation_categorical_accuracy: 0.5450\n",
      "Distillation Epoch 3/10\n",
      "1563/1563 [==============================] - 33s 21ms/step - Loss: 0.3826 - DistilLoss: 0.2835 - StudentLoss: 1.2747 - categorical_accuracy: 0.5549 - Distillation_categorical_accuracy: 0.5708\n",
      "313/313 [==============================] - 4s 12ms/step - Val_loss: 0.3428 - Val_distilLoss: 0.2414 - Val_studentLoss: 1.2553 - Val_categorical_accuracy: 0.5734 - Val_distillation_categorical_accuracy: 0.5730\n",
      "Distillation Epoch 4/10\n",
      "1563/1563 [==============================] - 34s 21ms/step - Loss: 0.3611 - DistilLoss: 0.2663 - StudentLoss: 1.2145 - categorical_accuracy: 0.5762 - Distillation_categorical_accuracy: 0.5936\n",
      "313/313 [==============================] - 4s 13ms/step - Val_loss: 0.3177 - Val_distilLoss: 0.2215 - Val_studentLoss: 1.1832 - Val_categorical_accuracy: 0.6115 - Val_distillation_categorical_accuracy: 0.6113\n",
      "Distillation Epoch 5/10\n",
      "1563/1563 [==============================] - 33s 21ms/step - Loss: 0.3513 - DistilLoss: 0.2585 - StudentLoss: 1.1862 - categorical_accuracy: 0.5857 - Distillation_categorical_accuracy: 0.6025\n",
      "313/313 [==============================] - 4s 12ms/step - Val_loss: 0.3118 - Val_distilLoss: 0.2162 - Val_studentLoss: 1.1717 - Val_categorical_accuracy: 0.6116 - Val_distillation_categorical_accuracy: 0.6115\n",
      "Distillation Epoch 6/10\n",
      "1563/1563 [==============================] - 34s 21ms/step - Loss: 0.3446 - DistilLoss: 0.2531 - StudentLoss: 1.1685 - categorical_accuracy: 0.5930 - Distillation_categorical_accuracy: 0.6105\n",
      "313/313 [==============================] - 4s 12ms/step - Val_loss: 0.3179 - Val_distilLoss: 0.2206 - Val_studentLoss: 1.1935 - Val_categorical_accuracy: 0.5990 - Val_distillation_categorical_accuracy: 0.5990\n",
      "Distillation Epoch 7/10\n",
      "1563/1563 [==============================] - 33s 21ms/step - Loss: 0.3412 - DistilLoss: 0.2502 - StudentLoss: 1.1603 - categorical_accuracy: 0.5984 - Distillation_categorical_accuracy: 0.6164\n",
      "313/313 [==============================] - 4s 12ms/step - Val_loss: 0.2979 - Val_distilLoss: 0.2056 - Val_studentLoss: 1.1282 - Val_categorical_accuracy: 0.6298 - Val_distillation_categorical_accuracy: 0.6297\n",
      "Distillation Epoch 8/10\n",
      "1563/1563 [==============================] - 33s 21ms/step - Loss: 0.3373 - DistilLoss: 0.2473 - StudentLoss: 1.1480 - categorical_accuracy: 0.6016 - Distillation_categorical_accuracy: 0.6202\n",
      "313/313 [==============================] - 4s 12ms/step - Val_loss: 0.3043 - Val_distilLoss: 0.2107 - Val_studentLoss: 1.1460 - Val_categorical_accuracy: 0.6151 - Val_distillation_categorical_accuracy: 0.6152\n",
      "Distillation Epoch 9/10\n",
      "1563/1563 [==============================] - 33s 21ms/step - Loss: 0.3357 - DistilLoss: 0.2460 - StudentLoss: 1.1429 - categorical_accuracy: 0.6015 - Distillation_categorical_accuracy: 0.6197\n",
      "313/313 [==============================] - 4s 13ms/step - Val_loss: 0.3059 - Val_distilLoss: 0.2116 - Val_studentLoss: 1.1544 - Val_categorical_accuracy: 0.6158 - Val_distillation_categorical_accuracy: 0.6156\n",
      "Distillation Epoch 10/10\n",
      "1563/1563 [==============================] - 33s 21ms/step - Loss: 0.3319 - DistilLoss: 0.2431 - StudentLoss: 1.1311 - categorical_accuracy: 0.6071 - Distillation_categorical_accuracy: 0.6264\n",
      "313/313 [==============================] - 4s 12ms/step - Val_loss: 0.3119 - Val_distilLoss: 0.2169 - Val_studentLoss: 1.1666 - Val_categorical_accuracy: 0.6083 - Val_distillation_categorical_accuracy: 0.6083\n"
     ]
    }
   ],
   "source": [
    "distilled = dist.distil(\n",
    "    trainData=train_cat,\n",
    "    valData=test_cat,\n",
    "    epochs=10\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "db3f1f64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313/313 [==============================] - 1s 2ms/step - loss: 5.6705 - categorical_accuracy: 0.6054\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[5.670470714569092, 0.605379045009613]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "distilled.evaluate(test_cat_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "834442aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "distilled.save('student.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2043c49",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
